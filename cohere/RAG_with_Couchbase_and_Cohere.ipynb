{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYUkZqeoEykk",
        "outputId": "39e6825b-b448-4c86-b70f-16fdf6798137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: couchbase in /usr/local/lib/python3.10/dist-packages (4.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.10/dist-packages (0.2.30)\n",
            "Requirement already satisfied: langchain_cohere in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: langchain_couchbase in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.1.21)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (17.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (2024.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (0.1.99)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (4.12.2)\n",
            "Requirement already satisfied: cohere<6.0,>=5.5.6 in /usr/local/lib/python3.10/dist-packages (from langchain_cohere) (5.8.0)\n",
            "Requirement already satisfied: langchain-experimental>=0.0.6 in /usr/local/lib/python3.10/dist-packages (from langchain_cohere) (0.0.64)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from langchain_cohere) (0.9.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.40.6)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: boto3<2.0.0,>=1.34.0 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (1.34.160)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (1.9.5)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (0.27.0)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (0.4.0)\n",
            "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (0.9.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (2.20.1)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (0.19.1)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (2.32.0.20240712)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: langchain-community<0.3.0,>=0.2.10 in /usr/local/lib/python3.10/dist-packages (from langchain-experimental>=0.0.6->langchain_cohere) (0.2.12)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (3.10.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.7.24)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.160 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain_cohere) (1.34.160)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain_cohere) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain_cohere) (0.10.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain_cohere) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain_cohere) (0.14.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.10->langchain-experimental>=0.0.6->langchain_cohere) (2.0.32)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.10->langchain-experimental>=0.0.6->langchain_cohere) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.13 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.2.10->langchain-experimental>=0.0.6->langchain_cohere) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.10->langchain-experimental>=0.0.6->langchain_cohere) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.10->langchain-experimental>=0.0.6->langchain_cohere) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.13->langchain-community<0.3.0,>=0.2.10->langchain-experimental>=0.0.6->langchain_cohere) (0.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.3.0,>=0.2.10->langchain-experimental>=0.0.6->langchain_cohere) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.10->langchain-experimental>=0.0.6->langchain_cohere) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "# Install python-dotenv\n",
        "!pip install python-dotenv couchbase datasets langchain_core langchain_cohere langchain_couchbase langchain_openai pyarrow requests fsspec tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Necessary Libraries\n",
        "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading. These libraries provide essential functions for working with data, managing database connections, and processing machine learning models."
      ],
      "metadata": {
        "id": "Dw3IL3GEJSj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import getpass\n",
        "from datetime import timedelta\n",
        "from uuid import uuid4\n",
        "\n",
        "import numpy as np\n",
        "from couchbase.auth import PasswordAuthenticator\n",
        "from couchbase.cluster import Cluster\n",
        "from couchbase.exceptions import (CouchbaseException,\n",
        "                                  InternalServerFailureException,\n",
        "                                  QueryIndexAlreadyExistsException)\n",
        "from couchbase.management.search import SearchIndex\n",
        "from couchbase.options import ClusterOptions\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "from langchain_cohere import ChatCohere, CohereEmbeddings\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.globals import set_llm_cache\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_couchbase.cache import CouchbaseCache\n",
        "from langchain_couchbase.vectorstores import CouchbaseVectorStore\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "oziN03NZJLQw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Logging\n",
        "Logging is configured to track the progress of the script and capture any errors or warnings. This is crucial for debugging and understanding the flow of execution. The logging output includes timestamps, log levels (e.g., INFO, ERROR), and messages that describe what is happening in the script.\n"
      ],
      "metadata": {
        "id": "TneKO5JOy0uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ],
      "metadata": {
        "id": "t0CIdLa9y39X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Environment Variables\n",
        "These variables typically include sensitive information like API keys, database usernames, and passwords. Using environment variables helps keep the code clean and secure by not hardcoding sensitive information directly into the script."
      ],
      "metadata": {
        "id": "zOwSwRoHJLXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COHERE_API_KEY = getpass.getpass('Enter your Cohere API key: ')\n",
        "CB_HOST = input('Enter your Couchbase host (default: couchbase://localhost): ') or 'couchbase://localhost'\n",
        "CB_USERNAME = input('Enter your Couchbase username (default: Administrator): ') or 'Administrator'\n",
        "CB_PASSWORD = getpass.getpass('Enter your Couchbase password (default: password): ') or 'password'\n",
        "CB_BUCKET_NAME = input('Enter your Couchbase bucket name (default: vector-search-testing): ') or 'vector-search-testing'\n",
        "INDEX_NAME = input('Enter your index name (default: vector_search_cohere): ') or 'vector_search_cohere'\n",
        "SCOPE_NAME = input('Enter your scope name (default: shared): ') or 'shared'\n",
        "COLLECTION_NAME = input('Enter your collection name (default: cohere): ') or 'cohere'\n",
        "CACHE_COLLECTION = input('Enter your cache collection name (default: cache): ') or 'cache'\n",
        "\n",
        "# Check if the variables are correctly loaded\n",
        "if not COHERE_API_KEY:\n",
        "    raise ValueError(\"COHERE_API_KEY is not provided and is required.\")\n",
        "if not CB_HOST:\n",
        "    warnings.warn(\"CB_HOST is not provided. Using default value: couchbase://localhost\")\n",
        "if not CB_USERNAME:\n",
        "    warnings.warn(\"CB_USERNAME is not provided. Using default value: Administrator\")\n",
        "if not CB_PASSWORD:\n",
        "    warnings.warn(\"CB_PASSWORD is not provided. Using default value: password\")\n"
      ],
      "metadata": {
        "id": "y2H9xphrJLbP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to Couchbase\n",
        "The script attempts to establish a connection to the Couchbase database using the credentials retrieved from the environment variables. Couchbase is a NoSQL database known for its flexibility, scalability, and support for various data models, including document-based storage. The connection is authenticated using a username and password, and the script waits until the connection is fully established before proceeding.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sdKdLg9pJLl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
        "    options = ClusterOptions(auth)\n",
        "    cluster = Cluster(CB_HOST, options)\n",
        "    cluster.wait_until_ready(timedelta(seconds=5))\n",
        "    logging.info(\"Successfully connected to Couchbase\")\n",
        "except Exception as e:\n",
        "    raise ConnectionError(f\"Failed to connect to Couchbase: {str(e)}\")"
      ],
      "metadata": {
        "id": "HubiGMCSJLqw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Index Definition\n",
        "The search index definition is loaded from a JSON file. This index defines how the data in Couchbase should be indexed for fast search and retrieval. Indexing is critical for optimizing search queries, especially when dealing with large datasets. The JSON file contains details about the index, such as its name, source type, and parameters."
      ],
      "metadata": {
        "id": "j4tYSkkDxS9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# index_definition_file_path = \"/path/to/cohere_index.json\"\n",
        "\n",
        "# try:\n",
        "#     with open(index_definition_file_path, 'r') as file:\n",
        "#         index_definition = json.load(file)\n",
        "# except Exception as e:\n",
        "#     raise ValueError(f\"Error loading index definition from {index_definition_file_path}: {str(e)}\")"
      ],
      "metadata": {
        "id": "szXN-oNGxTMF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create or Update Search Index\n",
        "The script checks if the search index already exists in Couchbase. If it exists, the index is updated; if not, a new index is created. This step ensures that the data is properly indexed, allowing for efficient search operations later in the script. The index is associated with a specific bucket, scope, and collection in Couchbase, which organizes the data.\n"
      ],
      "metadata": {
        "id": "TXGj5YokJLuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try:\n",
        "#     scope_index_manager = cluster.bucket(CB_BUCKET_NAME).scope(SCOPE_NAME).search_indexes()\n",
        "#     existing_indexes = scope_index_manager.get_all_indexes()\n",
        "#     index_name = index_definition[\"name\"]\n",
        "\n",
        "#     if index_name in [index.name for index in existing_indexes]:\n",
        "#         logging.info(f\"Index '{index_name}' already exists. Updating...\")\n",
        "#     else:\n",
        "#         logging.info(f\"Creating new index '{index_name}'...\")\n",
        "\n",
        "#     search_index = SearchIndex(\n",
        "#         name=index_definition[\"name\"],\n",
        "#         source_type=index_definition.get(\"sourceType\", \"couchbase\"),\n",
        "#         idx_type=index_definition[\"type\"],\n",
        "#         source_name=index_definition[\"sourceName\"],\n",
        "#         params=index_definition[\"params\"],\n",
        "#         source_params=index_definition.get(\"sourceParams\", {}),\n",
        "#         plan_params=index_definition.get(\"planParams\", {})\n",
        "#     )\n",
        "\n",
        "#     scope_index_manager.upsert_index(search_index)\n",
        "#     logging.info(f\"Index '{index_name}' successfully created/updated.\")\n",
        "\n",
        "# except QueryIndexAlreadyExistsException:\n",
        "#     logging.info(f\"Index '{index_name}' already exists. Skipping creation/update.\")\n",
        "# except InternalServerFailureException as e:\n",
        "#     error_message = str(e)\n",
        "#     logging.error(f\"InternalServerFailureException raised: {error_message}\")\n",
        "#     try:\n",
        "#         error_context = e.context\n",
        "#         response_body = error_context.response_body\n",
        "#         if response_body:\n",
        "#             error_details = json.loads(response_body)\n",
        "#             error_message = error_details.get('error', '')\n",
        "#             if \"collection: 'cohere' doesn't belong to scope: 'shared'\" in error_message:\n",
        "#                 raise ValueError(\"Collection 'cohere' does not belong to scope 'shared'. Please check the collection and scope names.\")\n",
        "#     except ValueError as ve:\n",
        "#         logging.error(str(ve))\n",
        "#         raise\n",
        "#     except Exception as json_error:\n",
        "#         logging.error(f\"Failed to parse the error message: {json_error}\")\n",
        "#         raise RuntimeError(f\"Internal server error while creating/updating search index: {error_message}\")\n",
        "\n",
        "# except Exception as e:\n",
        "#     logging.error(f\"Error creating/updating search index: {str(e)}\")\n",
        "#     raise RuntimeError(f\"Unexpected error while creating/updating search index: {str(e)}\")"
      ],
      "metadata": {
        "id": "VHeB_AVmLJlx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load TREC Dataset\n",
        "The TREC dataset is loaded using the datasets library. TREC is a well-known dataset used in information retrieval and natural language processing (NLP) tasks. In this script, the dataset will be used to generate embeddings, which are numerical representations of text that capture its meaning in a form suitable for machine learning models.\n"
      ],
      "metadata": {
        "id": "3if29jOix26X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    trec = load_dataset('trec', split='train[:1000]')\n",
        "    logging.info(f\"Successfully loaded TREC dataset with {len(trec)} samples\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error loading TREC dataset: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-2ygZRRx3DB",
        "outputId": "22897dd6-4f28-4609-c84c-65617ccf8487"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Embeddings\n",
        "Embeddings are created using the Cohere API. Embeddings are vectors (arrays of numbers) that represent the meaning of text in a high-dimensional space. These embeddings are crucial for tasks like semantic search, where the goal is to find text that is semantically similar to a query. The script uses a pre-trained model provided by Cohere to generate embeddings for the text in the TREC dataset."
      ],
      "metadata": {
        "id": "LT3s8x_Mx3KG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    embeddings = CohereEmbeddings(\n",
        "        cohere_api_key=COHERE_API_KEY,\n",
        "        model=\"embed-english-v3.0\",\n",
        "    )\n",
        "    logging.info(\"Successfully created CohereEmbeddings\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error creating CohereEmbeddings: {str(e)}\")"
      ],
      "metadata": {
        "id": "A6fG7Mopx3Np"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up Vector Store\n",
        "The vector store is set up to manage the embeddings created in the previous step. The vector store is essentially a database optimized for storing and retrieving high-dimensional vectors. In this case, the vector store is built on top of Couchbase, allowing the script to store the embeddings in a way that can be efficiently searched.\n"
      ],
      "metadata": {
        "id": "iar2fABrLJjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    vector_store = CouchbaseVectorStore(\n",
        "        cluster=cluster,\n",
        "        bucket_name=CB_BUCKET_NAME,\n",
        "        scope_name=SCOPE_NAME,\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        embedding=embeddings,\n",
        "        index_name=INDEX_NAME,\n",
        "    )\n",
        "    logging.info(\"Successfully created vector store\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Failed to create vector store: {str(e)}\")"
      ],
      "metadata": {
        "id": "cjASXR3dLJgZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Data to Vector Store in Batches\n",
        "To avoid overloading memory, the TREC dataset's text fields are saved to the vector store in batches. This step is important for handling large datasets, as it breaks down the data into manageable chunks that can be processed sequentially. Each piece of text is converted into a document, assigned a unique identifier, and then stored in the vector store.\n"
      ],
      "metadata": {
        "id": "LdmbxLdCLJdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    batch_size = 50\n",
        "    for i in tqdm(range(0, len(trec['text']), batch_size), desc=\"Processing Batches\"):\n",
        "        batch = trec['text'][i:i + batch_size]\n",
        "        documents = [Document(page_content=text) for text in batch]\n",
        "        uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "        vector_store.add_documents(documents=documents, ids=uuids)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to save documents to vector store: {str(e)}\")"
      ],
      "metadata": {
        "id": "HrJbgaKRLJbS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ab93b0-89f4-44fd-c3a8-ec6cd85bf6be"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batches: 100%|██████████| 20/20 [00:23<00:00,  1.18s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up Cache\n",
        " A cache is set up using Couchbase to store intermediate results and frequently accessed data. Caching is important for improving performance, as it reduces the need to repeatedly calculate or retrieve the same data. The cache is linked to a specific collection in Couchbase, and it is used later in the script to store the results of language model queries.\n"
      ],
      "metadata": {
        "id": "ToQ2acrSLJY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    cache = CouchbaseCache(\n",
        "        cluster=cluster,\n",
        "        bucket_name=CB_BUCKET_NAME,\n",
        "        scope_name=SCOPE_NAME,\n",
        "        collection_name=CACHE_COLLECTION,\n",
        "    )\n",
        "    logging.info(\"Successfully created cache\")\n",
        "    set_llm_cache(cache)\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Failed to create cache: {str(e)}\")"
      ],
      "metadata": {
        "id": "qZDXvq88LJWH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Language Model (LLM)\n",
        "The script initializes a Cohere language model (LLM) that will be used for generating responses to queries. LLMs are powerful tools for natural language understanding and generation, capable of producing human-like text based on input prompts. The model is configured with specific parameters, such as the temperature, which controls the randomness of its outputs.\n"
      ],
      "metadata": {
        "id": "GQpib0zKLJTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    llm = ChatCohere(\n",
        "        cohere_api_key=COHERE_API_KEY,\n",
        "        model=\"command\",\n",
        "        temperature=0\n",
        "    )\n",
        "    logging.info(f\"Successfully created Cohere LLM with model command\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error creating Cohere LLM: {str(e)}\")"
      ],
      "metadata": {
        "id": "7eV1X5xILJRC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Retrieval-Augmented Generation (RAG) Chain\n",
        "A RAG chain is created to combine the capabilities of the vector store and the language model. The RAG chain is a system that first retrieves relevant documents from the vector store and then uses the language model to generate a response based on those documents. This approach allows the model to produce more informed and contextually relevant answers to queries."
      ],
      "metadata": {
        "id": "Bt44X6-bLJOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"You are a helpful bot. If you cannot answer based on the context provided, respond with a generic answer. Answer the question as truthfully as possible using the context below:\n",
        "{context}\n",
        "\n",
        "Question: {question}\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\"context\": vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "logging.info(\"Successfully created RAG chain\")"
      ],
      "metadata": {
        "id": "6cGJfwS2LI_O"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Semantic Search\n",
        "The script performs a semantic search by running a sample query through the RAG chain and vector store. Semantic search goes beyond keyword matching and instead looks for the meaning of the query, returning results that are conceptually similar. This step involves calculating the similarity between the query and the documents in the vector store, ranking them based on their relevance."
      ],
      "metadata": {
        "id": "wQ0fNbphbWpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What caused the 1929 Great Depression?\"\n",
        "\n",
        "# Get RAG response\n",
        "start_time = time.time()\n",
        "rag_response = chain.invoke(query)\n",
        "rag_elapsed_time = time.time() - start_time\n",
        "logging.info(f\"RAG response generated in {rag_elapsed_time:.2f} seconds\")\n",
        "print(f\"RAG Response: {rag_response}\")\n",
        "\n",
        "# Perform semantic search\n",
        "try:\n",
        "    start_time = time.time()\n",
        "    search_results = vector_store.similarity_search_with_score(query, k=10)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    results = [{'id': doc.metadata.get('id', 'N/A'), 'text': doc.page_content, 'distance': score}\n",
        "               for doc, score in search_results]\n",
        "    logging.info(f\"Semantic search completed in {elapsed_time:.2f} seconds\")\n",
        "    print(f\"\\nSemantic Search Results (completed in {elapsed_time:.2f} seconds):\")\n",
        "    for result in results:\n",
        "        print(f\"Distance: {result['distance']:.4f}, Text: {result['text']}\")\n",
        "except CouchbaseException as e:\n",
        "    raise RuntimeError(f\"Error performing semantic search: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udcxHyloyoxE",
        "outputId": "27b7fb8f-57e2-418c-f029-14584884a5fa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Response: The 1929 Great Depression was caused by a combination of factors, including the stock market crash of 1929, financial institution failures, and a decline in consumer spending and investment. The stock market crash, known as \"Black Tuesday\", was a significant event that marked the beginning of the Great Depression. However, it was not the only factor contributing to the economic decline. \n",
            "\n",
            "Financial institution failures played a role in the depression as banks and other financial institutions became insolvent, which led to customers losing their deposits and a lack of lending further deepening the recession. The decline in consumer spending and investment also contributed to the depression as people lost their savings and were unable to spend money on goods and services, leading to decreased economic activity and job losses.\n",
            "\n",
            "Other factors included the agricultural crisis of the 1920s, which led to high unemployment rates and poverty, and the failure of President Hoover to address the crisis. Additionally, protective tariffs imposed by the US and other countries further isolated their economies, ultimately worsening the global effects of the depression. \n",
            "\n",
            "It is important to note that the causes of the 1929 Great Depression were complex and multi-faceted, and it was a combination of these factors that led to the economic decline.\n",
            "\n",
            "Semantic Search Results (completed in 0.53 seconds):\n",
            "Distance: 0.6208, Text: Why did the world enter a global depression in 1929 ?\n",
            "Distance: 0.6208, Text: Why did the world enter a global depression in 1929 ?\n",
            "Distance: 0.6208, Text: Why did the world enter a global depression in 1929 ?\n",
            "Distance: 0.6208, Text: Why did the world enter a global depression in 1929 ?\n",
            "Distance: 0.4913, Text: When was `` the Great Depression '' ?\n",
            "Distance: 0.4913, Text: When was `` the Great Depression '' ?\n",
            "Distance: 0.4913, Text: When was `` the Great Depression '' ?\n",
            "Distance: 0.4913, Text: When was `` the Great Depression '' ?\n",
            "Distance: 0.3775, Text: What crop failure caused the Irish Famine ?\n",
            "Distance: 0.3775, Text: What crop failure caused the Irish Famine ?\n"
          ]
        }
      ]
    }
  ]
}