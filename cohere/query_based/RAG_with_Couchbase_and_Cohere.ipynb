{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAPY14a2BOhq"
   },
   "source": [
    "## Introduction\n",
    "In this guide, we will walk you through building a powerful semantic search engine using Couchbase as the backend database and [Cohere](https://cohere.com/)\n",
    " as the AI-powered embedding and language model provider. Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval. This tutorial is designed to be beginner-friendly, with clear, step-by-step instructions that will equip you with the knowledge to create a fully functional semantic search system using Couchbase Hyperscale and Composite Vector Index from scratch. Alternatively if you want to perform semantic search using the Search Vector Index, please take a look at [this.](https://developer.couchbase.com/tutorial-cohere-couchbase-rag-with-search-vector-index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run this tutorial\n",
    "\n",
    "This tutorial is available as a Jupyter Notebook (`.ipynb` file) that you can run interactively. You can access the original notebook [here](https://github.com/couchbase-examples/vector-search-cookbook/blob/main/cohere/query_based/RAG_with_Couchbase_and_Cohere.ipynb).\n",
    "\n",
    "You can either download the notebook file and run it on [Google Colab](https://colab.research.google.com/) or run it on your system by setting up the Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "## Get Credentials for Cohere\n",
    "\n",
    "Please follow the [instructions](https://dashboard.cohere.com/welcome/register) to generate the Cohere credentials.\n",
    "\n",
    "## Create and Deploy Your Free Tier Operational cluster on Capella\n",
    "\n",
    "To get started with Couchbase Capella, create an account and use it to deploy a forever free tier operational cluster. This account provides you with an environment where you can explore and learn about Capella with no time constraint.\n",
    "\n",
    "To learn more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html).\n",
    "\n",
    "Note: To run this tutorial, you will need Capella with Couchbase Server version 8.0 or above as Hyperscale and Composite Vector Index search is supported only from version 8.0\n",
    "\n",
    "### Couchbase Capella Configuration\n",
    "\n",
    "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met.\n",
    "\n",
    "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the required bucket (Read and Write) used in the application.\n",
    "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYZzrd_tBdUC"
   },
   "source": [
    "## Setting the Stage: Installing Necessary Libraries\n",
    "To build our semantic search engine, we need a robust set of tools. The libraries we install handle everything from connecting to databases to performing complex machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYUkZqeoEykk"
   },
   "outputs": [],
   "source": [
    "%pip install --quiet datasets==3.5.0 langchain-couchbase==1.0.1 langchain-cohere==0.5.0 python-dotenv==1.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dw3IL3GEJSj7"
   },
   "source": [
    "## Importing Necessary Libraries\n",
    "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading. These libraries provide essential functions for working with data, managing database connections, and processing machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oziN03NZJLQw"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from uuid import uuid4\n",
    "\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.exceptions import (CouchbaseException,\n",
    "                                  InternalServerFailureException,\n",
    "                                  QueryIndexAlreadyExistsException,\n",
    "                                  ServiceUnavailableException)\n",
    "from couchbase.management.buckets import CreateBucketSettings\n",
    "from couchbase.management.search import SearchIndex\n",
    "from couchbase.options import ClusterOptions\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from langchain_cohere import ChatCohere, CohereEmbeddings\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_couchbase.cache import CouchbaseCache\n",
    "from langchain_couchbase.vectorstores import CouchbaseQueryVectorStore\n",
    "from langchain_couchbase.vectorstores import DistanceStrategy\n",
    "from langchain_couchbase.vectorstores import IndexType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXwzTRdbCLL1"
   },
   "source": [
    "## Setup Logging\n",
    "Logging is configured to track the progress of the script and capture any errors or warnings. This is crucial for debugging and understanding the flow of execution. The logging output includes timestamps, log levels (e.g., INFO, ERROR), and messages that describe what is happening in the script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-SanCZrCLdm"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',force=True)\n",
    "\n",
    "# Supress Excessive logging\n",
    "logging.getLogger('openai').setLevel(logging.WARNING)\n",
    "logging.getLogger('httpx').setLevel(logging.WARNING)\n",
    "logging.getLogger('langchain_cohere').setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOwSwRoHJLXv"
   },
   "source": [
    "## Loading Sensitive Informnation\n",
    "In this section, we prompt the user to input essential configuration settings needed for integrating Couchbase with Cohere's API. These settings include sensitive information like API keys, database credentials, and specific configuration names. Instead of hardcoding these details into the script, we request the user to provide them at runtime, ensuring flexibility and security.\n",
    "\n",
    "The script also validates that all required inputs are provided, raising an error if any crucial information is missing. This approach ensures that your integration is both secure and correctly configured without hardcoding sensitive information, enhancing the overall security and maintainability of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2H9xphrJLbP"
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "COHERE_API_KEY = os.getenv('COHERE_API_KEY') or getpass.getpass('Enter your Cohere API key: ')\n",
    "CB_HOST = os.getenv('CB_HOST') or input('Enter your Couchbase host (default: couchbase://localhost): ') or 'couchbase://localhost'\n",
    "CB_USERNAME = os.getenv('CB_USERNAME') or input('Enter your Couchbase username (default: Administrator): ') or 'Administrator'\n",
    "CB_PASSWORD = os.getenv('CB_PASSWORD') or getpass.getpass('Enter your Couchbase password (default: password): ') or 'password'\n",
    "CB_BUCKET_NAME = os.getenv('CB_BUCKET_NAME') or input('Enter your Couchbase bucket name (default: query-vector-search-testing): ') or 'query-vector-search-testing'\n",
    "SCOPE_NAME = os.getenv('SCOPE_NAME') or input('Enter your scope name (default: shared): ') or 'shared'\n",
    "COLLECTION_NAME = os.getenv('COLLECTION_NAME') or input('Enter your collection name (default: cohere): ') or 'cohere'\n",
    "CACHE_COLLECTION = os.getenv('CACHE_COLLECTION') or input('Enter your cache collection name (default: cache): ') or 'cache'\n",
    "\n",
    "# Check if the variables are correctly loaded\n",
    "if not COHERE_API_KEY:\n",
    "    raise ValueError(\"COHERE_API_KEY is not provided and is required.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sdKdLg9pJLl5"
   },
   "source": [
    "## Connect to Couchbase\n",
    "The script attempts to establish a connection to the Couchbase database using the credentials retrieved from the environment variables. Couchbase is a NoSQL database known for its flexibility, scalability, and support for various data models, including document-based storage. The connection is authenticated using a username and password, and the script waits until the connection is fully established before proceeding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HubiGMCSJLqw"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "    options = ClusterOptions(auth)\n",
    "    cluster = Cluster(CB_HOST, options)\n",
    "    cluster.wait_until_ready(timedelta(seconds=5))\n",
    "    logging.info(\"Successfully connected to Couchbase\")\n",
    "except Exception as e:\n",
    "    raise ConnectionError(f\"Failed to connect to Couchbase: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Collections in Couchbase\n",
    "\n",
    "The setup_collection() function handles creating and configuring the hierarchical data organization in Couchbase:\n",
    "\n",
    "1. Bucket Creation:\n",
    "   - Checks if specified bucket exists, creates it if not\n",
    "   - Sets bucket properties like RAM quota (1024MB) and replication (disabled)\n",
    "   - Note: You will not be able to create a bucket on Capella\n",
    "\n",
    "2. Scope Management:  \n",
    "   - Verifies if requested scope exists within bucket\n",
    "   - Creates new scope if needed (unless it's the default \"_default\" scope)\n",
    "\n",
    "3. Collection Setup:\n",
    "   - Checks for collection existence within scope\n",
    "   - Creates collection if it doesn't exist\n",
    "   - Waits 2 seconds for collection to be ready\n",
    "\n",
    "Additional Tasks:\n",
    "- Clears any existing documents for clean state\n",
    "- Implements comprehensive error handling and logging\n",
    "\n",
    "The function is called twice to set up:\n",
    "1. Main collection for vector embeddings\n",
    "2. Cache collection for storing results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_collection(cluster, bucket_name, scope_name, collection_name):\n",
    "    try:\n",
    "        # Check if bucket exists, create if it doesn't\n",
    "        try:\n",
    "            bucket = cluster.bucket(bucket_name)\n",
    "            logging.info(f\"Bucket '{bucket_name}' exists.\")\n",
    "        except Exception as e:\n",
    "            logging.info(f\"Bucket '{bucket_name}' does not exist. Creating it...\")\n",
    "            bucket_settings = CreateBucketSettings(\n",
    "                name=bucket_name,\n",
    "                bucket_type='couchbase',\n",
    "                ram_quota_mb=1024,\n",
    "                flush_enabled=True,\n",
    "                num_replicas=0\n",
    "            )\n",
    "            cluster.buckets().create_bucket(bucket_settings)\n",
    "            time.sleep(2)  # Wait for bucket creation to complete and become available\n",
    "            bucket = cluster.bucket(bucket_name)\n",
    "            logging.info(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "\n",
    "        bucket_manager = bucket.collections()\n",
    "\n",
    "        # Check if scope exists, create if it doesn't\n",
    "        scopes = bucket_manager.get_all_scopes()\n",
    "        scope_exists = any(scope.name == scope_name for scope in scopes)\n",
    "        \n",
    "        if not scope_exists and scope_name != \"_default\":\n",
    "            logging.info(f\"Scope '{scope_name}' does not exist. Creating it...\")\n",
    "            bucket_manager.create_scope(scope_name)\n",
    "            logging.info(f\"Scope '{scope_name}' created successfully.\")\n",
    "\n",
    "        # Check if collection exists, create if it doesn't\n",
    "        collection_exists = any(\n",
    "            scope.name == scope_name and collection_name in [col.name for col in scope.collections]\n",
    "            for scope in scopes\n",
    "        )\n",
    "\n",
    "        if not collection_exists:\n",
    "            logging.info(f\"Collection '{collection_name}' does not exist. Creating it...\")\n",
    "            bucket_manager.create_collection(scope_name, collection_name)\n",
    "            logging.info(f\"Collection '{collection_name}' created successfully.\")\n",
    "        else:\n",
    "            logging.info(f\"Collection '{collection_name}' already exists. Skipping creation.\")\n",
    "\n",
    "        # Wait for collection to be ready\n",
    "        collection = bucket.scope(scope_name).collection(collection_name)\n",
    "        time.sleep(2)  # Give the collection time to be ready for queries\n",
    "\n",
    "        # Clear all documents in the collection\n",
    "        try:\n",
    "            query = f\"DELETE FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
    "            cluster.query(query).execute()\n",
    "            logging.info(\"All documents cleared from the collection.\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error while clearing documents: {str(e)}. The collection might be empty.\")\n",
    "\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error setting up collection: {str(e)}\")\n",
    "    \n",
    "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)\n",
    "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, CACHE_COLLECTION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LT3s8x_Mx3KG"
   },
   "source": [
    "## Create Embeddings\n",
    "Embeddings are created using the Cohere API. Embeddings are vectors (arrays of numbers) that represent the meaning of text in a high-dimensional space. These embeddings are crucial for tasks like semantic search, where the goal is to find text that is semantically similar to a query. The script uses a pre-trained model provided by Cohere to generate embeddings for the text in the BBC News dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6fG7Mopx3Np"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    embeddings = CohereEmbeddings(\n",
    "        cohere_api_key=COHERE_API_KEY,\n",
    "        model=\"embed-english-v3.0\",\n",
    "    )\n",
    "    logging.info(\"Successfully created CohereEmbeddings\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating CohereEmbeddings: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iar2fABrLJjK"
   },
   "source": [
    "## Set Up Vector Store\n",
    "The vector store is set up to manage the embeddings created in the previous step. The vector store is essentially a database optimized for storing and retrieving high-dimensional vectors. In this case, the vector store is built on top of Couchbase, allowing the script to store the embeddings in a way that can be efficiently searched.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjASXR3dLJgZ"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    vector_store = CouchbaseQueryVectorStore(\n",
    "        cluster=cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding = embeddings,\n",
    "        distance_metric=DistanceStrategy.COSINE\n",
    "    )\n",
    "    logging.info(\"Successfully created vector store\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to create vector store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the BBC News Dataset\n",
    "To build a search engine, we need data to search through. We use the BBC News dataset from RealTimeData, which provides real-world news articles. This dataset contains news articles from BBC covering various topics and time periods. Loading the dataset is a crucial step because it provides the raw material that our search engine will work with. The quality and diversity of the news articles make it an excellent choice for testing and refining our search engine, ensuring it can handle real-world news content effectively.\n",
    "\n",
    "The BBC News dataset allows us to work with authentic news articles, enabling us to build and test a search engine that can effectively process and retrieve relevant news content. The dataset is loaded using the Hugging Face datasets library, specifically accessing the \"RealTimeData/bbc_news_alltime\" dataset with the \"2024-12\" version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    news_dataset = load_dataset(\n",
    "        \"RealTimeData/bbc_news_alltime\", \"2024-12\", split=\"train\"\n",
    "    )\n",
    "    print(f\"Loaded the BBC News dataset with {len(news_dataset)} rows\")\n",
    "    logging.info(f\"Successfully loaded the BBC News dataset with {len(news_dataset)} rows.\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading the BBC News dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the Data\n",
    "We will use the content of the news articles for our RAG system.\n",
    "\n",
    "The dataset contains a few duplicate records. We are removing them to avoid duplicate results in the retrieval stage of our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles = news_dataset[\"content\"]\n",
    "unique_articles = set()\n",
    "for article in news_articles:\n",
    "    if article:\n",
    "        unique_articles.add(article)\n",
    "unique_news_articles = list(unique_articles)\n",
    "print(f\"We have {len(unique_news_articles)} unique articles in our database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data to the Vector Store\n",
    "To efficiently handle the large number of articles, we process them in batches of 50 articles at a time. This batch processing approach helps manage memory usage and provides better control over the ingestion process.\n",
    "\n",
    "We first filter out any articles that exceed 50,000 characters to avoid potential issues with token limits. Then, using the vector store's add_texts method, we add the filtered articles to our vector database. The batch_size parameter controls how many articles are processed in each iteration.\n",
    "\n",
    "This approach offers several benefits:\n",
    "1. Memory Efficiency: Processing in smaller batches prevents memory overload\n",
    "2. Progress Tracking: Easier to monitor and track the ingestion progress\n",
    "3. Resource Management: Better control over CPU and network resource utilization\n",
    "\n",
    "We use a conservative batch size of 50 to ensure reliable operation.\n",
    "The optimal batch size depends on many factors including:\n",
    "- Document sizes being inserted\n",
    "- Available system resources\n",
    "- Network conditions\n",
    "- Concurrent workload\n",
    "\n",
    "Consider measuring performance with your specific workload before adjusting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "# Automatic Batch Processing\n",
    "articles = [article for article in unique_news_articles if article and len(article) <= 50000]\n",
    "\n",
    "try:\n",
    "    vector_store.add_texts(\n",
    "        texts=articles,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    logging.info(\"Document ingestion completed successfully.\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to save documents to vector store: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQpib0zKLJTh"
   },
   "source": [
    "## Create Language Model (LLM)\n",
    "The script initializes a Cohere language model (LLM) that will be used for generating responses to queries. LLMs are powerful tools for natural language understanding and generation, capable of producing human-like text based on input prompts. The model is configured with specific parameters, such as the temperature, which controls the randomness of its outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eV1X5xILJRC"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    llm = ChatCohere(\n",
    "        cohere_api_key=COHERE_API_KEY,\n",
    "        model=\"command-a-03-2025\",\n",
    "        temperature=0\n",
    "    )\n",
    "    logging.info(\"Successfully created Cohere LLM with model command\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating Cohere LLM: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQ0fNbphbWpu"
   },
   "source": [
    "## Vector Search Performance Testing\n",
    "\n",
    "Now let's demonstrate the performance benefits of Hyperscale Vector Index by testing pure vector search performance. We'll compare:\n",
    "\n",
    "1. **Baseline Performance**: Vector search without Hyperscale index optimization\n",
    "2. **Hyperscale-Optimized Performance**: Same search with Hyperscale index\n",
    "\n",
    "### Test 1: Baseline Performance (No Hyperscale Index)\n",
    "\n",
    "Semantic search in Couchbase involves converting queries and documents into vector representations using an embeddings model. These vectors capture the semantic meaning of the text and are stored directly in Couchbase. When a query is made, Couchbase performs a similarity search by comparing the query vector against the stored document vectors. The similarity metric used for this comparison is configurable, allowing flexibility in how the relevance of documents is determined.\n",
    "\n",
    "In the code below, we perform a baseline semantic search before creating the Hyperscale index to establish a performance baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udcxHyloyoxE"
   },
   "outputs": [],
   "source": [
    "query = \"What was manchester city manager pep guardiola's reaction to the team's current form?\"\n",
    "\n",
    "try:\n",
    "    # Perform the semantic search\n",
    "    start_time = time.time()\n",
    "    search_results = vector_store.similarity_search_with_score(query, k=10)\n",
    "    baseline_time = time.time() - start_time\n",
    "\n",
    "    logging.info(f\"Baseline search completed in {baseline_time:.2f} seconds\")\n",
    "\n",
    "    # Display search results\n",
    "    print(f\"\\nBaseline Semantic Search Results (completed in {baseline_time:.2f} seconds):\")\n",
    "    print(\"-\" * 80)\n",
    "    for doc, score in search_results:\n",
    "        print(f\"Distance: {score:.4f}, Text: {doc.page_content[:200]}...\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "except CouchbaseException as e:\n",
    "    raise RuntimeError(f\"Error performing semantic search: {str(e)}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Unexpected error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Vector Search with Hyperscale and Composite Vector Index\n",
    "\n",
    "While the above semantic search using similarity_search_with_score works effectively, we can significantly improve query performance by leveraging Couchbase's query-based vector indexing.\n",
    "\n",
    "Couchbase offers three types of vector indexes, but for Hyperscale and Composite Vector Index based search we focus on two main types:\n",
    "\n",
    "Hyperscale Vector Indexes\n",
    "- Best for pure vector searches - content discovery, recommendations, semantic search\n",
    "- High performance with low memory footprint - designed to scale to billions of vectors\n",
    "- Optimized for concurrent operations - supports simultaneous searches and inserts\n",
    "- Use when: You primarily perform vector-only queries without complex scalar filtering\n",
    "- Ideal for: Large-scale semantic search, recommendation systems, content discovery\n",
    "\n",
    "Composite Vector Indexes \n",
    "- Best for filtered vector searches - combines vector search with scalar value filtering\n",
    "- Efficient pre-filtering - scalar attributes reduce the vector comparison scope\n",
    "- Use when: Your queries combine vector similarity with scalar filters that eliminate large portions of data\n",
    "- Ideal for: Compliance-based filtering, user-specific searches, time-bounded queries\n",
    "\n",
    "Choosing the Right Index Type\n",
    "- Start with Hyperscale Vector Index for pure vector searches and large datasets\n",
    "- Use Composite Vector Index when scalar filters significantly reduce your search space\n",
    "- Consider your dataset size: Hyperscale scales to billions, Composite works well for tens of millions to billions\n",
    "\n",
    "For more details, see the [Couchbase Vector Index documentation](https://docs.couchbase.com/cloud/vector-index/use-vector-indexes.html).\n",
    "\n",
    "\n",
    "## Understanding Index Configuration (Couchbase 8.0 Feature)\n",
    "\n",
    "The index_description parameter controls how Couchbase optimizes vector storage and search performance through centroids and quantization:\n",
    "\n",
    "Format: `'IVF[<centroids>],{PQ|SQ}<settings>'`\n",
    "\n",
    "Centroids (IVF - Inverted File):\n",
    "- Controls how the dataset is subdivided for faster searches\n",
    "- More centroids = faster search, slower training  \n",
    "- Fewer centroids = slower search, faster training\n",
    "- If omitted (like IVF,SQ8), Couchbase auto-selects based on dataset size\n",
    "\n",
    "Quantization Options:\n",
    "- SQ (Scalar Quantization): SQ4, SQ6, SQ8 (4, 6, or 8 bits per dimension)\n",
    "- PQ (Product Quantization): PQ<subquantizers>x<bits> (e.g., PQ32x8)\n",
    "- Higher values = better accuracy, larger index size\n",
    "\n",
    "Common Examples:\n",
    "- IVF,SQ8 - Auto centroids, 8-bit scalar quantization (good default)\n",
    "- IVF1000,SQ6 - 1000 centroids, 6-bit scalar quantization  \n",
    "- IVF,PQ32x8 - Auto centroids, 32 subquantizers with 8 bits\n",
    "\n",
    "For detailed configuration options, see the [Quantization & Centroid Settings](https://docs.couchbase.com/cloud/vector-index/hyperscale-vector-index.html#algo_settings).\n",
    "\n",
    "In the code below, we demonstrate creating a Hyperscale index. This method takes an index type (HYPERSCALE or COMPOSITE) and description parameter for optimization settings. Alternatively, Hyperscale and Composite Vector Indexes can be created manually from the Couchbase UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.create_index(index_type=IndexType.HYPERSCALE, index_name=\"cohere_hyperscale_index\",index_description=\"IVF,SQ8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Hyperscale-Optimized Performance\n",
    "\n",
    "The example below shows running the same similarity search, but now using the Hyperscale index we created above. You'll notice improved performance as the index efficiently retrieves data.\n",
    "\n",
    "**Important**: When using Composite indexes, scalar filters take precedence over vector similarity, which can improve performance for filtered searches but may miss some semantically relevant results that don't match the scalar criteria.\n",
    "\n",
    "Note: In Hyperscale and Composite Vector Index search, the distance represents the vector distance between the query and document embeddings. Lower distance indicates higher similarity, while higher distance indicates lower similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What was manchester city manager pep guardiola's reaction to the team's current form?\"\n",
    "\n",
    "try:\n",
    "    # Perform the semantic search with Hyperscale index\n",
    "    start_time = time.time()\n",
    "    search_results = vector_store.similarity_search_with_score(query, k=10)\n",
    "    hyperscale_time = time.time() - start_time\n",
    "\n",
    "    logging.info(f\"Hyperscale search completed in {hyperscale_time:.2f} seconds\")\n",
    "\n",
    "    # Display search results\n",
    "    print(f\"\\nHyperscale Semantic Search Results (completed in {hyperscale_time:.2f} seconds):\")\n",
    "    print(\"-\" * 80)\n",
    "    for doc, score in search_results:\n",
    "        print(f\"Distance: {score:.4f}, Text: {doc.page_content[:200]}...\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "except CouchbaseException as e:\n",
    "    raise RuntimeError(f\"Error performing semantic search: {str(e)}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Unexpected error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Baseline Search Time:     {baseline_time:.4f} seconds\")\n",
    "\n",
    "if baseline_time and hyperscale_time:\n",
    "    speedup = baseline_time / hyperscale_time if hyperscale_time > 0 else float('inf')\n",
    "    percent_improvement = ((baseline_time - hyperscale_time) / baseline_time) * 100 if baseline_time > 0 else 0\n",
    "    print(f\"Hyperscale Search Time:   {hyperscale_time:.4f} seconds ({speedup:.2f}x faster, {percent_improvement:.1f}% improvement)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Index Recommendation:\")\n",
    "print(\"-\"*60)\n",
    "print(\"- Hyperscale: Best for pure vector searches, scales to billions of vectors\")\n",
    "print(\"- Composite: Best for filtered searches combining vector + scalar filters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Composite Index Configuration\n",
    "\n",
    "If your use case requires complex filtering with scalar attributes, you can create a Composite index instead:\n",
    "\n",
    "```python\n",
    "from langchain_couchbase.vectorstores import IndexType\n",
    "vector_store.create_index(\n",
    "    index_type=IndexType.COMPOSITE,  # Instead of IndexType.HYPERSCALE\n",
    "    index_name=\"cohere_composite_index\",\n",
    "    index_description=\"IVF,SQ8\"\n",
    ")\n",
    "```\n",
    "\n",
    "Choose based on your specific use case and query patterns. For this tutorial's news search scenario, either index type would work, but Hyperscale is more efficient for pure semantic search across news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Cache\n",
    " A cache is set up using Couchbase to store intermediate results and frequently accessed data. Caching is important for improving performance, as it reduces the need to repeatedly calculate or retrieve the same data. The cache is linked to a specific collection in Couchbase, and it is used later in the script to store the results of language model queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cache = CouchbaseCache(\n",
    "        cluster=cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=CACHE_COLLECTION,\n",
    "    )\n",
    "    logging.info(\"Successfully created cache\")\n",
    "    set_llm_cache(cache)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to create cache: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt44X6-bLJOb"
   },
   "source": [
    "## Retrieval-Augmented Generation (RAG) with Couchbase and Langchain\n",
    "Couchbase and LangChain can be seamlessly integrated to create RAG (Retrieval-Augmented Generation) chains, enhancing the process of generating contextually relevant responses. In this setup, Couchbase serves as the vector store, where embeddings of documents are stored. When a query is made, LangChain retrieves the most relevant documents from Couchbase by comparing the query’s embedding with the stored document embeddings. These documents, which provide contextual information, are then passed to a generative language model within LangChain.\n",
    "\n",
    "The language model, equipped with the context from the retrieved documents, generates a response that is both informed and contextually accurate. This integration allows the RAG chain to leverage Couchbase’s efficient storage and retrieval capabilities, while LangChain handles the generation of responses based on the context provided by the retrieved documents. Together, they create a powerful system that can deliver highly relevant and accurate answers by combining the strengths of both retrieval and generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cGJfwS2LI_O"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    template = \"\"\"You are a helpful bot. If you cannot answer based on the context provided, respond with a generic answer. Answer the question as truthfully as possible using the context below:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    logging.info(\"Successfully created RAG chain\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating RAG chain: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvuJyXPUFOux"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "try:\n",
    "    rag_response = rag_chain.invoke(query)\n",
    "    rag_elapsed_time = time.time() - start_time\n",
    "    print(f\"RAG Response: {rag_response}\")\n",
    "    print(f\"RAG response generated in {rag_elapsed_time:.2f} seconds\")\n",
    "except InternalServerFailureException as e:\n",
    "    if \"query request rejected\" in str(e):\n",
    "        print(\"Error: Search request was rejected due to rate limiting. Please try again later.\")\n",
    "    else:\n",
    "        print(f\"Internal server error occurred: {str(e)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUXEVXyxGlv2"
   },
   "source": [
    "## Using Couchbase as a caching mechanism\n",
    "Couchbase can be effectively used as a caching mechanism for RAG (Retrieval-Augmented Generation) responses by storing and retrieving precomputed results for specific queries. This approach enhances the system's efficiency and speed, particularly when dealing with repeated or similar queries. When a query is first processed, the RAG chain retrieves relevant documents, generates a response using the language model, and then stores this response in Couchbase, with the query serving as the key.\n",
    "\n",
    "For subsequent requests with the same query, the system checks Couchbase first. If a cached response is found, it is retrieved directly from Couchbase, bypassing the need to re-run the entire RAG process. This significantly reduces response time because the computationally expensive steps of document retrieval and response generation are skipped. Couchbase's role in this setup is to provide a fast and scalable storage solution for caching these responses, ensuring that frequently asked queries can be answered more quickly and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_PaTD2aGmGt"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    queries = [\n",
    "        \"What happened in the match between Fullham and Liverpool?\",\n",
    "        \"What was manchester city manager pep guardiola's reaction to the team's current form?\", # Repeated query\n",
    "        \"What happened in the match between Fullham and Liverpool?\", # Repeated query\n",
    "    ]\n",
    "\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\nQuery {i}: {query}\")\n",
    "        start_time = time.time()\n",
    "        response = rag_chain.invoke(query)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "except InternalServerFailureException as e:\n",
    "    if \"query request rejected\" in str(e):\n",
    "        print(\"Error: Search request was rejected due to rate limiting. Please try again later.\")\n",
    "    else:\n",
    "        print(f\"Internal server error occurred: {str(e)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've built a high-performance semantic search engine using Couchbase Hyperscale/Composite indexes with Cohere and LangChain. For the Search Vector Index alternative, see the [search_based tutorial](https://developer.couchbase.com/tutorial-cohere-couchbase-rag-with-search-vector-index)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
