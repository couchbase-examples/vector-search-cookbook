{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNdImxzypDlm"
      },
      "source": [
        "# Introduction\n",
        "In this guide, we will walk you through building a powerful semantic search engine using Couchbase as the backend database, [OpenAI](https://openai.com) as the embedding and LLM provider, and [Hugging Face smolagents](https://huggingface.co/docs/smolagents/en/index) as an agent framework. Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval. This tutorial is designed to be beginner-friendly, with clear, step-by-step instructions that will equip you with the knowledge to create a fully functional semantic search system from scratch. Alternatively if you want to perform semantic search using the Couchbase Hyperscale and Composite Vector Index, please take a look at [this.](https://developer.couchbase.com/tutorial-smolagents-couchbase-rag-with-hyperscale-or-composite-vector-index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How to run this tutorial\n",
        "\n",
        "This tutorial is available as a Jupyter Notebook (`.ipynb` file) that you can run interactively.\n",
        "\n",
        "You can either download the notebook file and run it on [Google Colab](https://colab.research.google.com/) or run it on your system by setting up the Python environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Before you start\n",
        "## Get Credentials for OpenAI\n",
        "Please follow the [instructions](https://platform.openai.com/docs/quickstart) to generate the OpenAI credentials.\n",
        "## Create and Deploy Your Free Tier Operational cluster on Capella\n",
        "\n",
        "To get started with Couchbase Capella, create an account and use it to deploy a forever free tier operational cluster. This account provides you with an environment where you can explore and learn about Capella with no time constraint.\n",
        "\n",
        "To learn more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html).\n",
        "\n",
        "### Couchbase Capella Configuration\n",
        "\n",
        "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met.\n",
        "\n",
        "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the required bucket (Read and Write) used in the application.\n",
        "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH2o6pqa69oG"
      },
      "source": [
        "# Setting the Stage: Installing Necessary Libraries\n",
        "To build our semantic search engine, we need a robust set of tools. The libraries we install handle everything from connecting to databases to performing complex machine learning tasks. Each library has a specific role: Couchbase libraries manage database operations, LangChain handles AI model integrations, and OpenAI provides advanced AI models for generating embeddings and understanding natural language. By setting up these libraries, we ensure our environment is equipped to handle the data-intensive and computationally complex tasks required for semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYhPj0Ta8l_A"
      },
      "outputs": [],
      "source": [
        "%pip install --quiet -U datasets==3.5.0 langchain-couchbase==0.3.0 langchain-openai==0.3.13 python-dotenv==1.1.0 smolagents==1.13.0 ipywidgets==8.1.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pp7GtNg8mB9"
      },
      "source": [
        "# Importing Necessary Libraries\n",
        "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading. These libraries provide essential functions for working with data, managing database connections, and processing machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8GzS6tfL8mFP"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "from couchbase.auth import PasswordAuthenticator\n",
        "from couchbase.cluster import Cluster\n",
        "from couchbase.exceptions import (InternalServerFailureException,\n",
        "                                  ServiceUnavailableException,\n",
        "                                  QueryIndexAlreadyExistsException)\n",
        "from couchbase.management.buckets import CreateBucketSettings\n",
        "from couchbase.management.search import SearchIndex\n",
        "from couchbase.options import ClusterOptions\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "from langchain_couchbase.vectorstores import CouchbaseSearchVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "from smolagents import Tool, OpenAIServerModel, ToolCallingAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBnMp5vb8mIb"
      },
      "source": [
        "# Setup Logging\n",
        "Logging is configured to track the progress of the script and capture any errors or warnings. This is crucial for debugging and understanding the flow of execution. The logging output includes timestamps, log levels (e.g., INFO, ERROR), and messages that describe what is happening in the script.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Yv8kWcuf8mLx"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9G5a0en8mPA"
      },
      "source": [
        "# Loading Sensitive Information\n",
        "In this section, we prompt the user to input essential configuration settings needed. These settings include sensitive information like API keys, database credentials, and specific configuration names. Instead of hardcoding these details into the script, we request the user to provide them at runtime, ensuring flexibility and security.\n",
        "\n",
        "The script also validates that all required inputs are provided, raising an error if any crucial information is missing. This approach ensures that your integration is both secure and correctly configured without hardcoding sensitive information, enhancing the overall security and maintainability of your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PFGyHll18mSe"
      },
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or getpass.getpass('Enter your OpenAI API Key: ')\n",
        "\n",
        "CB_HOST = os.getenv('CB_HOST') or input('Enter your Couchbase host (default: couchbase://localhost): ') or 'couchbase://localhost'\n",
        "CB_USERNAME = os.getenv('CB_USERNAME') or input('Enter your Couchbase username (default: Administrator): ') or 'Administrator'\n",
        "CB_PASSWORD = os.getenv('CB_PASSWORD') or getpass.getpass('Enter your Couchbase password (default: password): ') or 'password'\n",
        "CB_BUCKET_NAME = os.getenv('CB_BUCKET_NAME') or input('Enter your Couchbase bucket name (default: vector-search-testing): ') or 'vector-search-testing'\n",
        "INDEX_NAME = os.getenv('INDEX_NAME') or input('Enter your index name (default: vector_search_smolagents): ') or 'vector_search_smolagents'\n",
        "SCOPE_NAME = os.getenv('SCOPE_NAME') or input('Enter your scope name (default: shared): ') or 'shared'\n",
        "COLLECTION_NAME = os.getenv('COLLECTION_NAME') or input('Enter your collection name (default: smolagents): ') or 'smolagents'\n",
        "\n",
        "# Check if the variables are correctly loaded\n",
        "if not OPENAI_API_KEY:\n",
        "    raise ValueError(\"Missing OpenAI API Key\")\n",
        "\n",
        "if 'OPENAI_API_KEY' not in os.environ:\n",
        "    os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtGrYzUY8mV3"
      },
      "source": [
        "# Connecting to the Couchbase Cluster\n",
        "Connecting to a Couchbase cluster is the foundation of our project. Couchbase will serve as our primary data store, handling all the storage and retrieval operations required for our semantic search engine. By establishing this connection, we enable our application to interact with the database, allowing us to perform operations such as storing embeddings, querying data, and managing collections. This connection is the gateway through which all data will flow, so ensuring it's set up correctly is paramount.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Zb3kK-7W8mZK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-28 10:30:17,515 - INFO - Successfully connected to Couchbase\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
        "    options = ClusterOptions(auth)\n",
        "    cluster = Cluster(CB_HOST, options)\n",
        "    cluster.wait_until_ready(timedelta(seconds=5))\n",
        "    logging.info(\"Successfully connected to Couchbase\")\n",
        "except Exception as e:\n",
        "    raise ConnectionError(f\"Failed to connect to Couchbase: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Gpy32N8mcZ"
      },
      "source": [
        "# Setting Up Collections in Couchbase\n",
        "The setup_collection() function handles creating and configuring the hierarchical data organization in Couchbase:\n",
        "\n",
        "1. Bucket Creation:\n",
        "    - Checks if specified bucket exists, creates it if not\n",
        "    - Sets bucket properties like RAM quota (1024MB) and replication (disabled)\n",
        "    - Note: You will not be able to create a bucket on Capella\n",
        "2. Scope Management:\n",
        "    - Verifies if requested scope exists within bucket\n",
        "    - Creates new scope if needed (unless it's the default \"_default\" scope)\n",
        "3. Collection Setup:\n",
        "    - Checks for collection existence within scope\n",
        "    - Creates collection if it doesn't exist\n",
        "    - Waits 2 seconds for collection to be ready\n",
        "\n",
        "Additional Tasks:\n",
        "\n",
        "- Creates primary index on collection for query performance\n",
        "- Clears any existing documents for clean state\n",
        "- Implements comprehensive error handling and logging\n",
        "\n",
        "The function is called twice to set up:\n",
        "\n",
        "1. Main collection for vector embeddings\n",
        "2. Cache collection for storing results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ACZcwUnG8mf2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-28 10:30:20,855 - INFO - Bucket 'vector-search-testing' exists.\n",
            "2025-02-28 10:30:21,350 - INFO - Collection 'smolagents' does not exist. Creating it...\n",
            "2025-02-28 10:30:21,619 - INFO - Collection 'smolagents' created successfully.\n",
            "2025-02-28 10:30:26,886 - INFO - Primary index present or created successfully.\n",
            "2025-02-28 10:30:26,938 - INFO - All documents cleared from the collection.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<couchbase.collection.Collection at 0x10714e610>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def setup_collection(cluster, bucket_name, scope_name, collection_name):\n",
        "    try:\n",
        "        # Check if bucket exists, create if it doesn't\n",
        "        try:\n",
        "            bucket = cluster.bucket(bucket_name)\n",
        "            logging.info(f\"Bucket '{bucket_name}' exists.\")\n",
        "        except Exception as e:\n",
        "            logging.info(f\"Bucket '{bucket_name}' does not exist. Creating it...\")\n",
        "            bucket_settings = CreateBucketSettings(\n",
        "                name=bucket_name,\n",
        "                bucket_type='couchbase',\n",
        "                ram_quota_mb=1024,\n",
        "                flush_enabled=True,\n",
        "                num_replicas=0\n",
        "            )\n",
        "            cluster.buckets().create_bucket(bucket_settings)\n",
        "            time.sleep(2)  # Wait for bucket creation to complete and become available\n",
        "            bucket = cluster.bucket(bucket_name)\n",
        "            logging.info(f\"Bucket '{bucket_name}' created successfully.\")\n",
        "\n",
        "        bucket_manager = bucket.collections()\n",
        "\n",
        "        # Check if scope exists, create if it doesn't\n",
        "        scopes = bucket_manager.get_all_scopes()\n",
        "        scope_exists = any(scope.name == scope_name for scope in scopes)\n",
        "        \n",
        "        if not scope_exists and scope_name != \"_default\":\n",
        "            logging.info(f\"Scope '{scope_name}' does not exist. Creating it...\")\n",
        "            bucket_manager.create_scope(scope_name)\n",
        "            logging.info(f\"Scope '{scope_name}' created successfully.\")\n",
        "\n",
        "        # Check if collection exists, create if it doesn't\n",
        "        collections = bucket_manager.get_all_scopes()\n",
        "        collection_exists = any(\n",
        "            scope.name == scope_name and collection_name in [col.name for col in scope.collections]\n",
        "            for scope in collections\n",
        "        )\n",
        "\n",
        "        if not collection_exists:\n",
        "            logging.info(f\"Collection '{collection_name}' does not exist. Creating it...\")\n",
        "            bucket_manager.create_collection(scope_name, collection_name)\n",
        "            logging.info(f\"Collection '{collection_name}' created successfully.\")\n",
        "        else:\n",
        "            logging.info(f\"Collection '{collection_name}' already exists. Skipping creation.\")\n",
        "\n",
        "        # Wait for collection to be ready\n",
        "        collection = bucket.scope(scope_name).collection(collection_name)\n",
        "        time.sleep(2)  # Give the collection time to be ready for queries\n",
        "\n",
        "        # Ensure primary index exists\n",
        "        try:\n",
        "            cluster.query(f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{bucket_name}`.`{scope_name}`.`{collection_name}`\").execute()\n",
        "            logging.info(\"Primary index present or created successfully.\")\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error creating primary index: {str(e)}\")\n",
        "\n",
        "        # Clear all documents in the collection\n",
        "        try:\n",
        "            query = f\"DELETE FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "            cluster.query(query).execute()\n",
        "            logging.info(\"All documents cleared from the collection.\")\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error while clearing documents: {str(e)}. The collection might be empty.\")\n",
        "\n",
        "        return collection\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error setting up collection: {str(e)}\")\n",
        "    \n",
        "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMJ7RRYp8mjV"
      },
      "source": [
        "# Loading Couchbase Search Vector Index\n",
        "\n",
        "Semantic search requires an efficient way to retrieve relevant documents based on a user's query. This is where the Couchbase **Search Vector Index** comes into play. In this step, we load the Search Vector Index definition from a JSON file, which specifies how the index should be structured. This includes the fields to be indexed, the dimensions of the vectors, and other parameters that determine how the search engine processes queries based on vector similarity.\n",
        "\n",
        "This search vector index configuration requires specific default settings to function properly. This tutorial uses the bucket named `vector-search-testing` with the scope `shared` and collection `smolagents`. The configuration is set up for vectors with exactly `1536 dimensions`, using dot product similarity and optimized for recall. If you want to use a different bucket, scope, or collection, you will need to modify the index configuration accordingly.\n",
        "\n",
        "For more information on creating a  search vector index, please follow the [instructions](https://docs.couchbase.com/cloud/vector-search/create-vector-search-index-ui.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "y7xiCrOc8mmj"
      },
      "outputs": [],
      "source": [
        "# If you are running this script locally (not in Google Colab), uncomment the following line\n",
        "# and provide the path to your index definition file.\n",
        "\n",
        "# index_definition_path = '/path_to_your_index_file/smolagents_index.json'  # Local setup: specify your file path here\n",
        "\n",
        "# # Version for Google Colab\n",
        "# def load_index_definition_colab():\n",
        "#     from google.colab import files\n",
        "#     print(\"Upload your index definition file\")\n",
        "#     uploaded = files.upload()\n",
        "#     index_definition_path = list(uploaded.keys())[0]\n",
        "\n",
        "#     try:\n",
        "#         with open(index_definition_path, 'r') as file:\n",
        "#             index_definition = json.load(file)\n",
        "#         return index_definition\n",
        "#     except Exception as e:\n",
        "#         raise ValueError(f\"Error loading index definition from {index_definition_path}: {str(e)}\")\n",
        "\n",
        "# Version for Local Environment\n",
        "def load_index_definition_local(index_definition_path):\n",
        "    try:\n",
        "        with open(index_definition_path, 'r') as file:\n",
        "            index_definition = json.load(file)\n",
        "        return index_definition\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error loading index definition from {index_definition_path}: {str(e)}\")\n",
        "\n",
        "# Usage\n",
        "# Uncomment the appropriate line based on your environment\n",
        "# index_definition = load_index_definition_colab()\n",
        "index_definition = load_index_definition_local('smolagents_index.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_ddPQ_Y8mpm"
      },
      "source": [
        "# Creating or Updating Search Vector Index\n",
        "\n",
        "With the index definition loaded, the next step is to create or update the **Search Vector Index** in Couchbase. This step is crucial because it optimizes our database for vector similarity search operations, allowing us to perform searches based on the semantic content of documents rather than just keywords. By creating or updating a Search Vector Index, we enable our search engine to handle complex queries that involve finding semantically similar documents using vector embeddings, which is essential for a robust semantic search engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bHEpUu1l8msx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-28 10:30:32,890 - INFO - Creating new index 'vector-search-testing.shared.vector_search_smolagents'...\n",
            "2025-02-28 10:30:33,058 - INFO - Index 'vector-search-testing.shared.vector_search_smolagents' successfully created/updated.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    scope_index_manager = cluster.bucket(CB_BUCKET_NAME).scope(SCOPE_NAME).search_indexes()\n",
        "\n",
        "    # Check if index already exists\n",
        "    existing_indexes = scope_index_manager.get_all_indexes()\n",
        "    index_name = index_definition[\"name\"]\n",
        "\n",
        "    if index_name in [index.name for index in existing_indexes]:\n",
        "        logging.info(f\"Index '{index_name}' found\")\n",
        "    else:\n",
        "        logging.info(f\"Creating new index '{index_name}'...\")\n",
        "\n",
        "    # Create SearchIndex object from JSON definition\n",
        "    search_index = SearchIndex.from_json(index_definition)\n",
        "\n",
        "    # Upsert the index (create if not exists, update if exists)\n",
        "    scope_index_manager.upsert_index(search_index)\n",
        "    logging.info(f\"Index '{index_name}' successfully created/updated.\")\n",
        "\n",
        "except QueryIndexAlreadyExistsException:\n",
        "    logging.info(f\"Index '{index_name}' already exists. Skipping creation/update.\")\n",
        "except ServiceUnavailableException:\n",
        "    raise RuntimeError(\"Search service is not available. Please ensure the Search service is enabled in your Couchbase cluster.\")\n",
        "except InternalServerFailureException as e:\n",
        "    logging.error(f\"Internal server error: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FvxRsg38m3G"
      },
      "source": [
        "# Creating OpenAI Embeddings\n",
        "Embeddings are at the heart of semantic search. They are numerical representations of text that capture the semantic meaning of the words and phrases. Unlike traditional keyword-based search, which looks for exact matches, embeddings allow our search engine to understand the context and nuances of language, enabling it to retrieve documents that are semantically similar to the query, even if they don't contain the exact keywords. By creating embeddings using OpenAI, we equip our search engine with the ability to understand and process natural language in a way that's much closer to how humans understand language. This step transforms our raw text data into a format that the search engine can use to find and rank relevant documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_75ZyCRh8m6m"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-28 10:30:36,983 - INFO - Successfully created OpenAIEmbeddings\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        api_key=OPENAI_API_KEY,\n",
        "    )\n",
        "    logging.info(\"Successfully created OpenAIEmbeddings\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error creating OpenAIEmbeddings: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IwZMUnF8m-N"
      },
      "source": [
        "#  Setting Up the Couchbase Vector Store\n",
        "A vector store is where we'll keep our embeddings. Unlike the FTS index, which is used for text-based search, the vector store is specifically designed to handle embeddings and perform similarity searches. When a user inputs a query, the search engine converts the query into an embedding and compares it against the embeddings stored in the vector store. This allows the engine to find documents that are semantically similar to the query, even if they don't contain the exact same words. By setting up the vector store in Couchbase, we create a powerful tool that enables our search engine to understand and retrieve information based on the meaning and context of the query, rather than just the specific words used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DwIJQjYT9RV_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-28 10:30:40,503 - INFO - Successfully created vector store\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    vector_store = CouchbaseSearchVectorStore(\n",
        "        cluster=cluster,\n",
        "        bucket_name=CB_BUCKET_NAME,\n",
        "        scope_name=SCOPE_NAME,\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        embedding=embeddings,\n",
        "        index_name=INDEX_NAME,\n",
        "    )\n",
        "    logging.info(\"Successfully created vector store\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Failed to create vector store: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load the BBC News Dataset\n",
        "To build a search engine, we need data to search through. We use the BBC News dataset from RealTimeData, which provides real-world news articles. This dataset contains news articles from BBC covering various topics and time periods. Loading the dataset is a crucial step because it provides the raw material that our search engine will work with. The quality and diversity of the news articles make it an excellent choice for testing and refining our search engine, ensuring it can handle real-world news content effectively.\n",
        "\n",
        "The BBC News dataset allows us to work with authentic news articles, enabling us to build and test a search engine that can effectively process and retrieve relevant news content. The dataset is loaded using the Hugging Face datasets library, specifically accessing the \"RealTimeData/bbc_news_alltime\" dataset with the \"2024-12\" version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-28 10:30:51,981 - INFO - Successfully loaded the BBC News dataset with 2687 rows.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded the BBC News dataset with 2687 rows\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    news_dataset = load_dataset(\n",
        "        \"RealTimeData/bbc_news_alltime\", \"2024-12\", split=\"train\"\n",
        "    )\n",
        "    print(f\"Loaded the BBC News dataset with {len(news_dataset)} rows\")\n",
        "    logging.info(f\"Successfully loaded the BBC News dataset with {len(news_dataset)} rows.\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error loading the BBC News dataset: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleaning up the Data\n",
        "We will use the content of the news articles for our RAG system.\n",
        "\n",
        "The dataset contains a few duplicate records. We are removing them to avoid duplicate results in the retrieval stage of our RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have 1749 unique articles in our database.\n"
          ]
        }
      ],
      "source": [
        "news_articles = news_dataset[\"content\"]\n",
        "unique_articles = set()\n",
        "for article in news_articles:\n",
        "    if article:\n",
        "        unique_articles.add(article)\n",
        "unique_news_articles = list(unique_articles)\n",
        "print(f\"We have {len(unique_news_articles)} unique articles in our database.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving Data to the Vector Store\n",
        "To efficiently handle the large number of articles, we process them in batches of articles at a time. This batch processing approach helps manage memory usage and provides better control over the ingestion process.\n",
        "\n",
        "We first filter out any articles that exceed 50,000 characters to avoid potential issues with token limits. Then, using the vector store's add_texts method, we add the filtered articles to our vector database. The batch_size parameter controls how many articles are processed in each iteration.\n",
        "\n",
        "This approach offers several benefits:\n",
        "\n",
        "1. Memory Efficiency: Processing in smaller batches prevents memory overload\n",
        "2. Error Handling: If an error occurs, only the current batch is affected\n",
        "3. Progress Tracking: Easier to monitor and track the ingestion progress\n",
        "4. Resource Management: Better control over CPU and network resource utilization\n",
        "\n",
        "We use a conservative batch size of 100 to ensure reliable operation. The optimal batch size depends on many factors including:\n",
        "\n",
        "- Document sizes being inserted\n",
        "- Available system resources\n",
        "- Network conditions\n",
        "- Concurrent workload\n",
        "\n",
        "Consider measuring performance with your specific workload before adjusting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the current logging level\n",
        "current_logging_level = logging.getLogger().getEffectiveLevel()\n",
        "\n",
        "# # Set logging level to CRITICAL to suppress lower level logs\n",
        "logging.getLogger().setLevel(logging.CRITICAL)\n",
        "\n",
        "articles = [article for article in unique_news_articles if article and len(article) <= 50000]\n",
        "\n",
        "try:\n",
        "    vector_store.add_texts(\n",
        "        texts=articles,\n",
        "        batch_size=100\n",
        "    )\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Failed to save documents to vector store: {str(e)}\")\n",
        "\n",
        "# Restore the original logging level\n",
        "logging.getLogger().setLevel(current_logging_level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# smolagents: An Introduction\n",
        "[smolagents](https://huggingface.co/docs/smolagents/en/index) is a agentic framework by Hugging Face for easy creation of agents in a few lines of code.\n",
        "\n",
        "Some of the features of smolagents are:\n",
        "\n",
        "- âœ¨ Simplicity: the logic for agents fits in ~1,000 lines of code (see agents.py). We kept abstractions to their minimal shape above raw code!\n",
        "\n",
        "- ğŸ§‘â€ğŸ’» First-class support for Code Agents. Our CodeAgent writes its actions in code (as opposed to \"agents being used to write code\"). To make it secure, we support executing in sandboxed environments via E2B.\n",
        "\n",
        "- ğŸ¤— Hub integrations: you can share/pull tools to/from the Hub, and more is to come!\n",
        "\n",
        "- ğŸŒ Model-agnostic: smolagents supports any LLM. It can be a local transformers or ollama model, one of many providers on the Hub, or any model from OpenAI, Anthropic and many others via our LiteLLM integration.\n",
        "\n",
        "- ğŸ‘ï¸ Modality-agnostic: Agents support text, vision, video, even audio inputs! Cf this tutorial for vision.\n",
        "\n",
        "- ğŸ› ï¸ Tool-agnostic: you can use tools from LangChain, Anthropic's MCP, you can even use a Hub Space as a tool.\n",
        "\n",
        "# Building a RAG Agent using smolagents\n",
        "\n",
        "smolagents allows users to define their own tools for the agent to use. These tools can be of two types:\n",
        "1. Tools defined as classes: These tools are subclassed from the `Tool` class and must override the `forward` method, which is called when the tool is used.\n",
        "2. Tools defined as functions: These are simple functions that are called when the tool is used, and are decorated with the `@tool` decorator.\n",
        "\n",
        "In our case, we will use the first method, and we define our `RetrieverTool` below. We define a name, a description and a dictionary of inputs that the tool accepts. This helps the LLM properly identify and use the tool.\n",
        "\n",
        "The `RetrieverTool` is simple: it takes a query generated by the user, and uses Couchbase's performant vector search service under the hood to search for semantically similar documents to the query. The LLM can then use this context to answer the user's question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RetrieverTool(Tool):\n",
        "    name = \"retriever\"\n",
        "    description = \"Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query.\"\n",
        "    inputs = {\n",
        "        \"query\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, vector_store: CouchbaseSearchVectorStore, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vector_store = vector_store\n",
        "\n",
        "    def forward(self, query: str) -> str:\n",
        "        assert isinstance(query, str), \"Query must be a string\"\n",
        "\n",
        "        docs = self.vector_store.similarity_search_with_score(query, k=5)\n",
        "        return \"\\n\\n\".join(\n",
        "            f\"# Documents:\\n{doc.page_content}\"\n",
        "            for doc, score in docs\n",
        "        )\n",
        "\n",
        "retriever_tool = RetrieverTool(vector_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Defining Our Agent\n",
        "smolagents have predefined configurations for agents that we can use. We use the `ToolCallingAgent`, which writes its tool calls in a JSON format. Alternatively, there also exists a `CodeAgent`, in which the LLM defines it's functions in code.\n",
        "\n",
        "The `CodeAgent` is offers benefits in certain challenging scenarios: it can lead to [higher performance in difficult benchmarks](https://huggingface.co/papers/2411.01747) and use [30% fewer steps to solve problems](https://huggingface.co/papers/2402.01030). However, since our use case is just a simple RAG tool, a `ToolCallingAgent` will suffice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = ToolCallingAgent(\n",
        "    tools=[retriever_tool],\n",
        "    model=OpenAIServerModel(\n",
        "        model_id=\"gpt-4o-2024-08-06\",\n",
        "        api_key=OPENAI_API_KEY,\n",
        "    ),\n",
        "    max_steps=4,\n",
        "    verbosity_level=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Running our Agent\n",
        "We have now finished setting up our vector store and agent! The system is now ready to accept queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span> <span style=\"font-weight: bold\">What was manchester city manager pep guardiola's reaction to the team's current form?</span>                           <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”‚</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">â•°â”€ OpenAIServerModel - gpt-4o-2024-08-06 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[38;2;212;183;2mâ•­â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â•®\u001b[0m\n",
              "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                 \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
              "\u001b[38;2;212;183;2mâ”‚\u001b[0m \u001b[1mWhat was manchester city manager pep guardiola's reaction to the team's current form?\u001b[0m                           \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
              "\u001b[38;2;212;183;2mâ”‚\u001b[0m                                                                                                                 \u001b[38;2;212;183;2mâ”‚\u001b[0m\n",
              "\u001b[38;2;212;183;2mâ•°â”€\u001b[0m\u001b[38;2;212;183;2m OpenAIServerModel - gpt-4o-2024-08-06 \u001b[0m\u001b[38;2;212;183;2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[38;2;212;183;2mâ”€â•¯\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-28 10:32:28,032 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚ Calling tool: 'retriever' with arguments: {'query': \"Pep Guardiola's reaction to Manchester City's current      â”‚\n",
              "â”‚ form\"}                                                                                                          â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚ Calling tool: 'retriever' with arguments: {'query': \"Pep Guardiola's reaction to Manchester City's current      â”‚\n",
              "â”‚ form\"}                                                                                                          â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-28 10:32:28,466 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 0: Duration 2.25 seconds| Input tokens: 1,010 | Output tokens: 23]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2m[Step 0: Duration 2.25 seconds| Input tokens: 1,010 | Output tokens: 23]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[38;2;212;183;2mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-28 10:32:31,724 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚ Calling tool: 'final_answer' with arguments: {'answer': 'Manchester City manager Pep Guardiola has expressed a  â”‚\n",
              "â”‚ mix of concern and determination regarding the team\\'s current form. Guardiola admitted that this is the worst  â”‚\n",
              "â”‚ run of results in his managerial career and that it has affected his sleep and diet. He described his state of  â”‚\n",
              "â”‚ mind as \"ugly\" and acknowledged that City needs to defend better and avoid making mistakes. Despite his         â”‚\n",
              "â”‚ personal challenges, Guardiola stated that he is \"fine\" and focused on finding solutions.\\n\\nGuardiola also     â”‚\n",
              "â”‚ took responsibility for the team\\'s struggles, stating he is \"not good enough\" and has to find solutions. He    â”‚\n",
              "â”‚ expressed self-doubt but is striving to improve the team\\'s situation step by step. Guardiola has faced         â”‚\n",
              "â”‚ criticism due to the team\\'s poor form, which has seen them lose several matches and fall behind in the title   â”‚\n",
              "â”‚ race.\\n\\nHe emphasized the need to restore their defensive strength and regain confidence in their play.        â”‚\n",
              "â”‚ Guardiola is planning a significant rebuild of the squad to address these challenges, aiming to replace several â”‚\n",
              "â”‚ regular starters and emphasize improvements in the team\\'s intensity and defensive concepts.'}                  â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
              "â”‚ Calling tool: 'final_answer' with arguments: {'answer': 'Manchester City manager Pep Guardiola has expressed a  â”‚\n",
              "â”‚ mix of concern and determination regarding the team\\'s current form. Guardiola admitted that this is the worst  â”‚\n",
              "â”‚ run of results in his managerial career and that it has affected his sleep and diet. He described his state of  â”‚\n",
              "â”‚ mind as \"ugly\" and acknowledged that City needs to defend better and avoid making mistakes. Despite his         â”‚\n",
              "â”‚ personal challenges, Guardiola stated that he is \"fine\" and focused on finding solutions.\\n\\nGuardiola also     â”‚\n",
              "â”‚ took responsibility for the team\\'s struggles, stating he is \"not good enough\" and has to find solutions. He    â”‚\n",
              "â”‚ expressed self-doubt but is striving to improve the team\\'s situation step by step. Guardiola has faced         â”‚\n",
              "â”‚ criticism due to the team\\'s poor form, which has seen them lose several matches and fall behind in the title   â”‚\n",
              "â”‚ race.\\n\\nHe emphasized the need to restore their defensive strength and regain confidence in their play.        â”‚\n",
              "â”‚ Guardiola is planning a significant rebuild of the squad to address these challenges, aiming to replace several â”‚\n",
              "â”‚ regular starters and emphasize improvements in the team\\'s intensity and defensive concepts.'}                  â”‚\n",
              "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: Manchester City manager Pep Guardiola has expressed a mix of concern and determination regarding the </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">team's current form. Guardiola admitted that this is the worst run of results in his managerial career and that it </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">has affected his sleep and diet. He described his state of mind as \"ugly\" and acknowledged that City needs to </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">defend better and avoid making mistakes. Despite his personal challenges, Guardiola stated that he is \"fine\" and </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">focused on finding solutions.</span>\n",
              "\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Guardiola also took responsibility for the team's struggles, stating he is \"not good enough\" and has to find </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">solutions. He expressed self-doubt but is striving to improve the team's situation step by step. Guardiola has </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">faced criticism due to the team's poor form, which has seen them lose several matches and fall behind in the title </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">race.</span>\n",
              "\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">He emphasized the need to restore their defensive strength and regain confidence in their play. Guardiola is </span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">planning a significant rebuild of the squad to address these challenges, aiming to replace several regular starters</span>\n",
              "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">and emphasize improvements in the team's intensity and defensive concepts.</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;212;183;2mFinal answer: Manchester City manager Pep Guardiola has expressed a mix of concern and determination regarding the \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mteam's current form. Guardiola admitted that this is the worst run of results in his managerial career and that it \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mhas affected his sleep and diet. He described his state of mind as \"ugly\" and acknowledged that City needs to \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mdefend better and avoid making mistakes. Despite his personal challenges, Guardiola stated that he is \"fine\" and \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mfocused on finding solutions.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;212;183;2mGuardiola also took responsibility for the team's struggles, stating he is \"not good enough\" and has to find \u001b[0m\n",
              "\u001b[1;38;2;212;183;2msolutions. He expressed self-doubt but is striving to improve the team's situation step by step. Guardiola has \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mfaced criticism due to the team's poor form, which has seen them lose several matches and fall behind in the title \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mrace.\u001b[0m\n",
              "\n",
              "\u001b[1;38;2;212;183;2mHe emphasized the need to restore their defensive strength and regain confidence in their play. Guardiola is \u001b[0m\n",
              "\u001b[1;38;2;212;183;2mplanning a significant rebuild of the squad to address these challenges, aiming to replace several regular starters\u001b[0m\n",
              "\u001b[1;38;2;212;183;2mand emphasize improvements in the team's intensity and defensive concepts.\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.74 seconds| Input tokens: 7,162 | Output tokens: 241]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2m[Step 1: Duration 2.74 seconds| Input tokens: 7,162 | Output tokens: 241]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"What was manchester city manager pep guardiola's reaction to the team's current form?\"\n",
        "\n",
        "agent_output = agent.run(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyzing the Agent\n",
        "When the agent runs, smolagents prints out the steps that the agent takes along with the tools called in each step. In the above tool call, two steps occur:\n",
        "\n",
        "**Step 1**: First, the agent determines that it requires a tool to be used, and the `retriever` tool is called. The agent also specifies the query parameter for the tool (a string). The tool returns semantically similar documents to the query from Couchbase's vector store.\n",
        "\n",
        "**Step 2**: Next, the agent determines that the context retrieved from the tool is sufficient to answer the question. It then calls the `final_answer` tool, which is predefined for each agent: this tool is called when the agent returns the final answer to the user. In this step, the LLM answers the user's query from the context retrieved in step 1 and passes it to the `final_answer` tool, at which point the agent's execution ends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n",
        "\n",
        "By following these steps, youâ€™ll have a fully functional agentic RAG system that leverages the strengths of Couchbase and smolagents, along with OpenAI. This guide is designed not just to show you how to build the system, but also to explain why each step is necessary, giving you a deeper understanding of the principles behind semantic search and how to implement it effectively. Whether youâ€™re a newcomer to software development or an experienced developer looking to expand your skills, this guide will provide you with the knowledge and tools you need to create a powerful, RAG-driven chat system."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
