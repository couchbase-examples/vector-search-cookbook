{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedrock Agents with Lambda Functions for Actions (Experimental Approach)\n",
    "\n",
    "This notebook demonstrates an experimental approach to using AWS Bedrock Agents where the agent's actions (Action Groups) are implemented as separate AWS Lambda functions. This contrasts with the `lamda-approach` directory where the core logic resides within the notebook itself.\n",
    "\n",
    "**Goal:** Create a Bedrock Agent that can:\n",
    "1.  **Research:** Use a Couchbase vector store (populated with data) to find relevant documents based on a user query (via `bedrock_agent_researcher` Lambda).\n",
    "2.  **Write/Format:** Format the research findings using a Bedrock LLM (via `bedrock_agent_writer` Lambda).\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1.  **Configuration:** Set up environment variables and paths.\n",
    "2.  **AWS & Couchbase Setup:** Initialize AWS clients and connect to Couchbase.\n",
    "3.  **Couchbase Resources:** Create/verify the necessary Couchbase bucket, scope, collection, and search index.\n",
    "4.  **Data Loading:** Load documents into the Couchbase vector store.\n",
    "5.  **IAM Role:** Create an IAM role for the Bedrock Agent and Lambda functions.\n",
    "6.  **Lambda Deployment:** Package and deploy the `researcher` and `writer` Lambda functions.\n",
    "7.  **Agent Creation:** Define and create the Bedrock Agent.\n",
    "8.  **Action Group Creation:** Create action groups linking the agent to the deployed Lambda functions using OpenAPI schemas.\n",
    "9.  **Agent Preparation:** Prepare the agent, making it ready for invocation.\n",
    "10. **Agent Invocation:** Test the agent with a sample prompt.\n",
    "11. **Cleanup (Optional):** Provide steps to delete the created AWS resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration\n",
    "\n",
    "Import necessary libraries and load configuration from environment variables or a `.env` file. Ensure you have a `.env` file in the `awsbedrock-agents/lambda-experiments/` directory with your AWS credentials, Couchbase details, and AWS Account ID, or set these as environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 07:48:26,060 - INFO - Attempting to load .env file from: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/.env\n",
      "2025-05-01 07:48:26,062 - INFO - .env file loaded successfully.\n",
      "2025-05-01 07:48:26,063 - INFO - All required environment variables are set.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import traceback\n",
    "import uuid\n",
    "from datetime import timedelta\n",
    "import shutil\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.exceptions import (\n",
    "    BucketNotFoundException,\n",
    "    CollectionNotFoundException,\n",
    "    CouchbaseException,\n",
    "    InternalServerFailureException,\n",
    "    QueryIndexAlreadyExistsException,\n",
    "    ScopeNotFoundException,\n",
    "    ServiceUnavailableException,\n",
    "    SearchIndexNotFoundException\n",
    ")\n",
    "from couchbase.management.buckets import (\n",
    "    BucketSettings, BucketType,\n",
    "    CreateBucketSettings\n",
    ")\n",
    "from couchbase.management.collections import CollectionSpec\n",
    "from couchbase.management.search import SearchIndex, SearchIndexManager\n",
    "from couchbase.options import ClusterOptions, QueryOptions\n",
    "from dotenv import load_dotenv\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_couchbase.vectorstores import CouchbaseSearchVectorStore\n",
    "from botocore.config import Config\n",
    "from botocore.waiter import WaiterModel, create_waiter_with_client\n",
    "\n",
    "# --- Configuration ---\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables from notebook's directory .env\n",
    "dotenv_path = os.path.join(os.getcwd(), '.env') # Assumes .env is where notebook runs\n",
    "logger.info(f\"Attempting to load .env file from: {dotenv_path}\")\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    logger.info(\".env file loaded successfully.\")\n",
    "else:\n",
    "    logger.warning(f\".env file not found at {dotenv_path}. Relying on environment variables.\")\n",
    "\n",
    "# Couchbase Configuration\n",
    "CB_HOST = os.getenv(\"CB_HOST\", \"couchbase://localhost\")\n",
    "CB_USERNAME = os.getenv(\"CB_USERNAME\", \"Administrator\")\n",
    "CB_PASSWORD = os.getenv(\"CB_PASSWORD\") # Ensure this is set in .env\n",
    "CB_BUCKET_NAME = os.getenv(\"CB_BUCKET_NAME\", \"vector-search-exp\")\n",
    "SCOPE_NAME = os.getenv(\"SCOPE_NAME\", \"bedrock_exp\")\n",
    "COLLECTION_NAME = os.getenv(\"COLLECTION_NAME\", \"docs_exp\")\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\", \"vector_search_bedrock_exp\")\n",
    "\n",
    "# AWS Configuration\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\") # Ensure this is set\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\") # Ensure this is set\n",
    "AWS_ACCOUNT_ID = os.getenv(\"AWS_ACCOUNT_ID\") # Ensure this is set\n",
    "\n",
    "# Bedrock Model IDs\n",
    "EMBEDDING_MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
    "AGENT_MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\" # Using Sonnet for the agent\n",
    "\n",
    "# Paths (relative to this notebook's location)\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "SCHEMAS_DIR = os.path.join(NOTEBOOK_DIR, 'schemas')\n",
    "LAMBDA_FUNCTIONS_DIR = os.path.join(NOTEBOOK_DIR, 'lambda_functions')\n",
    "RESEARCHER_SCHEMA_PATH = os.path.join(SCHEMAS_DIR, 'researcher_schema.json')\n",
    "WRITER_SCHEMA_PATH = os.path.join(SCHEMAS_DIR, 'writer_schema.json')\n",
    "INDEX_JSON_PATH = os.path.join(NOTEBOOK_DIR, 'aws_index.json')\n",
    "DOCS_JSON_PATH = os.path.join(NOTEBOOK_DIR, 'documents.json')\n",
    "\n",
    "# --- Check Environment Variables ---\n",
    "def check_environment_variables():\n",
    "    \"\"\"Check if required environment variables are set.\"\"\"\n",
    "    required_vars = [\"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\", \"AWS_ACCOUNT_ID\", \"CB_PASSWORD\"]\n",
    "    missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "    if missing_vars:\n",
    "        logger.error(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "        logger.error(\"Please set these variables in your environment or .env file\")\n",
    "        return False\n",
    "    logger.info(\"All required environment variables are set.\")\n",
    "    return True\n",
    "\n",
    "if not check_environment_variables():\n",
    "    raise EnvironmentError(\"Missing required environment variables. Please check configuration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize AWS Clients and Connect to Couchbase\n",
    "\n",
    "Create the necessary Boto3 clients for interacting with AWS services (Bedrock Runtime, IAM, Lambda, Bedrock Agent, Bedrock Agent Runtime) and establish a connection to the Couchbase cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 07:48:26,070 - INFO - Initializing AWS clients in region: us-east-1\n",
      "2025-05-01 07:48:26,420 - INFO - AWS clients initialized successfully.\n",
      "2025-05-01 07:48:26,421 - INFO - Connecting to Couchbase cluster at couchbases://cb.hlcup4o4jmjr55yf.cloud.couchbase.com...\n",
      "2025-05-01 07:48:28,457 - INFO - Successfully connected to Couchbase.\n"
     ]
    }
   ],
   "source": [
    "def initialize_aws_clients():\n",
    "    \"\"\"Initialize required AWS clients.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Initializing AWS clients in region: {AWS_REGION}\")\n",
    "        session = boto3.Session(\n",
    "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "            region_name=AWS_REGION\n",
    "        )\n",
    "        # Use a config with longer timeouts for agent operations\n",
    "        agent_config = Config(\n",
    "            connect_timeout=120,\n",
    "            read_timeout=600, # Agent preparation can take time\n",
    "            retries={'max_attempts': 5, 'mode': 'adaptive'}\n",
    "        )\n",
    "        bedrock_runtime = session.client('bedrock-runtime', region_name=AWS_REGION)\n",
    "        iam_client = session.client('iam', region_name=AWS_REGION)\n",
    "        lambda_client = session.client('lambda', region_name=AWS_REGION)\n",
    "        bedrock_agent_client = session.client('bedrock-agent', region_name=AWS_REGION, config=agent_config)\n",
    "        bedrock_agent_runtime_client = session.client('bedrock-agent-runtime', region_name=AWS_REGION, config=agent_config)\n",
    "        logger.info(\"AWS clients initialized successfully.\")\n",
    "        return bedrock_runtime, iam_client, lambda_client, bedrock_agent_client, bedrock_agent_runtime_client\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing AWS clients: {e}\")\n",
    "        raise\n",
    "\n",
    "def connect_couchbase():\n",
    "    \"\"\"Connect to Couchbase cluster.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Connecting to Couchbase cluster at {CB_HOST}...\")\n",
    "        auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "        options = ClusterOptions(auth)\n",
    "        cluster = Cluster(CB_HOST, options)\n",
    "        cluster.wait_until_ready(timedelta(seconds=10))\n",
    "        logger.info(\"Successfully connected to Couchbase.\")\n",
    "        return cluster\n",
    "    except CouchbaseException as e:\n",
    "        logger.error(f\"Couchbase connection error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error connecting to Couchbase: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialize clients\n",
    "bedrock_runtime_client, iam_client, lambda_client, bedrock_agent_client, bedrock_agent_runtime_client = initialize_aws_clients()\n",
    "cb_cluster = connect_couchbase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Couchbase Bucket, Scope, Collection, and Search Index\n",
    "\n",
    "Define functions to create the Couchbase bucket, scope, and collection if they don't exist. Also, create or update the vector search index required by the `CouchbaseSearchVectorStore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 07:48:28,476 - INFO - Setting up collection: vector-search-testing/shared/bedrock\n",
      "2025-05-01 07:48:29,568 - INFO - Bucket 'vector-search-testing' exists.\n",
      "2025-05-01 07:48:30,502 - INFO - Scope 'shared' already exists.\n",
      "2025-05-01 07:48:31,415 - INFO - Collection 'bedrock' already exists.\n",
      "2025-05-01 07:48:31,417 - INFO - Ensuring primary index exists on `vector-search-testing`.`shared`.`bedrock`...\n",
      "2025-05-01 07:48:32,342 - INFO - Primary index present or created successfully.\n",
      "2025-05-01 07:48:32,343 - INFO - Collection setup complete.\n",
      "2025-05-01 07:48:32,344 - INFO - Looking for index definition at: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lamda-approach/aws_index.json\n",
      "2025-05-01 07:48:32,348 - INFO - Loaded index definition, ensuring name is 'vector_search_bedrock' and source is 'vector-search-testing'.\n",
      "2025-05-01 07:48:32,349 - INFO - Upserting search index 'vector_search_bedrock'...\n",
      "2025-05-01 07:48:33,036 - WARNING - Search index 'vector_search_bedrock' likely already existed (caught QueryIndexAlreadyExistsException, check if applicable). Upsert attempted.\n"
     ]
    }
   ],
   "source": [
    "def setup_collection(cluster, bucket_name, scope_name, collection_name):\n",
    "    \"\"\"Set up Couchbase collection.\"\"\"\n",
    "    logger.info(f\"Setting up collection: {bucket_name}/{scope_name}/{collection_name}\")\n",
    "    try:\n",
    "        # Check/Create Bucket\n",
    "        try:\n",
    "            bucket = cluster.bucket(bucket_name)\n",
    "            logger.info(f\"Bucket '{bucket_name}' exists.\")\n",
    "        except BucketNotFoundException:\n",
    "            logger.info(f\"Bucket '{bucket_name}' does not exist. Creating it...\")\n",
    "            bucket_settings = BucketSettings(\n",
    "                name=bucket_name,\n",
    "                bucket_type=BucketType.COUCHBASE,\n",
    "                ram_quota_mb=256,\n",
    "                flush_enabled=True,\n",
    "                num_replicas=0\n",
    "            )\n",
    "            try:\n",
    "                 cluster.buckets().create_bucket(bucket_settings)\n",
    "                 logger.info(f\"Bucket '{bucket_name}' created. Waiting for ready state (10s)...\")\n",
    "                 time.sleep(10)\n",
    "                 bucket = cluster.bucket(bucket_name)\n",
    "            except Exception as create_e:\n",
    "                 logger.error(f\"Failed to create bucket '{bucket_name}': {create_e}\")\n",
    "                 raise\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error getting bucket '{bucket_name}': {e}\")\n",
    "             raise\n",
    "\n",
    "        bucket_manager = bucket.collections()\n",
    "\n",
    "        # Check/Create Scope\n",
    "        scopes = bucket_manager.get_all_scopes()\n",
    "        scope_exists = any(s.name == scope_name for s in scopes)\n",
    "        if not scope_exists:\n",
    "            logger.info(f\"Scope '{scope_name}' does not exist. Creating it...\")\n",
    "            try:\n",
    "                 bucket_manager.create_scope(scope_name)\n",
    "                 logger.info(f\"Scope '{scope_name}' created. Waiting (2s)...\")\n",
    "                 time.sleep(2)\n",
    "            except CouchbaseException as e:\n",
    "                 if \"already exists\" in str(e).lower() or \"scope_exists\" in str(e).lower():\n",
    "                      logger.info(f\"Scope '{scope_name}' likely already exists.\")\n",
    "                 else:\n",
    "                      logger.error(f\"Failed to create scope '{scope_name}': {e}\")\n",
    "                      raise\n",
    "        else:\n",
    "             logger.info(f\"Scope '{scope_name}' already exists.\")\n",
    "\n",
    "        # Check/Create Collection\n",
    "        scopes = bucket_manager.get_all_scopes() # Re-fetch\n",
    "        collection_exists = False\n",
    "        for s in scopes:\n",
    "             if s.name == scope_name:\n",
    "                  if any(c.name == collection_name for c in s.collections):\n",
    "                       collection_exists = True\n",
    "                       break\n",
    "        if not collection_exists:\n",
    "            logger.info(f\"Collection '{collection_name}' does not exist. Creating it...\")\n",
    "            try:\n",
    "                collection_spec = CollectionSpec(collection_name, scope_name)\n",
    "                bucket_manager.create_collection(collection_spec)\n",
    "                logger.info(f\"Collection '{collection_name}' created. Waiting (2s)...\")\n",
    "                time.sleep(2)\n",
    "            except CouchbaseException as e:\n",
    "                 if \"already exists\" in str(e).lower() or \"collection_exists\" in str(e).lower():\n",
    "                     logger.info(f\"Collection '{collection_name}' likely already exists.\")\n",
    "                 else:\n",
    "                     logger.error(f\"Failed to create collection '{collection_name}': {e}\")\n",
    "                     raise\n",
    "        else:\n",
    "            logger.info(f\"Collection '{collection_name}' already exists.\")\n",
    "\n",
    "        # Ensure primary index exists\n",
    "        try:\n",
    "            logger.info(f\"Ensuring primary index exists on `{bucket_name}`.`{scope_name}`.`{collection_name}`...\")\n",
    "            cluster.query(f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{bucket_name}`.`{scope_name}`.`{collection_name}`\").execute()\n",
    "            logger.info(\"Primary index present or created successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating primary index: {str(e)}\")\n",
    "\n",
    "        logger.info(\"Collection setup complete.\")\n",
    "        return cluster.bucket(bucket_name).scope(scope_name).collection(collection_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up collection: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "def setup_search_index(cluster, index_name, bucket_name, scope_name, collection_name, index_definition_path):\n",
    "    \"\"\"Set up search indexes.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Looking for index definition at: {index_definition_path}\")\n",
    "        if not os.path.exists(index_definition_path):\n",
    "             raise FileNotFoundError(f\"Index definition file not found: {index_definition_path}\")\n",
    "\n",
    "        with open(index_definition_path, 'r') as file:\n",
    "            index_definition = json.load(file)\n",
    "            index_definition['name'] = index_name\n",
    "            index_definition['sourceName'] = bucket_name\n",
    "            logger.info(f\"Loaded index definition, ensuring name is '{index_name}' and source is '{bucket_name}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading index definition: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        search_index_manager = cluster.search_indexes()\n",
    "        search_index = SearchIndex.from_json(index_definition)\n",
    "        logger.info(f\"Upserting search index '{index_name}'...\")\n",
    "        search_index_manager.upsert_index(search_index)\n",
    "        logger.info(f\"Index '{index_name}' upsert submitted. Waiting for indexing (10s)...\")\n",
    "        time.sleep(10)\n",
    "        logger.info(f\"Search index '{index_name}' setup complete.\")\n",
    "        \n",
    "    except QueryIndexAlreadyExistsException:\n",
    "        logger.warning(f\"Search index '{index_name}' likely already existed (caught QueryIndexAlreadyExistsException, check if applicable). Upsert attempted.\")\n",
    "\n",
    "    except CouchbaseException as e:\n",
    "        logger.error(f\"Couchbase error during search index setup for '{index_name}': {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during search index setup for '{index_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "def clear_collection(cluster, bucket_name, scope_name, collection_name):\n",
    "    \"\"\"Delete all documents from the specified collection.\"\"\"\n",
    "    try:\n",
    "        logger.warning(f\"Attempting to clear all documents from `{bucket_name}`.`{scope_name}`.`{collection_name}`...\")\n",
    "        query = f\"DELETE FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
    "        result = cluster.query(query).execute()\n",
    "        mutation_count = 0\n",
    "        try:\n",
    "             metrics_data = result.meta_data().metrics()\n",
    "             if metrics_data:\n",
    "                  mutation_count = metrics_data.mutation_count()\n",
    "        except Exception as metrics_e:\n",
    "             logger.warning(f\"Could not retrieve mutation count after delete: {metrics_e}\")\n",
    "        logger.info(f\"Successfully cleared documents (approx. {mutation_count} mutations).\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error clearing documents: {e}. Collection might be empty or index not ready.\")\n",
    "\n",
    "# Execute setup\n",
    "cb_collection = setup_collection(cb_cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)\n",
    "setup_search_index(cb_cluster, INDEX_NAME, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME, INDEX_JSON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Vector Store and Load Data\n",
    "\n",
    "Clear any existing data from the collection, initialize the `CouchbaseSearchVectorStore` using the Bedrock embeddings client, and load documents from the `documents.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 07:48:33,047 - WARNING - Attempting to clear all documents from `vector-search-testing`.`shared`.`bedrock`...\n",
      "2025-05-01 07:48:33,273 - WARNING - Could not retrieve mutation count after delete: 'list' object has no attribute 'meta_data'\n",
      "2025-05-01 07:48:33,274 - INFO - Successfully cleared documents (approx. 0 mutations).\n",
      "2025-05-01 07:48:33,276 - INFO - Initializing Bedrock Embeddings client with model: amazon.titan-embed-text-v2:0\n",
      "2025-05-01 07:48:33,277 - INFO - Successfully created Bedrock embeddings client.\n",
      "2025-05-01 07:48:33,278 - INFO - Initializing CouchbaseSearchVectorStore with index: vector_search_bedrock\n",
      "2025-05-01 07:48:36,745 - INFO - Successfully created Couchbase vector store.\n",
      "2025-05-01 07:48:36,747 - INFO - Looking for documents at: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lamda-approach/documents.json\n",
      "2025-05-01 07:48:36,749 - INFO - Loaded 7 documents from /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lamda-approach/documents.json\n",
      "2025-05-01 07:48:36,749 - INFO - Adding 7 documents to vector store...\n",
      "2025-05-01 07:48:40,575 - INFO - Successfully added 7 documents to the vector store.\n",
      "2025-05-01 07:48:40,575 - INFO - Waiting briefly for vector indexing...\n",
      "2025-05-01 07:48:45,592 - INFO - --- Couchbase Setup and Data Loading Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Clear existing documents first\n",
    "clear_collection(cb_cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)\n",
    "\n",
    "try:\n",
    "    logger.info(f\"Initializing Bedrock Embeddings client with model: {EMBEDDING_MODEL_ID}\")\n",
    "    embeddings = BedrockEmbeddings(\n",
    "        client=bedrock_runtime_client,\n",
    "        model_id=EMBEDDING_MODEL_ID\n",
    "    )\n",
    "    logger.info(\"Successfully created Bedrock embeddings client.\")\n",
    "\n",
    "    logger.info(f\"Initializing CouchbaseSearchVectorStore with index: {INDEX_NAME}\")\n",
    "    vector_store = CouchbaseSearchVectorStore(\n",
    "        cluster=cb_cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=embeddings,\n",
    "        index_name=INDEX_NAME\n",
    "    )\n",
    "    logger.info(\"Successfully created Couchbase vector store.\")\n",
    "\n",
    "    # Load documents from JSON file\n",
    "    logger.info(f\"Looking for documents at: {DOCS_JSON_PATH}\")\n",
    "    if not os.path.exists(DOCS_JSON_PATH):\n",
    "         raise FileNotFoundError(f\"Documents file not found: {DOCS_JSON_PATH}\")\n",
    "\n",
    "    with open(DOCS_JSON_PATH, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        documents_to_load = data.get('documents', [])\n",
    "    logger.info(f\"Loaded {len(documents_to_load)} documents from {DOCS_JSON_PATH}\")\n",
    "\n",
    "    # Add documents to vector store\n",
    "    if documents_to_load:\n",
    "        logger.info(f\"Adding {len(documents_to_load)} documents to vector store...\")\n",
    "        texts = [doc.get('text', '') for doc in documents_to_load]\n",
    "        metadatas = []\n",
    "        for i, doc in enumerate(documents_to_load):\n",
    "            metadata_raw = doc.get('metadata', {})\n",
    "            if isinstance(metadata_raw, str):\n",
    "                try:\n",
    "                    metadata = json.loads(metadata_raw)\n",
    "                    if not isinstance(metadata, dict):\n",
    "                         logger.warning(f\"Parsed metadata not a dict: {metadata}. Using empty.\")\n",
    "                         metadata = {}\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.warning(f\"Could not parse metadata string: {metadata_raw}. Using empty.\")\n",
    "                    metadata = {}\n",
    "            elif isinstance(metadata_raw, dict):\n",
    "                metadata = metadata_raw\n",
    "            else:\n",
    "                logger.warning(f\"Metadata not string or dict: {metadata_raw}. Using empty.\")\n",
    "                metadata = {}\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "        inserted_ids = vector_store.add_texts(texts=texts, metadatas=metadatas)\n",
    "        logger.info(f\"Successfully added {len(inserted_ids)} documents to the vector store.\")\n",
    "        logger.info(\"Waiting briefly for vector indexing...\")\n",
    "        time.sleep(5)\n",
    "    else:\n",
    "         logger.warning(\"No documents found in the JSON file to add.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "     logger.error(f\"Setup failed: {e}\")\n",
    "     raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during vector store setup or data loading: {e}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise\n",
    "\n",
    "logger.info(\"--- Couchbase Setup and Data Loading Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create IAM Role for Agent and Lambdas\n",
    "\n",
    "Define and create an IAM role that grants necessary permissions for Bedrock Agents and Lambda functions to interact with each other and other AWS services (like CloudWatch Logs and Bedrock)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 07:48:45,615 - INFO - Checking/Creating IAM role: bedrock_agent_lambda_exp_role\n",
      "2025-05-01 07:48:46,733 - INFO - IAM role 'bedrock_agent_lambda_exp_role' already exists. Updating trust policy.\n",
      "2025-05-01 07:48:47,060 - INFO - Trust policy updated for role 'bedrock_agent_lambda_exp_role'.\n",
      "2025-05-01 07:48:47,060 - INFO - Attaching basic Lambda execution policy to role 'bedrock_agent_lambda_exp_role'...\n",
      "2025-05-01 07:48:47,402 - INFO - Attached basic Lambda execution policy.\n",
      "2025-05-01 07:48:47,404 - INFO - Putting inline policy 'LambdaBasicLoggingPermissions'...\n",
      "2025-05-01 07:48:47,729 - INFO - Putting inline policy 'BedrockAgentPermissions'...\n",
      "2025-05-01 07:48:48,057 - INFO - Inline policies updated. Waiting 10s...\n",
      "2025-05-01 07:48:58,061 - INFO - Agent IAM Role ARN: arn:aws:iam::598307997273:role/bedrock_agent_lambda_exp_role\n"
     ]
    }
   ],
   "source": [
    "def create_agent_role(iam_client, role_name, aws_account_id):\n",
    "    \"\"\"Creates or gets the IAM role for the Bedrock Agent Lambda functions.\"\"\"\n",
    "    logger.info(f\"Checking/Creating IAM role: {role_name}\")\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": [\n",
    "                        \"lambda.amazonaws.com\",\n",
    "                        \"bedrock.amazonaws.com\"\n",
    "                    ]\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    role_arn = None\n",
    "    try:\n",
    "        get_role_response = iam_client.get_role(RoleName=role_name)\n",
    "        role_arn = get_role_response['Role']['Arn']\n",
    "        logger.info(f\"IAM role '{role_name}' already exists. Updating trust policy.\")\n",
    "        iam_client.update_assume_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyDocument=json.dumps(assume_role_policy_document)\n",
    "        )\n",
    "        logger.info(f\"Trust policy updated for role '{role_name}'.\")\n",
    "\n",
    "    except iam_client.exceptions.NoSuchEntityException:\n",
    "        logger.info(f\"IAM role '{role_name}' not found. Creating...\")\n",
    "        try:\n",
    "            create_role_response = iam_client.create_role(\n",
    "                RoleName=role_name,\n",
    "                AssumeRolePolicyDocument=json.dumps(assume_role_policy_document),\n",
    "                Description='IAM role for Bedrock Agent Lambda functions (Experiment)',\n",
    "                MaxSessionDuration=3600\n",
    "            )\n",
    "            role_arn = create_role_response['Role']['Arn']\n",
    "            logger.info(f\"Created IAM role '{role_name}'. ARN: {role_arn}. Waiting 15s...\")\n",
    "            time.sleep(15)\n",
    "        except ClientError as e:\n",
    "            logger.error(f\"Error creating IAM role '{role_name}': {e}\")\n",
    "            raise\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error getting/updating IAM role '{role_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "    # Attach basic execution policy\n",
    "    try:\n",
    "        logger.info(f\"Attaching basic Lambda execution policy to role '{role_name}'...\")\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    "        )\n",
    "        logger.info(\"Attached basic Lambda execution policy.\")\n",
    "    except ClientError as e:\n",
    "        logger.warning(f\"Error attaching basic Lambda execution policy (may already exist): {e}\")\n",
    "\n",
    "    # Add inline policies for logging and Bedrock\n",
    "    basic_inline_policy_name = \"LambdaBasicLoggingPermissions\"\n",
    "    basic_inline_policy_doc = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\"logs:CreateLogGroup\", \"logs:CreateLogStream\", \"logs:PutLogEvents\"],\n",
    "                \"Resource\": f\"arn:aws:logs:{AWS_REGION}:{aws_account_id}:log-group:/aws/lambda/*:*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    bedrock_policy_name = \"BedrockAgentPermissions\"\n",
    "    bedrock_policy_doc = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\"bedrock:*\"],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    try:\n",
    "        logger.info(f\"Putting inline policy '{basic_inline_policy_name}'...\")\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=basic_inline_policy_name,\n",
    "            PolicyDocument=json.dumps(basic_inline_policy_doc)\n",
    "        )\n",
    "        logger.info(f\"Putting inline policy '{bedrock_policy_name}'...\")\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=bedrock_policy_name,\n",
    "            PolicyDocument=json.dumps(bedrock_policy_doc)\n",
    "        )\n",
    "        logger.info(\"Inline policies updated. Waiting 10s...\")\n",
    "        time.sleep(10)\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error putting inline policy: {e}\")\n",
    "\n",
    "    if not role_arn:\n",
    "         raise Exception(f\"Failed to create or retrieve ARN for role {role_name}\")\n",
    "\n",
    "    return role_arn\n",
    "\n",
    "# Execute role creation\n",
    "agent_role_name = \"bedrock_agent_lambda_exp_role\"\n",
    "try:\n",
    "    agent_role_arn = create_agent_role(iam_client, agent_role_name, AWS_ACCOUNT_ID)\n",
    "    logger.info(f\"Agent IAM Role ARN: {agent_role_arn}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create/verify IAM role: {e}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Package and Deploy Lambda Functions\n",
    "\n",
    "Define functions to:\n",
    "1.  **Package:** Use the `Makefile` in the `lambda_functions` directory to install dependencies and create deployment zip files for each Lambda function.\n",
    "2.  **Upload (if needed):** Upload the zip file to an S3 bucket if it exceeds the direct upload size limit.\n",
    "3.  **Deploy:** Create or update the AWS Lambda functions using the packaged code and the previously created IAM role.\n",
    "4.  **Delete (Helper):** A function to delete existing Lambda functions before deployment.\n",
    "\n",
    "**Note:** Running `make` requires the `make` utility to be installed in your environment. The `Makefile` handles dependency installation into a `package_dir` and zipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 07:48:58,087 - INFO - --- Starting Lambda Deployment --- \n",
      "2025-05-01 07:48:58,088 - INFO - Attempting to delete Lambda function: bedrock_agent_researcher_exp...\n",
      "2025-05-01 07:48:58,088 - INFO - Attempting to remove permission AllowBedrockInvokeBasic-bedrock_agent_researcher_exp...\n",
      "2025-05-01 07:48:58,886 - INFO - Permission AllowBedrockInvokeBasic-bedrock_agent_researcher_exp not found. Skipping removal.\n",
      "2025-05-01 07:48:59,213 - INFO - Lambda function 'bedrock_agent_researcher_exp' does not exist.\n",
      "2025-05-01 07:48:59,215 - INFO - Attempting to delete Lambda function: bedrock_agent_writer_exp...\n",
      "2025-05-01 07:48:59,217 - INFO - Attempting to remove permission AllowBedrockInvokeBasic-bedrock_agent_writer_exp...\n",
      "2025-05-01 07:48:59,495 - INFO - Permission AllowBedrockInvokeBasic-bedrock_agent_writer_exp not found. Skipping removal.\n",
      "2025-05-01 07:48:59,828 - INFO - Lambda function 'bedrock_agent_writer_exp' does not exist.\n",
      "2025-05-01 07:48:59,829 - INFO - --- Packaging function bedrock_agent_researcher ---\n",
      "2025-05-01 07:48:59,830 - INFO - Source Dir (Makefile location): /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions\n",
      "2025-05-01 07:48:59,830 - INFO - Build Dir (Final zip location): /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments\n",
      "2025-05-01 07:48:59,831 - INFO - Copying /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/bedrock_agent_researcher.py to /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/lambda_function.py\n",
      "2025-05-01 07:48:59,834 - INFO - Running make command: make -f /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/Makefile clean package (in /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions)\n",
      "2025-05-01 07:49:22,781 - INFO - Make command completed successfully.\n",
      "2025-05-01 07:49:22,782 - INFO - Moving and renaming /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/lambda_package.zip to /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_researcher.zip\n",
      "2025-05-01 07:49:22,782 - INFO - Zip file ready: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_researcher.zip\n",
      "2025-05-01 07:49:22,783 - INFO - Cleaning up temporary script: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/lambda_function.py\n",
      "2025-05-01 07:49:22,783 - INFO - --- Packaging function bedrock_agent_writer ---\n",
      "2025-05-01 07:49:22,784 - INFO - Source Dir (Makefile location): /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions\n",
      "2025-05-01 07:49:22,784 - INFO - Build Dir (Final zip location): /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments\n",
      "2025-05-01 07:49:22,784 - INFO - Copying /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/bedrock_agent_writer.py to /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/lambda_function.py\n",
      "2025-05-01 07:49:22,789 - INFO - Running make command: make -f /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/Makefile clean package (in /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions)\n",
      "2025-05-01 07:49:45,950 - INFO - Make command completed successfully.\n",
      "2025-05-01 07:49:45,952 - INFO - Moving and renaming /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/lambda_package.zip to /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_writer.zip\n",
      "2025-05-01 07:49:45,953 - INFO - Zip file ready: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_writer.zip\n",
      "2025-05-01 07:49:45,953 - INFO - Cleaning up temporary script: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/lambda_function.py\n",
      "2025-05-01 07:49:45,955 - INFO - Deploying Lambda function bedrock_agent_researcher_exp from /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_researcher.zip...\n",
      "2025-05-01 07:49:45,963 - INFO - Found credentials in environment variables.\n",
      "2025-05-01 07:49:46,058 - INFO - Zip file size: 50.56 MB\n",
      "2025-05-01 07:49:46,058 - INFO - Package size requires S3 deployment.\n",
      "2025-05-01 07:49:46,059 - INFO - Preparing to upload /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_researcher.zip to S3 in region us-east-1...\n",
      "2025-05-01 07:49:47,138 - INFO - Generated S3 bucket name: lambda-deployment-598307997273-1746065987\n",
      "2025-05-01 07:49:47,840 - INFO - Creating S3 bucket: lambda-deployment-598307997273-1746065987...\n",
      "2025-05-01 07:49:48,217 - INFO - Created S3 bucket: lambda-deployment-598307997273-1746065987. Waiting...\n",
      "2025-05-01 07:49:48,485 - INFO - Bucket lambda-deployment-598307997273-1746065987 is available.\n",
      "2025-05-01 07:49:48,486 - INFO - Uploading /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_researcher.zip to s3://lambda-deployment-598307997273-1746065987/lambda/bedrock_agent_researcher.zip-359d3e25...\n",
      "2025-05-01 07:50:07,474 - INFO - Successfully uploaded to s3://lambda-deployment-598307997273-1746065987/lambda/bedrock_agent_researcher.zip-359d3e25\n",
      "2025-05-01 07:50:07,477 - INFO - Creating function 'bedrock_agent_researcher_exp' (attempt 1)...\n",
      "2025-05-01 07:50:10,592 - INFO - Created function 'bedrock_agent_researcher_exp'. ARN: arn:aws:lambda:us-east-1:598307997273:function:bedrock_agent_researcher_exp\n",
      "2025-05-01 07:50:15,595 - INFO - Adding invoke permission (AllowBedrockInvokeBasic-bedrock_agent_researcher_exp)...\n",
      "2025-05-01 07:50:15,898 - INFO - Added permission AllowBedrockInvokeBasic-bedrock_agent_researcher_exp.\n",
      "2025-05-01 07:50:15,898 - INFO - Waiting for function 'bedrock_agent_researcher_exp' to become active...\n",
      "2025-05-01 07:50:21,490 - INFO - Function 'bedrock_agent_researcher_exp' is active.\n",
      "2025-05-01 07:50:21,496 - INFO - Researcher Lambda Deployed: arn:aws:lambda:us-east-1:598307997273:function:bedrock_agent_researcher_exp\n",
      "2025-05-01 07:50:21,497 - INFO - Deploying Lambda function bedrock_agent_writer_exp from /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_writer.zip...\n",
      "2025-05-01 07:50:21,505 - INFO - Zip file size: 50.56 MB\n",
      "2025-05-01 07:50:21,506 - INFO - Package size requires S3 deployment.\n",
      "2025-05-01 07:50:21,506 - INFO - Preparing to upload /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_writer.zip to S3 in region us-east-1...\n",
      "2025-05-01 07:50:22,446 - INFO - Generated S3 bucket name: lambda-deployment-598307997273-1746066022\n",
      "2025-05-01 07:50:23,135 - INFO - Creating S3 bucket: lambda-deployment-598307997273-1746066022...\n",
      "2025-05-01 07:50:23,543 - INFO - Created S3 bucket: lambda-deployment-598307997273-1746066022. Waiting...\n",
      "2025-05-01 07:50:23,765 - INFO - Bucket lambda-deployment-598307997273-1746066022 is available.\n",
      "2025-05-01 07:50:23,767 - INFO - Uploading /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_writer.zip to s3://lambda-deployment-598307997273-1746066022/lambda/bedrock_agent_writer.zip-c3f0117f...\n",
      "2025-05-01 07:50:41,383 - INFO - Successfully uploaded to s3://lambda-deployment-598307997273-1746066022/lambda/bedrock_agent_writer.zip-c3f0117f\n",
      "2025-05-01 07:50:41,391 - INFO - Creating function 'bedrock_agent_writer_exp' (attempt 1)...\n",
      "2025-05-01 07:50:44,558 - INFO - Created function 'bedrock_agent_writer_exp'. ARN: arn:aws:lambda:us-east-1:598307997273:function:bedrock_agent_writer_exp\n",
      "2025-05-01 07:50:49,559 - INFO - Adding invoke permission (AllowBedrockInvokeBasic-bedrock_agent_writer_exp)...\n",
      "2025-05-01 07:50:49,893 - INFO - Added permission AllowBedrockInvokeBasic-bedrock_agent_writer_exp.\n",
      "2025-05-01 07:50:49,893 - INFO - Waiting for function 'bedrock_agent_writer_exp' to become active...\n",
      "2025-05-01 07:50:50,254 - INFO - Function 'bedrock_agent_writer_exp' is active.\n",
      "2025-05-01 07:50:50,256 - INFO - Writer Lambda Deployed: arn:aws:lambda:us-east-1:598307997273:function:bedrock_agent_writer_exp\n",
      "2025-05-01 07:50:50,256 - INFO - Cleaning up deployment zip files...\n",
      "2025-05-01 07:50:50,259 - INFO - --- Lambda Deployment Complete --- \n"
     ]
    }
   ],
   "source": [
    "def delete_lambda_function(lambda_client, function_name):\n",
    "    \"\"\"Delete Lambda function if it exists.\"\"\"\n",
    "    logger.info(f\"Attempting to delete Lambda function: {function_name}...\")\n",
    "    try:\n",
    "        statement_id = f\"AllowBedrockInvokeBasic-{function_name}\"\n",
    "        try:\n",
    "            logger.info(f\"Attempting to remove permission {statement_id}...\")\n",
    "            lambda_client.remove_permission(FunctionName=function_name, StatementId=statement_id)\n",
    "            logger.info(f\"Removed permission {statement_id}.\")\n",
    "            time.sleep(2)\n",
    "        except lambda_client.exceptions.ResourceNotFoundException:\n",
    "            logger.info(f\"Permission {statement_id} not found. Skipping removal.\")\n",
    "        except ClientError as perm_e:\n",
    "            logger.warning(f\"Error removing permission {statement_id}: {str(perm_e)}\")\n",
    "\n",
    "        lambda_client.get_function(FunctionName=function_name)\n",
    "        logger.info(f\"Function {function_name} exists. Deleting...\")\n",
    "        lambda_client.delete_function(FunctionName=function_name)\n",
    "        logger.info(f\"Waiting for {function_name} deletion...\")\n",
    "        time.sleep(10)\n",
    "        logger.info(f\"Function {function_name} deletion initiated.\")\n",
    "        return True\n",
    "    except lambda_client.exceptions.ResourceNotFoundException:\n",
    "        logger.info(f\"Lambda function '{function_name}' does not exist.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error deleting Lambda function '{function_name}': {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def upload_to_s3(zip_file, region, bucket_name=None):\n",
    "    \"\"\"Upload zip file to S3 and return S3 location.\"\"\"\n",
    "    logger.info(f\"Preparing to upload {zip_file} to S3 in region {region}...\")\n",
    "    config = Config(connect_timeout=60, read_timeout=300, retries={'max_attempts': 3, 'mode': 'adaptive'})\n",
    "    s3_client = boto3.client('s3', region_name=region, config=config)\n",
    "    sts_client = boto3.client('sts', region_name=region, config=config)\n",
    "\n",
    "    if bucket_name is None:\n",
    "        try:\n",
    "            account_id = sts_client.get_caller_identity().get('Account')\n",
    "            timestamp = int(time.time())\n",
    "            bucket_name = f\"lambda-deployment-{account_id}-{timestamp}\"\n",
    "            logger.info(f\"Generated S3 bucket name: {bucket_name}\")\n",
    "        except Exception as e:\n",
    "            fallback_id = uuid.uuid4().hex[:12]\n",
    "            bucket_name = f\"lambda-deployment-{fallback_id}\"\n",
    "            logger.warning(f\"Error getting account ID ({e}). Using fallback: {bucket_name}\")\n",
    "\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        logger.info(f\"Using existing S3 bucket: {bucket_name}\")\n",
    "    except ClientError as e:\n",
    "        error_code = int(e.response['Error']['Code'])\n",
    "        if error_code == 404:\n",
    "            logger.info(f\"Creating S3 bucket: {bucket_name}...\")\n",
    "            try:\n",
    "                if region == 'us-east-1':\n",
    "                    s3_client.create_bucket(Bucket=bucket_name)\n",
    "                else:\n",
    "                    s3_client.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': region})\n",
    "                logger.info(f\"Created S3 bucket: {bucket_name}. Waiting...\")\n",
    "                waiter = s3_client.get_waiter('bucket_exists')\n",
    "                waiter.wait(Bucket=bucket_name, WaiterConfig={'Delay': 5, 'MaxAttempts': 12})\n",
    "                logger.info(f\"Bucket {bucket_name} is available.\")\n",
    "            except Exception as create_e:\n",
    "                logger.error(f\"Error creating bucket '{bucket_name}': {create_e}\")\n",
    "                raise\n",
    "        else:\n",
    "            logger.error(f\"Error checking bucket '{bucket_name}': {e}\")\n",
    "            raise\n",
    "\n",
    "    s3_key = f\"lambda/{os.path.basename(zip_file)}-{uuid.uuid4().hex[:8]}\"\n",
    "    try:\n",
    "        logger.info(f\"Uploading {zip_file} to s3://{bucket_name}/{s3_key}...\")\n",
    "        file_size = os.path.getsize(zip_file)\n",
    "        if file_size > 100 * 1024 * 1024:\n",
    "            logger.info(\"Using multipart upload...\")\n",
    "            transfer_config = boto3.s3.transfer.TransferConfig(multipart_threshold=10*1024*1024, max_concurrency=10, multipart_chunksize=10*1024*1024, use_threads=True)\n",
    "            s3_transfer = boto3.s3.transfer.S3Transfer(client=s3_client, config=transfer_config)\n",
    "            s3_transfer.upload_file(zip_file, bucket_name, s3_key)\n",
    "        else:\n",
    "            with open(zip_file, 'rb') as f:\n",
    "                s3_client.put_object(Bucket=bucket_name, Key=s3_key, Body=f)\n",
    "        logger.info(f\"Successfully uploaded to s3://{bucket_name}/{s3_key}\")\n",
    "        return {'S3Bucket': bucket_name, 'S3Key': s3_key}\n",
    "    except Exception as upload_e:\n",
    "        logger.error(f\"S3 upload failed: {upload_e}\")\n",
    "        raise\n",
    "\n",
    "def package_function(function_name, source_dir, build_dir):\n",
    "    \"\"\"Package Lambda function using Makefile found in source_dir.\"\"\"\n",
    "    makefile_path = os.path.join(source_dir, 'Makefile')\n",
    "    temp_package_dir = os.path.join(source_dir, 'package_dir')\n",
    "    source_req_path = os.path.join(source_dir, 'requirements.txt')\n",
    "    source_func_script_path = os.path.join(source_dir, f'{function_name}.py')\n",
    "    target_func_script_path = os.path.join(source_dir, 'lambda_function.py') # Target name for make\n",
    "    make_output_zip = os.path.join(source_dir, 'lambda_package.zip') # Output from make\n",
    "    final_zip_path = os.path.join(build_dir, f'{function_name}.zip') # Final location\n",
    "\n",
    "    logger.info(f\"--- Packaging function {function_name} ---\")\n",
    "    logger.info(f\"Source Dir (Makefile location): {source_dir}\")\n",
    "    logger.info(f\"Build Dir (Final zip location): {build_dir}\")\n",
    "\n",
    "    if not os.path.exists(source_func_script_path):\n",
    "        raise FileNotFoundError(f\"Source script not found: {source_func_script_path}\")\n",
    "    if not os.path.exists(source_req_path):\n",
    "        raise FileNotFoundError(f\"Requirements file not found: {source_req_path}\")\n",
    "    if not os.path.exists(makefile_path):\n",
    "        raise FileNotFoundError(f\"Makefile not found: {makefile_path}\")\n",
    "\n",
    "    if os.path.exists(target_func_script_path):\n",
    "        logger.warning(f\"Removing existing target script: {target_func_script_path}\")\n",
    "        os.remove(target_func_script_path)\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Copying {source_func_script_path} to {target_func_script_path}\")\n",
    "        shutil.copy(source_func_script_path, target_func_script_path)\n",
    "\n",
    "        make_command = ['make', '-f', makefile_path, 'clean', 'package']\n",
    "        logger.info(f\"Running make command: {' '.join(make_command)} (in {source_dir})\")\n",
    "        # Use shell=True cautiously or ensure make path is absolute/in PATH\n",
    "        # Redirect stdout to avoid cluttering notebook output\n",
    "        process = subprocess.run(make_command, cwd=source_dir, capture_output=True, text=True, check=True)\n",
    "        logger.info(\"Make command completed successfully.\")\n",
    "        # logger.debug(f\"Make stdout:\\n{process.stdout}\") # Uncomment for debugging make output\n",
    "\n",
    "        if not os.path.exists(make_output_zip):\n",
    "            raise FileNotFoundError(f\"Makefile did not produce expected output: {make_output_zip}\")\n",
    "\n",
    "        logger.info(f\"Moving and renaming {make_output_zip} to {final_zip_path}\")\n",
    "        if os.path.exists(final_zip_path):\n",
    "             logger.warning(f\"Removing existing final zip: {final_zip_path}\")\n",
    "             os.remove(final_zip_path)\n",
    "        os.rename(make_output_zip, final_zip_path)\n",
    "        logger.info(f\"Zip file ready: {final_zip_path}\")\n",
    "\n",
    "        return final_zip_path\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.error(f\"Error running Makefile for {function_name}: {e}\")\n",
    "        logger.error(f\"Make stderr:\\n{e.stderr}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error packaging function {function_name}: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "    finally:\n",
    "        if os.path.exists(target_func_script_path):\n",
    "            logger.info(f\"Cleaning up temporary script: {target_func_script_path}\")\n",
    "            os.remove(target_func_script_path)\n",
    "        if os.path.exists(make_output_zip):\n",
    "            logger.warning(f\"Cleaning up intermediate zip: {make_output_zip}\")\n",
    "            os.remove(make_output_zip)\n",
    "\n",
    "def create_lambda_function(lambda_client, function_name, handler, role_arn, zip_file, region):\n",
    "    \"\"\"Create or update Lambda function.\"\"\"\n",
    "    logger.info(f\"Deploying Lambda function {function_name} from {zip_file}...\")\n",
    "    config = Config(connect_timeout=120, read_timeout=300, retries={'max_attempts': 5, 'mode': 'adaptive'})\n",
    "    lambda_client_local = boto3.client('lambda', region_name=region, config=config)\n",
    "\n",
    "    zip_size_mb = 0\n",
    "    try:\n",
    "        zip_size_bytes = os.path.getsize(zip_file)\n",
    "        zip_size_mb = zip_size_bytes / (1024 * 1024)\n",
    "        logger.info(f\"Zip file size: {zip_size_mb:.2f} MB\")\n",
    "    except OSError as e:\n",
    "         logger.error(f\"Could not get size of zip file {zip_file}: {e}\")\n",
    "         raise\n",
    "\n",
    "    use_s3 = zip_size_mb > 45\n",
    "    s3_location = None\n",
    "    zip_content = None\n",
    "\n",
    "    if use_s3:\n",
    "        logger.info(f\"Package size requires S3 deployment.\")\n",
    "        s3_location = upload_to_s3(zip_file, region)\n",
    "        if not s3_location:\n",
    "             raise Exception(\"Failed to upload Lambda package to S3.\")\n",
    "    else:\n",
    "         logger.info(\"Deploying package directly.\")\n",
    "         try:\n",
    "             with open(zip_file, 'rb') as f:\n",
    "                 zip_content = f.read()\n",
    "         except OSError as e:\n",
    "              logger.error(f\"Could not read zip file {zip_file}: {e}\")\n",
    "              raise\n",
    "\n",
    "    common_args = {\n",
    "        'FunctionName': function_name,\n",
    "        'Runtime': 'python3.9',\n",
    "        'Role': role_arn,\n",
    "        'Handler': handler,\n",
    "        'Timeout': 180,\n",
    "        'MemorySize': 1536,\n",
    "        'Environment': {\n",
    "            'Variables': {\n",
    "                'CB_HOST': os.getenv('CB_HOST', 'couchbase://localhost'),\n",
    "                'CB_USERNAME': os.getenv('CB_USERNAME', 'Administrator'),\n",
    "                'CB_PASSWORD': os.getenv('CB_PASSWORD'), # Passed from notebook env\n",
    "                'CB_BUCKET_NAME': os.getenv('CB_BUCKET_NAME', 'vector-search-exp'),\n",
    "                'SCOPE_NAME': os.getenv('SCOPE_NAME', 'bedrock_exp'),\n",
    "                'COLLECTION_NAME': os.getenv('COLLECTION_NAME', 'docs_exp'),\n",
    "                'INDEX_NAME': os.getenv('INDEX_NAME', 'vector_search_bedrock_exp'),\n",
    "                'EMBEDDING_MODEL_ID': os.getenv('EMBEDDING_MODEL_ID', EMBEDDING_MODEL_ID),\n",
    "                'AGENT_MODEL_ID': os.getenv('AGENT_MODEL_ID', AGENT_MODEL_ID),\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if use_s3:\n",
    "        code_arg = {'S3Bucket': s3_location['S3Bucket'], 'S3Key': s3_location['S3Key']}\n",
    "    else:\n",
    "        code_arg = {'ZipFile': zip_content}\n",
    "\n",
    "    max_retries = 3\n",
    "    base_delay = 10\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            logger.info(f\"Creating function '{function_name}' (attempt {attempt})...\")\n",
    "            create_args = common_args.copy()\n",
    "            create_args['Code'] = code_arg\n",
    "            create_args['Publish'] = True\n",
    "\n",
    "            create_response = lambda_client_local.create_function(**create_args)\n",
    "            function_arn = create_response['FunctionArn']\n",
    "            logger.info(f\"Created function '{function_name}'. ARN: {function_arn}\")\n",
    "\n",
    "            time.sleep(5)\n",
    "            statement_id = f\"AllowBedrockInvokeBasic-{function_name}\"\n",
    "            try:\n",
    "                logger.info(f\"Adding invoke permission ({statement_id})...\")\n",
    "                lambda_client_local.add_permission(\n",
    "                    FunctionName=function_name, StatementId=statement_id,\n",
    "                    Action='lambda:InvokeFunction', Principal='bedrock.amazonaws.com'\n",
    "                )\n",
    "                logger.info(f\"Added permission {statement_id}.\")\n",
    "            except lambda_client_local.exceptions.ResourceConflictException:\n",
    "                 logger.info(f\"Permission {statement_id} already exists.\")\n",
    "            except ClientError as perm_e:\n",
    "                logger.warning(f\"Failed to add permission {statement_id}: {perm_e}\")\n",
    "\n",
    "            logger.info(f\"Waiting for function '{function_name}' to become active...\")\n",
    "            waiter = lambda_client_local.get_waiter('function_active_v2')\n",
    "            waiter.wait(FunctionName=function_name, WaiterConfig={'Delay': 5, 'MaxAttempts': 24})\n",
    "            logger.info(f\"Function '{function_name}' is active.\")\n",
    "            return function_arn\n",
    "\n",
    "        except lambda_client_local.exceptions.ResourceConflictException:\n",
    "             logger.warning(f\"Function '{function_name}' already exists. Updating code...\")\n",
    "             try:\n",
    "                 update_args = {'FunctionName': function_name, 'Publish': True}\n",
    "                 if use_s3:\n",
    "                     update_args['S3Bucket'] = s3_location['S3Bucket']\n",
    "                     update_args['S3Key'] = s3_location['S3Key']\n",
    "                 else:\n",
    "                     update_args['ZipFile'] = zip_content\n",
    "                 update_response = lambda_client_local.update_function_code(**update_args)\n",
    "                 function_arn = update_response['FunctionArn']\n",
    "                 logger.info(f\"Updated function code for '{function_name}'. New version ARN: {function_arn}\")\n",
    "\n",
    "                 try:\n",
    "                      logger.info(f\"Updating configuration for '{function_name}'...\")\n",
    "                      lambda_client_local.update_function_configuration(**common_args)\n",
    "                      logger.info(f\"Configuration updated.\")\n",
    "                 except ClientError as conf_e:\n",
    "                      logger.warning(f\"Could not update configuration: {conf_e}\")\n",
    "\n",
    "                 time.sleep(5)\n",
    "                 statement_id = f\"AllowBedrockInvokeBasic-{function_name}\"\n",
    "                 try:\n",
    "                     logger.info(f\"Verifying/Adding invoke permission ({statement_id}) after update...\")\n",
    "                     lambda_client_local.add_permission(\n",
    "                         FunctionName=function_name, StatementId=statement_id,\n",
    "                         Action='lambda:InvokeFunction', Principal='bedrock.amazonaws.com'\n",
    "                     )\n",
    "                     logger.info(f\"Permission {statement_id} added/verified.\")\n",
    "                 except lambda_client_local.exceptions.ResourceConflictException:\n",
    "                     logger.info(f\"Permission {statement_id} already exists.\")\n",
    "                 except ClientError as perm_e:\n",
    "                     logger.warning(f\"Failed to add/verify permission {statement_id}: {perm_e}\")\n",
    "\n",
    "                 logger.info(f\"Waiting for function '{function_name}' update...\")\n",
    "                 waiter = lambda_client_local.get_waiter('function_updated_v2')\n",
    "                 waiter.wait(FunctionName=function_name, WaiterConfig={'Delay': 5, 'MaxAttempts': 24})\n",
    "                 logger.info(f\"Function '{function_name}' update complete.\")\n",
    "                 return function_arn\n",
    "\n",
    "             except ClientError as update_e:\n",
    "                 logger.error(f\"Failed to update function '{function_name}': {update_e}\")\n",
    "                 if attempt < max_retries:\n",
    "                      delay = base_delay * (2 ** (attempt - 1))\n",
    "                      logger.info(f\"Retrying update in {delay} seconds...\")\n",
    "                      time.sleep(delay)\n",
    "                 else:\n",
    "                      logger.error(\"Max update retries reached.\")\n",
    "                      raise update_e\n",
    "\n",
    "        except ClientError as e:\n",
    "            error_code = e.response.get('Error', {}).get('Code')\n",
    "            if error_code in ['ThrottlingException', 'ProvisionedConcurrencyConfigNotFoundException', 'EC2ThrottledException'] or 'Rate exceeded' in str(e):\n",
    "                logger.warning(f\"Retryable error on attempt {attempt}: {e}\")\n",
    "                if attempt < max_retries:\n",
    "                    delay = base_delay * (2 ** (attempt - 1)) + (uuid.uuid4().int % 5)\n",
    "                    logger.info(f\"Retrying in {delay} seconds...\")\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    logger.error(\"Max retries reached.\")\n",
    "                    raise e\n",
    "            else:\n",
    "                logger.error(f\"Error creating/updating Lambda '{function_name}': {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "                raise e\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Unexpected error during Lambda deployment: {e}\")\n",
    "             logger.error(traceback.format_exc())\n",
    "             raise e\n",
    "\n",
    "    raise Exception(f\"Failed to deploy Lambda function {function_name} after {max_retries} attempts.\")\n",
    "\n",
    "# --- Deploy Lambdas ---\n",
    "researcher_lambda_name = \"bedrock_agent_researcher_exp\"\n",
    "writer_lambda_name = \"bedrock_agent_writer_exp\"\n",
    "lambda_build_dir = NOTEBOOK_DIR # Final zip ends up here\n",
    "\n",
    "logger.info(\"--- Starting Lambda Deployment --- \")\n",
    "researcher_lambda_arn = None\n",
    "writer_lambda_arn = None\n",
    "researcher_zip_path = None\n",
    "writer_zip_path = None\n",
    "\n",
    "try:\n",
    "    # Delete existing functions first\n",
    "    delete_lambda_function(lambda_client, researcher_lambda_name)\n",
    "    delete_lambda_function(lambda_client, writer_lambda_name)\n",
    "\n",
    "    # Package functions\n",
    "    researcher_zip_path = package_function(\"bedrock_agent_researcher\", LAMBDA_FUNCTIONS_DIR, lambda_build_dir)\n",
    "    writer_zip_path = package_function(\"bedrock_agent_writer\", LAMBDA_FUNCTIONS_DIR, lambda_build_dir)\n",
    "\n",
    "    # Create/Update functions\n",
    "    researcher_lambda_arn = create_lambda_function(\n",
    "        lambda_client=lambda_client, function_name=researcher_lambda_name,\n",
    "        handler='lambda_function.lambda_handler', role_arn=agent_role_arn,\n",
    "        zip_file=researcher_zip_path, region=AWS_REGION\n",
    "    )\n",
    "    logger.info(f\"Researcher Lambda Deployed: {researcher_lambda_arn}\")\n",
    "\n",
    "    writer_lambda_arn = create_lambda_function(\n",
    "        lambda_client=lambda_client, function_name=writer_lambda_name,\n",
    "        handler='lambda_function.lambda_handler', role_arn=agent_role_arn,\n",
    "        zip_file=writer_zip_path, region=AWS_REGION\n",
    "    )\n",
    "    logger.info(f\"Writer Lambda Deployed: {writer_lambda_arn}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "     logger.error(f\"Lambda packaging failed: Required file not found. {e}\")\n",
    "     raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Lambda deployment failed: {e}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise\n",
    "finally:\n",
    "    logger.info(\"Cleaning up deployment zip files...\")\n",
    "    if researcher_zip_path and os.path.exists(researcher_zip_path):\n",
    "        try: os.remove(researcher_zip_path)\n",
    "        except OSError as e: logger.warning(f\"Could not remove zip {researcher_zip_path}: {e}\")\n",
    "    if writer_zip_path and os.path.exists(writer_zip_path):\n",
    "         try: os.remove(writer_zip_path)\n",
    "         except OSError as e: logger.warning(f\"Could not remove zip {writer_zip_path}: {e}\")\n",
    "\n",
    "logger.info(\"--- Lambda Deployment Complete --- \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define Agent Creation and Deletion Functions\n",
    "\n",
    "Define helper functions to manage the Bedrock Agent lifecycle:\n",
    "*   `get_agent_by_name`: Find an existing agent's ID.\n",
    "*   `delete_action_group`: Delete a specific action group.\n",
    "*   `delete_agent_and_resources`: Delete an agent and all its associated action groups (useful for cleanup).\n",
    "*   `create_agent`: Create a new Bedrock Agent with a specified foundation model and instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_by_name(agent_client, agent_name):\n",
    "    \"\"\"Find an agent ID by its name using list_agents.\"\"\"\n",
    "    logger.info(f\"Attempting to find agent by name: {agent_name}\")\n",
    "    try:\n",
    "        paginator = agent_client.get_paginator('list_agents')\n",
    "        for page in paginator.paginate():\n",
    "            for agent_summary in page.get('agentSummaries', []):\n",
    "                if agent_summary.get('agentName') == agent_name:\n",
    "                    agent_id = agent_summary.get('agentId')\n",
    "                    logger.info(f\"Found agent '{agent_name}' with ID: {agent_id}\")\n",
    "                    return agent_id\n",
    "        logger.info(f\"Agent '{agent_name}' not found.\")\n",
    "        return None\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error listing agents to find '{agent_name}': {e}\")\n",
    "        return None\n",
    "\n",
    "def delete_action_group(agent_client, agent_id, action_group_id):\n",
    "    \"\"\"Deletes a specific action group for an agent.\"\"\"\n",
    "    logger.info(f\"Attempting to delete action group {action_group_id} for agent {agent_id} DRAFT...\")\n",
    "    try:\n",
    "        agent_client.delete_agent_action_group(\n",
    "            agentId=agent_id,\n",
    "            agentVersion='DRAFT',\n",
    "            actionGroupId=action_group_id,\n",
    "            skipResourceInUseCheck=True\n",
    "        )\n",
    "        logger.info(f\"Successfully deleted action group {action_group_id}.\")\n",
    "        time.sleep(5)\n",
    "        return True\n",
    "    except agent_client.exceptions.ResourceNotFoundException:\n",
    "        logger.info(f\"Action group {action_group_id} not found. Skipping deletion.\")\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        error_code = e.response.get('Error', {}).get('Code')\n",
    "        if error_code == 'ConflictException':\n",
    "            logger.warning(f\"Conflict deleting action group {action_group_id}. Retrying once...\")\n",
    "            time.sleep(15)\n",
    "            try:\n",
    "                agent_client.delete_agent_action_group(\n",
    "                    agentId=agent_id, agentVersion='DRAFT', actionGroupId=action_group_id, skipResourceInUseCheck=True\n",
    "                )\n",
    "                logger.info(f\"Successfully deleted action group {action_group_id} after retry.\")\n",
    "                return True\n",
    "            except Exception as retry_e:\n",
    "                 logger.error(f\"Error deleting action group {action_group_id} on retry: {retry_e}\")\n",
    "                 return False\n",
    "        else:\n",
    "            logger.error(f\"Error deleting action group {action_group_id}: {e}\")\n",
    "            return False\n",
    "\n",
    "def delete_agent_and_resources(agent_client, agent_name):\n",
    "    \"\"\"Deletes the agent and its associated action groups.\"\"\"\n",
    "    agent_id = get_agent_by_name(agent_client, agent_name)\n",
    "    if not agent_id:\n",
    "        logger.info(f\"Agent '{agent_name}' not found, no deletion needed.\")\n",
    "        return\n",
    "\n",
    "    logger.warning(f\"--- Deleting Agent Resources for '{agent_name}' (ID: {agent_id}) ---\")\n",
    "    try:\n",
    "        logger.info(f\"Listing action groups for agent {agent_id} DRAFT...\")\n",
    "        action_groups = agent_client.list_agent_action_groups(\n",
    "            agentId=agent_id,\n",
    "            agentVersion='DRAFT'\n",
    "        ).get('actionGroupSummaries', [])\n",
    "\n",
    "        if action_groups:\n",
    "            logger.info(f\"Found {len(action_groups)} action groups to delete.\")\n",
    "            for ag in action_groups:\n",
    "                delete_action_group(agent_client, agent_id, ag['actionGroupId'])\n",
    "        else:\n",
    "            logger.info(\"No action groups found to delete.\")\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error listing/deleting action groups for agent {agent_id}: {e}\")\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Attempting to delete agent {agent_id} ('{agent_name}')...\")\n",
    "        agent_client.delete_agent(agentId=agent_id, skipResourceInUseCheck=True)\n",
    "        logger.info(f\"Waiting up to 2 minutes for agent {agent_id} deletion...\")\n",
    "        deleted = False\n",
    "        for _ in range(24):\n",
    "            try:\n",
    "                agent_client.get_agent(agentId=agent_id)\n",
    "                time.sleep(5)\n",
    "            except agent_client.exceptions.ResourceNotFoundException:\n",
    "                logger.info(f\"Agent {agent_id} successfully deleted.\")\n",
    "                deleted = True\n",
    "                break\n",
    "            except ClientError as e:\n",
    "                 error_code = e.response.get('Error', {}).get('Code')\n",
    "                 if error_code == 'ThrottlingException':\n",
    "                     logger.warning(\"Throttled checking deletion status, waiting...\")\n",
    "                     time.sleep(10)\n",
    "                 else:\n",
    "                     logger.error(f\"Error checking deletion status: {e}\")\n",
    "                     break\n",
    "        if not deleted:\n",
    "             logger.warning(f\"Agent {agent_id} deletion confirmation timed out.\")\n",
    "    except agent_client.exceptions.ResourceNotFoundException:\n",
    "        logger.info(f\"Agent {agent_id} ('{agent_name}') already deleted or not found.\")\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error deleting agent {agent_id}: {e}\")\n",
    "\n",
    "    logger.info(f\"--- Agent Resource Deletion Complete for '{agent_name}' ---\")\n",
    "\n",
    "def create_agent(agent_client, agent_name, agent_role_arn, foundation_model_id):\n",
    "    \"\"\"Creates a new Bedrock Agent.\"\"\"\n",
    "    logger.info(f\"--- Creating Agent: {agent_name} ---\")\n",
    "    try:\n",
    "        instruction = (\n",
    "            \"You are a multi-step research assistant. Your goal is to answer user questions based on documents retrieved using your tools. \"\n",
    "            \"Follow these steps precisely: \"\n",
    "            \"1. If the user asks an informational question, you MUST first use the `search_documents` function available in the `ResearcherActionGroup` to find relevant documents. Provide the user's core question as the 'query' parameter. \"\n",
    "            \"2. Examine the text returned by `search_documents`. This is the research result. \"\n",
    "            \"3. If the user's original request included specific formatting instructions (e.g., 'summarize', 'bullet points', 'list the key points'), you MUST then use the `format_content` function available in the `WriterActionGroup`. Pass the research result (the text returned by `search_documents`) as the 'content' parameter and the requested format (e.g., 'bullet points') as the 'style' parameter. \"\n",
    "            \"4. If no specific formatting was requested, present the research result directly. \"\n",
    "            \"5. Only use the tools provided and follow this exact sequence. Do not answer informational questions from memory or make up information. Always start by searching.\"\n",
    "        )\n",
    "        response = agent_client.create_agent(\n",
    "            agentName=agent_name,\n",
    "            agentResourceRoleArn=agent_role_arn,\n",
    "            foundationModel=foundation_model_id,\n",
    "            instruction=instruction,\n",
    "            idleSessionTTLInSeconds=1800,\n",
    "            description=f\"Experimental agent: Couchbase search & format ({foundation_model_id})\"\n",
    "        )\n",
    "        agent_info = response.get('agent')\n",
    "        agent_id = agent_info.get('agentId')\n",
    "        agent_arn = agent_info.get('agentArn')\n",
    "        agent_status = agent_info.get('agentStatus')\n",
    "        logger.info(f\"Agent creation initiated. ID: {agent_id}, ARN: {agent_arn}, Status: {agent_status}\")\n",
    "\n",
    "        logger.info(f\"Waiting for agent {agent_id} to reach initial state...\")\n",
    "        for _ in range(12):\n",
    "             current_status = agent_client.get_agent(agentId=agent_id)['agent']['agentStatus']\n",
    "             logger.info(f\"Agent {agent_id} status: {current_status}\")\n",
    "             if current_status != 'CREATING':\n",
    "                  break\n",
    "             time.sleep(5)\n",
    "\n",
    "        final_status = agent_client.get_agent(agentId=agent_id)['agent']['agentStatus']\n",
    "        if final_status == 'FAILED':\n",
    "            raise Exception(f\"Agent creation failed for {agent_name}\")\n",
    "        elif final_status != 'NOT_PREPARED':\n",
    "            logger.warning(f\"Agent {agent_id} reached unexpected status '{final_status}'.\")\n",
    "        else:\n",
    "             logger.info(f\"Agent {agent_id} created (Status: {final_status}).\")\n",
    "        return agent_id, agent_arn\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error creating agent '{agent_name}': {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Agent Action Groups\n",
    "\n",
    "Define a function to create or update an action group. This function links an agent action (defined by an OpenAPI schema) to a specific Lambda function ARN. We will create two action groups:\n",
    "*   `ResearcherActionGroup`: Linked to the `bedrock_agent_researcher_exp` Lambda.\n",
    "*   `WriterActionGroup`: Linked to the `bedrock_agent_writer_exp` Lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_action_group(agent_client, agent_id, action_group_name, function_arn, schema_path):\n",
    "    \"\"\"Creates or updates an action group for the agent using a schema file.\"\"\"\n",
    "    logger.info(f\"--- Creating/Updating Action Group: {action_group_name} for Agent: {agent_id} ---\")\n",
    "    logger.info(f\"Lambda ARN: {function_arn}\")\n",
    "    logger.info(f\"Schema Path: {schema_path}\")\n",
    "\n",
    "    if not os.path.exists(schema_path):\n",
    "        raise FileNotFoundError(f\"Action group schema file not found: {schema_path}\")\n",
    "\n",
    "    try:\n",
    "        with open(schema_path, 'r') as f:\n",
    "            schema_definition = f.read()\n",
    "\n",
    "        # Check if Action Group already exists for the DRAFT version\n",
    "        try:\n",
    "             logger.info(f\"Checking if action group '{action_group_name}' exists for DRAFT...\")\n",
    "             paginator = agent_client.get_paginator('list_agent_action_groups')\n",
    "             existing_group = None\n",
    "             for page in paginator.paginate(agentId=agent_id, agentVersion='DRAFT'):\n",
    "                 for ag_summary in page.get('actionGroupSummaries', []):\n",
    "                      if ag_summary.get('actionGroupName') == action_group_name:\n",
    "                           existing_group = ag_summary\n",
    "                           break\n",
    "                 if existing_group:\n",
    "                      break\n",
    "\n",
    "             if existing_group:\n",
    "                 ag_id = existing_group['actionGroupId']\n",
    "                 logger.warning(f\"Action Group '{action_group_name}' (ID: {ag_id}) exists. Updating.\")\n",
    "                 response = agent_client.update_agent_action_group(\n",
    "                     agentId=agent_id, agentVersion='DRAFT', actionGroupId=ag_id,\n",
    "                     actionGroupName=action_group_name, functionArn=function_arn,\n",
    "                     actionGroupExecutor={'lambda': function_arn},\n",
    "                     apiSchema={'payload': schema_definition},\n",
    "                     actionGroupState='ENABLED'\n",
    "                 )\n",
    "                 ag_info = response.get('agentActionGroup')\n",
    "                 logger.info(f\"Updated Action Group '{action_group_name}' (ID: {ag_info.get('actionGroupId')}).\")\n",
    "                 time.sleep(5)\n",
    "                 return ag_info.get('actionGroupId')\n",
    "             else:\n",
    "                  logger.info(f\"Action group '{action_group_name}' does not exist. Creating new.\")\n",
    "\n",
    "        except ClientError as e:\n",
    "             logger.error(f\"Error checking existing action group '{action_group_name}': {e}. Proceeding with create.\")\n",
    "\n",
    "        # Create new action group\n",
    "        response = agent_client.create_agent_action_group(\n",
    "            agentId=agent_id, agentVersion='DRAFT',\n",
    "            actionGroupName=action_group_name,\n",
    "            actionGroupExecutor={'lambda': function_arn},\n",
    "            apiSchema={'payload': schema_definition},\n",
    "            actionGroupState='ENABLED'\n",
    "        )\n",
    "        ag_info = response.get('agentActionGroup')\n",
    "        ag_id = ag_info.get('actionGroupId')\n",
    "        logger.info(f\"Created Action Group '{action_group_name}' with ID: {ag_id}\")\n",
    "        time.sleep(5)\n",
    "        return ag_id\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error creating/updating action group '{action_group_name}': {e}\")\n",
    "        raise\n",
    "    except FileNotFoundError as e:\n",
    "         logger.error(f\"Schema file error for '{action_group_name}': {e}\")\n",
    "         raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create and Prepare the Agent\n",
    "\n",
    "Now, execute the agent creation and preparation steps:\n",
    "1.  Delete any existing agent with the same name to ensure a clean state.\n",
    "2.  Create the agent using the `create_agent` function.\n",
    "3.  Create the two action groups using `create_action_group`, linking them to the respective Lambda ARNs and schema files.\n",
    "4.  Prepare the agent using the `prepare_agent` function. This step compiles the agent configuration and makes it ready for use. This can take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 07:50:50,295 - INFO - Attempting to find agent by name: couchbase_research_writer_agent_exp\n",
      "2025-05-01 07:50:51,128 - INFO - Found agent 'couchbase_research_writer_agent_exp' with ID: ZJJNYBIBXY\n",
      "2025-05-01 07:50:51,128 - WARNING - --- Deleting Agent Resources for 'couchbase_research_writer_agent_exp' (ID: ZJJNYBIBXY) ---\n",
      "2025-05-01 07:50:51,128 - INFO - Listing action groups for agent ZJJNYBIBXY DRAFT...\n",
      "2025-05-01 07:50:51,417 - INFO - Found 2 action groups to delete.\n",
      "2025-05-01 07:50:51,417 - INFO - Attempting to delete action group 5IZ2EPNM9R for agent ZJJNYBIBXY DRAFT...\n",
      "2025-05-01 07:50:51,691 - INFO - Successfully deleted action group 5IZ2EPNM9R.\n",
      "2025-05-01 07:50:56,696 - INFO - Attempting to delete action group H7YPF1NQDV for agent ZJJNYBIBXY DRAFT...\n",
      "2025-05-01 07:50:56,976 - INFO - Successfully deleted action group H7YPF1NQDV.\n",
      "2025-05-01 07:51:01,980 - INFO - Attempting to delete agent ZJJNYBIBXY ('couchbase_research_writer_agent_exp')...\n",
      "2025-05-01 07:51:02,281 - INFO - Waiting up to 2 minutes for agent ZJJNYBIBXY deletion...\n",
      "2025-05-01 07:51:08,163 - INFO - Agent ZJJNYBIBXY successfully deleted.\n",
      "2025-05-01 07:51:08,163 - INFO - --- Agent Resource Deletion Complete for 'couchbase_research_writer_agent_exp' ---\n",
      "2025-05-01 07:51:08,163 - INFO - --- Creating Agent: couchbase_research_writer_agent_exp ---\n",
      "2025-05-01 07:51:08,730 - INFO - Agent creation initiated. ID: QRON0D6Y95, ARN: arn:aws:bedrock:us-east-1:598307997273:agent/QRON0D6Y95, Status: CREATING\n",
      "2025-05-01 07:51:08,730 - INFO - Waiting for agent QRON0D6Y95 to reach initial state...\n",
      "2025-05-01 07:51:09,142 - INFO - Agent QRON0D6Y95 status: CREATING\n",
      "2025-05-01 07:51:14,572 - INFO - Agent QRON0D6Y95 status: NOT_PREPARED\n",
      "2025-05-01 07:51:14,994 - INFO - Agent QRON0D6Y95 created (Status: NOT_PREPARED).\n",
      "2025-05-01 07:51:14,995 - INFO - --- Creating/Updating Action Group: ResearcherActionGroup for Agent: QRON0D6Y95 ---\n",
      "2025-05-01 07:51:14,995 - INFO - Lambda ARN: arn:aws:lambda:us-east-1:598307997273:function:bedrock_agent_researcher_exp\n",
      "2025-05-01 07:51:14,996 - INFO - Schema Path: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/schemas/researcher_schema.json\n",
      "2025-05-01 07:51:14,997 - INFO - Checking if action group 'ResearcherActionGroup' exists for DRAFT...\n",
      "2025-05-01 07:51:15,271 - INFO - Action group 'ResearcherActionGroup' does not exist. Creating new.\n",
      "2025-05-01 07:51:15,657 - INFO - Created Action Group 'ResearcherActionGroup' with ID: UVJS9GMVQB\n",
      "2025-05-01 07:51:20,659 - INFO - --- Creating/Updating Action Group: WriterActionGroup for Agent: QRON0D6Y95 ---\n",
      "2025-05-01 07:51:20,660 - INFO - Lambda ARN: arn:aws:lambda:us-east-1:598307997273:function:bedrock_agent_writer_exp\n",
      "2025-05-01 07:51:20,660 - INFO - Schema Path: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/schemas/writer_schema.json\n",
      "2025-05-01 07:51:20,663 - INFO - Checking if action group 'WriterActionGroup' exists for DRAFT...\n",
      "2025-05-01 07:51:20,977 - INFO - Action group 'WriterActionGroup' does not exist. Creating new.\n",
      "2025-05-01 07:51:21,391 - INFO - Created Action Group 'WriterActionGroup' with ID: 52NILQ8WVQ\n",
      "2025-05-01 07:51:26,395 - INFO - --- Preparing Agent: QRON0D6Y95 ---\n",
      "2025-05-01 07:51:26,925 - INFO - Agent preparation initiated for version 'DRAFT'. Status: PREPARING. Prepared At: 2025-05-01 02:21:26.921837+00:00\n",
      "2025-05-01 07:51:26,926 - INFO - Waiting for agent QRON0D6Y95 preparation (up to 10 minutes)...\n",
      "2025-05-01 07:51:57,787 - INFO - Agent QRON0D6Y95 successfully prepared.\n",
      "2025-05-01 07:51:57,787 - INFO - --- Bedrock Agent 'couchbase_research_writer_agent_exp' (ID: QRON0D6Y95) Setup Complete ---\n"
     ]
    }
   ],
   "source": [
    "def prepare_agent(agent_client, agent_id):\n",
    "    \"\"\"Prepares the DRAFT version of the agent.\"\"\"\n",
    "    logger.info(f\"--- Preparing Agent: {agent_id} ---\")\n",
    "    try:\n",
    "        response = agent_client.prepare_agent(agentId=agent_id)\n",
    "        agent_version = response.get('agentVersion')\n",
    "        prepared_at = response.get('preparedAt')\n",
    "        status = response.get('agentStatus')\n",
    "        logger.info(f\"Agent preparation initiated for version '{agent_version}'. Status: {status}. Prepared At: {prepared_at}\")\n",
    "\n",
    "        logger.info(f\"Waiting for agent {agent_id} preparation (up to 10 minutes)...\")\n",
    "        waiter_config = {\n",
    "            'version': 2,\n",
    "            'waiters': {\n",
    "                'AgentPrepared': {\n",
    "                    'delay': 30,\n",
    "                    'operation': 'GetAgent',\n",
    "                    'maxAttempts': 20,\n",
    "                    'acceptors': [\n",
    "                        {'matcher': 'path', 'expected': 'PREPARED', 'argument': 'agent.agentStatus', 'state': 'success'},\n",
    "                        {'matcher': 'path', 'expected': 'FAILED', 'argument': 'agent.agentStatus', 'state': 'failure'},\n",
    "                        {'matcher': 'path', 'expected': 'UPDATING', 'argument': 'agent.agentStatus', 'state': 'retry'}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        waiter_model = WaiterModel(waiter_config)\n",
    "        custom_waiter = create_waiter_with_client('AgentPrepared', waiter_model, agent_client)\n",
    "\n",
    "        try:\n",
    "             custom_waiter.wait(agentId=agent_id)\n",
    "             logger.info(f\"Agent {agent_id} successfully prepared.\")\n",
    "        except Exception as wait_e:\n",
    "             logger.error(f\"Agent {agent_id} preparation failed or timed out: {wait_e}\")\n",
    "             try:\n",
    "                 final_status = agent_client.get_agent(agentId=agent_id)['agent']['agentStatus']\n",
    "                 logger.error(f\"Final agent status: {final_status}\")\n",
    "             except Exception as get_e:\n",
    "                 logger.error(f\"Could not get final agent status: {get_e}\")\n",
    "             raise Exception(f\"Agent preparation failed for {agent_id}\")\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error initiating agent preparation for {agent_id}: {e}\")\n",
    "        raise\n",
    "\n",
    "# --- Execute Agent Setup ---\n",
    "agent_name = \"couchbase_research_writer_agent_exp\" # Unique name\n",
    "agent_id = None\n",
    "agent_arn = None\n",
    "\n",
    "# Ensure Lambda ARNs are valid\n",
    "if not researcher_lambda_arn or not writer_lambda_arn:\n",
    "    raise ValueError(\"Lambda ARNs not available. Deployment likely failed.\")\n",
    "\n",
    "try:\n",
    "    # 1. Delete existing agent first\n",
    "    delete_agent_and_resources(bedrock_agent_client, agent_name)\n",
    "\n",
    "    # 2. Create the agent\n",
    "    agent_id, agent_arn = create_agent(\n",
    "        agent_client=bedrock_agent_client,\n",
    "        agent_name=agent_name,\n",
    "        agent_role_arn=agent_role_arn,\n",
    "        foundation_model_id=AGENT_MODEL_ID\n",
    "    )\n",
    "\n",
    "    # 3. Create Action Groups\n",
    "    researcher_ag_id = create_action_group(\n",
    "        agent_client=bedrock_agent_client,\n",
    "        agent_id=agent_id,\n",
    "        action_group_name=\"ResearcherActionGroup\",\n",
    "        function_arn=researcher_lambda_arn,\n",
    "        schema_path=RESEARCHER_SCHEMA_PATH\n",
    "    )\n",
    "\n",
    "    writer_ag_id = create_action_group(\n",
    "        agent_client=bedrock_agent_client,\n",
    "        agent_id=agent_id,\n",
    "        action_group_name=\"WriterActionGroup\",\n",
    "        function_arn=writer_lambda_arn,\n",
    "        schema_path=WRITER_SCHEMA_PATH\n",
    "    )\n",
    "\n",
    "    # 4. Prepare the agent\n",
    "    prepare_agent(bedrock_agent_client, agent_id)\n",
    "\n",
    "    logger.info(f\"--- Bedrock Agent '{agent_name}' (ID: {agent_id}) Setup Complete ---\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "     logger.error(f\"Agent setup failed: Schema file not found. {e}\")\n",
    "     raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Bedrock Agent setup failed: {e}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    logger.info(\"Attempting cleanup after agent setup failure...\")\n",
    "    delete_agent_and_resources(bedrock_agent_client, agent_name)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Agent Invocation\n",
    "\n",
    "Define a function to invoke the agent using the `bedrock-agent-runtime` client. Invoke the prepared agent using its ID and the special `TSTALIASID` alias (recommended for testing the DRAFT version). Provide a sample prompt and observe the agent's response and trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 07:51:57,799 - INFO - --- Testing Agent Invocation (Agent ID: QRON0D6Y95, Alias: TSTALIASID) ---\n",
      "2025-05-01 07:51:57,800 - INFO - Session ID: 3cab08ee-2e58-4bf6-959c-e462a5320283\n",
      "2025-05-01 07:51:57,803 - INFO - Prompt: \"What do you know about the Cline AI assistant? Format the answer as bullet points.\"\n",
      "2025-05-01 07:51:58,932 - INFO - Agent invocation successful. Processing response stream...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Agent Response Stream ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 07:52:15,257 - INFO - \n",
      "--- Invocation Trace Summary ---\n",
      "2025-05-01 07:52:15,257 - INFO - Trace 1: Type=None, Step=None\n",
      "2025-05-01 07:52:15,257 - INFO - Trace 2: Type=None, Step=None\n",
      "2025-05-01 07:52:15,257 - INFO - Trace 3: Type=None, Step=None\n",
      "2025-05-01 07:52:15,258 - INFO - Trace 4: Type=None, Step=None\n",
      "2025-05-01 07:52:15,258 - INFO - Trace 5: Type=None, Step=None\n",
      "2025-05-01 07:52:15,258 - INFO - Trace 6: Type=None, Step=None\n",
      "2025-05-01 07:52:15,258 - INFO - Trace 7: Type=None, Step=None\n",
      "2025-05-01 07:52:15,259 - INFO - Trace 8: Type=None, Step=None\n",
      "2025-05-01 07:52:15,259 - INFO - Trace 9: Type=None, Step=None\n",
      "2025-05-01 07:52:15,259 - INFO - Trace 10: Type=None, Step=None\n",
      "2025-05-01 07:52:15,260 - INFO - Trace 11: Type=None, Step=None\n",
      "2025-05-01 07:52:15,260 - INFO - Trace 12: Type=None, Step=None\n",
      "2025-05-01 07:52:15,260 - INFO - Trace 13: Type=None, Step=None\n",
      "2025-05-01 07:52:15,261 - INFO - --- Script Execution Finished --- \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• I do not have any information about the Cline AI assistant in my knowledge base.\n",
      "--- End of Stream ---\n",
      "\n",
      "\n",
      "--- Final Agent Response Text ---\n",
      "• I do not have any information about the Cline AI assistant in my knowledge base.\n"
     ]
    }
   ],
   "source": [
    "def test_agent_invocation(agent_runtime_client, agent_id, agent_alias_id, session_id, prompt):\n",
    "    \"\"\"Invokes the agent and prints the response stream and trace.\"\"\"\n",
    "    logger.info(f\"--- Testing Agent Invocation (Agent ID: {agent_id}, Alias: {agent_alias_id}) ---\")\n",
    "    logger.info(f\"Session ID: {session_id}\")\n",
    "    logger.info(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "\n",
    "    try:\n",
    "        response = agent_runtime_client.invoke_agent(\n",
    "            agentId=agent_id,\n",
    "            agentAliasId=agent_alias_id,\n",
    "            sessionId=session_id,\n",
    "            inputText=prompt,\n",
    "            enableTrace=True\n",
    "        )\n",
    "\n",
    "        logger.info(\"Agent invocation successful. Processing response stream...\")\n",
    "        completion_text = \"\"\n",
    "        trace_events = []\n",
    "        final_response_text = \"\"\n",
    "\n",
    "        print(\"\\n--- Agent Response Stream ---\")\n",
    "        for event in response.get('completion', []):\n",
    "            if 'chunk' in event:\n",
    "                data = event['chunk'].get('bytes', b'')\n",
    "                decoded_chunk = data.decode('utf-8')\n",
    "                print(decoded_chunk, end=\"\") # Print stream in real-time\n",
    "                final_response_text += decoded_chunk\n",
    "            elif 'trace' in event:\n",
    "                trace_part = event['trace'].get('trace')\n",
    "                if trace_part:\n",
    "                     trace_events.append(trace_part)\n",
    "            # else: # Optional: log unhandled event types\n",
    "                 # logger.warning(f\"Unhandled event type: {event}\")\n",
    "        print(\"\\n--- End of Stream ---\")\n",
    "\n",
    "        # Log trace summary\n",
    "        if trace_events:\n",
    "             logger.info(\"\\n--- Invocation Trace Summary ---\")\n",
    "             for i, trace in enumerate(trace_events):\n",
    "                  trace_type = trace.get('type')\n",
    "                  step_type = trace.get('orchestration', {}).get('stepType')\n",
    "                  rationale = trace.get('rationale', {}).get('text')\n",
    "                  observation = trace.get('observation') # Includes function results\n",
    "                  model_invocation_input = trace.get('modelInvocationInput') # Can be large\n",
    "\n",
    "                  log_line = f\"Trace {i+1}: Type={trace_type}, Step={step_type}\"\n",
    "                  if rationale: log_line += f\", Rationale='{rationale[:100]}...'\"\n",
    "                  logger.info(log_line)\n",
    "\n",
    "                  # Log observation details (e.g., function results)\n",
    "                  if observation:\n",
    "                      func_result = observation.get('finalResponse', {}).get('text')\n",
    "                      if func_result:\n",
    "                          logger.info(f\"  Observation (Function Result): {func_result[:150]}...\")\n",
    "                      # else: logger.info(f\"  Observation: {observation}\") # Log full observation if needed\n",
    "\n",
    "                  # Log model input summary (optional, can be verbose)\n",
    "                  # if model_invocation_input:\n",
    "                  #     fm_input = model_invocation_input.get('text','') or model_invocation_input.get('prompt', '')\n",
    "                  #     logger.info(f\"  Model Input: {fm_input[:150]}...\")\n",
    "        else:\n",
    "            logger.info(\"No trace events received.\")\n",
    "\n",
    "        return final_response_text\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error invoking agent: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return f\"ERROR: {e}\"\n",
    "    except Exception as e:\n",
    "         logger.error(f\"Unexpected error during agent invocation: {e}\")\n",
    "         logger.error(traceback.format_exc())\n",
    "         return f\"UNEXPECTED ERROR: {e}\"\n",
    "\n",
    "# --- Execute Test Invocation ---\n",
    "agent_alias_id = \"TSTALIASID\"\n",
    "session_id = str(uuid.uuid4())\n",
    "# test_prompt = \"What were the key findings in the llama 2 paper? Format the answer as bullet points.\"\n",
    "test_prompt = \"What do you know about the Cline AI assistant? Format the answer as bullet points.\"\n",
    "\n",
    "if agent_id and agent_alias_id:\n",
    "    agent_response = test_agent_invocation(\n",
    "        agent_runtime_client=bedrock_agent_runtime_client,\n",
    "        agent_id=agent_id,\n",
    "        agent_alias_id=agent_alias_id,\n",
    "        session_id=session_id,\n",
    "        prompt=test_prompt\n",
    "    )\n",
    "    print(f\"\\n\\n--- Final Agent Response Text ---\\n{agent_response}\")\n",
    "else:\n",
    "    logger.error(\"Agent ID or Alias ID not available, skipping invocation test.\")\n",
    "\n",
    "logger.info(\"--- Script Execution Finished --- \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Cleanup (Optional)\n",
    "\n",
    "To avoid incurring further costs, you can delete the AWS resources created by this notebook. Run the following cell to delete the Bedrock Agent (and its action groups) and the Lambda functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-01 07:52:15,268 - INFO - Cleanup skipped (RUN_CLEANUP is False).\n"
     ]
    }
   ],
   "source": [
    "# Set this to True to run the cleanup steps\n",
    "RUN_CLEANUP = False\n",
    "\n",
    "if RUN_CLEANUP:\n",
    "    logger.warning(\"--- Starting Resource Cleanup --- \")\n",
    "\n",
    "    # Delete Agent and Action Groups\n",
    "    if agent_name: # Use the name defined earlier\n",
    "        logger.info(f\"Deleting agent '{agent_name}' and its resources...\")\n",
    "        delete_agent_and_resources(bedrock_agent_client, agent_name)\n",
    "    else:\n",
    "        logger.info(\"Agent name not defined, skipping agent deletion.\")\n",
    "\n",
    "    # Delete Lambda Functions\n",
    "    if researcher_lambda_name:\n",
    "        logger.info(f\"Deleting Lambda function '{researcher_lambda_name}'...\")\n",
    "        delete_lambda_function(lambda_client, researcher_lambda_name)\n",
    "    if writer_lambda_name:\n",
    "        logger.info(f\"Deleting Lambda function '{writer_lambda_name}'...\")\n",
    "        delete_lambda_function(lambda_client, writer_lambda_name)\n",
    "\n",
    "    # Note: IAM role deletion is often manual or requires careful checks\n",
    "    # to ensure it's not used by other resources.\n",
    "    # Consider deleting the role 'bedrock_agent_lambda_exp_role' via the AWS Console/CLI\n",
    "    # if it's no longer needed.\n",
    "    logger.warning(f\"Cleanup script does not automatically delete the IAM role: {agent_role_name}\")\n",
    "    logger.warning(f\"Please delete it manually via AWS Console/CLI if no longer needed.\")\n",
    "\n",
    "    # Note: S3 bucket created for large Lambda uploads is also not deleted automatically.\n",
    "    # Check S3 for buckets named like 'lambda-deployment-*' and delete if necessary.\n",
    "    logger.warning(\"Cleanup script does not automatically delete S3 buckets used for deployment.\")\n",
    "\n",
    "    # Note: Couchbase bucket/scope/collection are not deleted.\n",
    "    logger.warning(f\"Cleanup script does not delete Couchbase resources ({CB_BUCKET_NAME}/{SCOPE_NAME}/{COLLECTION_NAME}).\")\n",
    "\n",
    "    logger.info(\"--- Resource Cleanup Attempt Complete --- \")\n",
    "else:\n",
    "    logger.info(\"Cleanup skipped (RUN_CLEANUP is False).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
