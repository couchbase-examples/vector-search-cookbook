{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bedrock Agent Experiment with Couchbase Vector Search\n",
    "\n",
    "This notebook demonstrates setting up and invoking an AWS Bedrock Agent that uses a Lambda function to perform vector searches against a Couchbase database and format the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "Import necessary libraries for AWS SDK (Boto3), Couchbase, LangChain, environment variable handling, and standard Python utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import traceback\n",
    "import uuid\n",
    "from datetime import timedelta\n",
    "\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.exceptions import \\\n",
    "    SearchIndexNotFoundException  # Added more specific exceptions\n",
    "from couchbase.exceptions import (BucketNotFoundException,\n",
    "                                  CollectionNotFoundException,\n",
    "                                  CouchbaseException,\n",
    "                                  InternalServerFailureException,\n",
    "                                  QueryIndexAlreadyExistsException,\n",
    "                                  ScopeNotFoundException,\n",
    "                                  ServiceUnavailableException)\n",
    "from couchbase.management.buckets import (BucketSettings, BucketType,\n",
    "                                          CreateBucketSettings)\n",
    "from couchbase.management.collections import CollectionSpec\n",
    "from couchbase.management.search import SearchIndex, SearchIndexManager\n",
    "from couchbase.options import ClusterOptions, QueryOptions\n",
    "from dotenv import load_dotenv\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_couchbase.vectorstores import CouchbaseSearchVectorStore\n",
    "from botocore.config import Config\n",
    "from botocore.waiter import WaiterModel, create_waiter_with_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set up logging, load environment variables from a `.env` file (if present), and define configuration constants for Couchbase, AWS, Bedrock models, and file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 19:46:43,943 - INFO - Attempting to load .env file from: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/.env\n",
      "2025-05-02 19:46:43,946 - INFO - .env file loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables from project root .env\n",
    "# In a notebook environment, '__file__' is not defined. Use a relative path or absolute path directly.\n",
    "# Assuming the notebook is run from the 'lambda-experiments' directory\n",
    "dotenv_path = os.path.join(os.getcwd(), '.env') # Or specify the full path if needed\n",
    "logger.info(f\"Attempting to load .env file from: {dotenv_path}\")\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    logger.info(\".env file loaded successfully.\")\n",
    "else:\n",
    "    # Try loading from parent directory if not found in current\n",
    "    parent_dotenv_path = os.path.join(os.path.dirname(os.getcwd()), '.env')\n",
    "    if os.path.exists(parent_dotenv_path):\n",
    "        load_dotenv(dotenv_path=parent_dotenv_path)\n",
    "        logger.info(f\".env file loaded successfully from parent directory: {parent_dotenv_path}\")\n",
    "    else:\n",
    "        logger.warning(f\".env file not found at {dotenv_path} or {parent_dotenv_path}. Relying on environment variables.\")\n",
    "\n",
    "\n",
    "# Couchbase Configuration\n",
    "CB_HOST = os.getenv(\"CB_HOST\", \"couchbase://localhost\")\n",
    "CB_USERNAME = os.getenv(\"CB_USERNAME\", \"Administrator\")\n",
    "CB_PASSWORD = os.getenv(\"CB_PASSWORD\", \"password\")\n",
    "# Using a new bucket/scope/collection for experiments to avoid conflicts\n",
    "CB_BUCKET_NAME = os.getenv(\"CB_BUCKET_NAME\", \"vector-search-exp\")\n",
    "SCOPE_NAME = os.getenv(\"SCOPE_NAME\", \"bedrock_exp\")\n",
    "COLLECTION_NAME = os.getenv(\"COLLECTION_NAME\", \"docs_exp\")\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\", \"vector_search_bedrock_exp\")\n",
    "\n",
    "# AWS Configuration\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_ACCOUNT_ID = os.getenv(\"AWS_ACCOUNT_ID\")\n",
    "\n",
    "# Bedrock Model IDs\n",
    "EMBEDDING_MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
    "AGENT_MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\" # Using Sonnet for the agent\n",
    "\n",
    "# Paths (relative to the notebook's execution directory)\n",
    "SCRIPT_DIR = os.getcwd() # Use current working directory for notebook context\n",
    "SCHEMAS_DIR = os.path.join(SCRIPT_DIR, 'schemas') # New Schemas Dir\n",
    "SEARCH_FORMAT_SCHEMA_PATH = os.path.join(SCHEMAS_DIR, 'search_and_format_schema.json') # Added\n",
    "INDEX_JSON_PATH = os.path.join(SCRIPT_DIR, 'aws_index.json') # Keep\n",
    "DOCS_JSON_PATH = os.path.join(SCRIPT_DIR, 'documents.json') # Changed to load from script's directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "Define various helper functions for:\n",
    "*   Checking environment variables.\n",
    "*   Initializing AWS clients (Boto3).\n",
    "*   Connecting to the Couchbase cluster.\n",
    "*   Setting up Couchbase buckets, scopes, and collections.\n",
    "*   Setting up Couchbase Search indexes.\n",
    "*   Clearing documents from a collection.\n",
    "*   Creating IAM roles with necessary permissions for Bedrock Agents and Lambda.\n",
    "*   Packaging and deploying Lambda functions (including S3 upload for large packages).\n",
    "*   Deleting agent resources (action groups, agent itself).\n",
    "*   Creating Bedrock Agents.\n",
    "*   Creating agent action groups (linking Lambda functions).\n",
    "*   Preparing agents (making them ready for invocation).\n",
    "*   Invoking the agent and processing the response stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 check_environment_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_environment_variables():\n",
    "    \"\"\"Check if required environment variables are set.\"\"\"\n",
    "    required_vars = [\"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\", \"AWS_ACCOUNT_ID\", \"CB_PASSWORD\"]\n",
    "    missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "    if missing_vars:\n",
    "        logger.error(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "        logger.error(\"Please set these variables in your environment or .env file\")\n",
    "        return False\n",
    "    logger.info(\"All required environment variables are set.\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 initialize_aws_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_aws_clients():\n",
    "    \"\"\"Initialize required AWS clients.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Initializing AWS clients in region: {AWS_REGION}\")\n",
    "        session = boto3.Session(\n",
    "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "            region_name=AWS_REGION\n",
    "        )\n",
    "        # Use a config with longer timeouts for agent operations\n",
    "        agent_config = Config(\n",
    "            connect_timeout=120,\n",
    "            read_timeout=600, # Agent preparation can take time\n",
    "            retries={'max_attempts': 5, 'mode': 'adaptive'}\n",
    "        )\n",
    "        bedrock_runtime = session.client('bedrock-runtime', region_name=AWS_REGION)\n",
    "        iam_client = session.client('iam', region_name=AWS_REGION) \n",
    "        lambda_client = session.client('lambda', region_name=AWS_REGION)\n",
    "        bedrock_agent_client = session.client('bedrock-agent', region_name=AWS_REGION, config=agent_config) # Add agent client\n",
    "        bedrock_agent_runtime_client = session.client('bedrock-agent-runtime', region_name=AWS_REGION, config=agent_config) # Add agent runtime client\n",
    "        logger.info(\"AWS clients initialized successfully.\")\n",
    "        return bedrock_runtime, iam_client, lambda_client, bedrock_agent_client, bedrock_agent_runtime_client # Return agent runtime client\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing AWS clients: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 connect_couchbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_couchbase():\n",
    "    \"\"\"Connect to Couchbase cluster.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Connecting to Couchbase cluster at {CB_HOST}...\")\n",
    "        auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "        # Use robust options\n",
    "        options = ClusterOptions(\n",
    "             auth,\n",
    "             # query_timeout=timedelta(seconds=75), # Example: longer timeout\n",
    "             # kv_timeout=timedelta(seconds=10)\n",
    "        )\n",
    "        cluster = Cluster(CB_HOST, options)\n",
    "        cluster.wait_until_ready(timedelta(seconds=10)) # Wait longer if needed\n",
    "        logger.info(\"Successfully connected to Couchbase.\")\n",
    "        return cluster\n",
    "    except CouchbaseException as e:\n",
    "        logger.error(f\"Couchbase connection error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error connecting to Couchbase: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 setup_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_collection(cluster, bucket_name, scope_name, collection_name):\n",
    "    \"\"\"Set up Couchbase collection (Original Logic from lamda-approach)\"\"\"\n",
    "    logger.info(f\"Setting up collection: {bucket_name}/{scope_name}/{collection_name}\")\n",
    "    try:\n",
    "        # Check if bucket exists, create if it doesn't\n",
    "        try:\n",
    "            bucket = cluster.bucket(bucket_name)\n",
    "            logger.info(f\"Bucket '{bucket_name}' exists.\")\n",
    "        except BucketNotFoundException:\n",
    "            logger.info(f\"Bucket '{bucket_name}' does not exist. Creating it...\")\n",
    "            # Use BucketSettings with potentially lower RAM for experiment\n",
    "            bucket_settings = BucketSettings(\n",
    "                name=bucket_name,\n",
    "                bucket_type=BucketType.COUCHBASE,\n",
    "                ram_quota_mb=256, # Adjusted from 1024\n",
    "                flush_enabled=True,\n",
    "                num_replicas=0\n",
    "            )\n",
    "            try:\n",
    "                 cluster.buckets().create_bucket(bucket_settings)\n",
    "                 # Wait longer after bucket creation\n",
    "                 logger.info(f\"Bucket '{bucket_name}' created. Waiting for ready state (10s)...\")\n",
    "                 time.sleep(10) \n",
    "                 bucket = cluster.bucket(bucket_name) # Re-assign bucket object\n",
    "            except Exception as create_e:\n",
    "                 logger.error(f\"Failed to create bucket '{bucket_name}': {create_e}\")\n",
    "                 raise\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error getting bucket '{bucket_name}': {e}\")\n",
    "             raise\n",
    "\n",
    "        bucket_manager = bucket.collections()\n",
    "\n",
    "        # Check if scope exists, create if it doesn't\n",
    "        scopes = bucket_manager.get_all_scopes()\n",
    "        scope_exists = any(s.name == scope_name for s in scopes)\n",
    "\n",
    "        if not scope_exists:\n",
    "            logger.info(f\"Scope '{scope_name}' does not exist. Creating it...\")\n",
    "            try:\n",
    "                 bucket_manager.create_scope(scope_name)\n",
    "                 logger.info(f\"Scope '{scope_name}' created. Waiting (2s)...\")\n",
    "                 time.sleep(2)\n",
    "            except CouchbaseException as e:\n",
    "                 # Handle potential race condition or already exists error more robustly\n",
    "                 if \"already exists\" in str(e).lower() or \"scope_exists\" in str(e).lower():\n",
    "                      logger.info(f\"Scope '{scope_name}' likely already exists (caught during creation attempt).\")\n",
    "                 else:\n",
    "                      logger.error(f\"Failed to create scope '{scope_name}': {e}\")\n",
    "                      raise\n",
    "        else:\n",
    "             logger.info(f\"Scope '{scope_name}' already exists.\")\n",
    "\n",
    "        # Check if collection exists, create if it doesn't\n",
    "        # Re-fetch scopes in case it was just created\n",
    "        scopes = bucket_manager.get_all_scopes()\n",
    "        collection_exists = False\n",
    "        for s in scopes:\n",
    "             if s.name == scope_name:\n",
    "                  if any(c.name == collection_name for c in s.collections):\n",
    "                       collection_exists = True\n",
    "                       break\n",
    "        \n",
    "        if not collection_exists:\n",
    "            logger.info(f\"Collection '{collection_name}' does not exist in scope '{scope_name}'. Creating it...\")\n",
    "            try:\n",
    "                # Use CollectionSpec\n",
    "                collection_spec = CollectionSpec(collection_name, scope_name)\n",
    "                bucket_manager.create_collection(collection_spec)\n",
    "                logger.info(f\"Collection '{collection_name}' created. Waiting (2s)...\")\n",
    "                time.sleep(2)\n",
    "            except CouchbaseException as e:\n",
    "                 if \"already exists\" in str(e).lower() or \"collection_exists\" in str(e).lower():\n",
    "                     logger.info(f\"Collection '{collection_name}' likely already exists (caught during creation attempt).\")\n",
    "                 else:\n",
    "                     logger.error(f\"Failed to create collection '{collection_name}': {e}\")\n",
    "                     raise\n",
    "        else:\n",
    "            logger.info(f\"Collection '{collection_name}' already exists.\")\n",
    "\n",
    "        # Ensure primary index exists\n",
    "        try:\n",
    "            logger.info(f\"Ensuring primary index exists on `{bucket_name}`.`{scope_name}`.`{collection_name}`...\")\n",
    "            cluster.query(f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{bucket_name}`.`{scope_name}`.`{collection_name}`\").execute()\n",
    "            logger.info(\"Primary index present or created successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating primary index: {str(e)}\")\n",
    "            # Decide if this is fatal\n",
    "\n",
    "        logger.info(\"Collection setup complete.\")\n",
    "        # Return the collection object for use\n",
    "        return cluster.bucket(bucket_name).scope(scope_name).collection(collection_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up collection: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 setup_search_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_search_index(cluster, index_name, bucket_name, scope_name, collection_name, index_definition_path):\n",
    "    \"\"\"Set up search indexes (Original Logic, adapted) \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Looking for index definition at: {index_definition_path}\")\n",
    "        if not os.path.exists(index_definition_path):\n",
    "             logger.error(f\"Index definition file not found: {index_definition_path}\")\n",
    "             raise FileNotFoundError(f\"Index definition file not found: {index_definition_path}\")\n",
    "\n",
    "        with open(index_definition_path, 'r') as file:\n",
    "            index_definition = json.load(file)\n",
    "            # Update name and source based on function arguments\n",
    "            index_definition['name'] = index_name\n",
    "            index_definition['sourceName'] = bucket_name\n",
    "            # Optional: update params to explicitly target scope.collection if needed\n",
    "            # index_definition['planParams']['indexPartitions'] = 1 # Example\n",
    "            # index_definition['params'] = {\n",
    "            #     'mapping': {\n",
    "            #         'types': {\n",
    "            #             f'{scope_name}.{collection_name}': {\n",
    "            #                 'enabled': True,\n",
    "            #                 'dynamic': True # Or specify fields\n",
    "            #             }\n",
    "            #         },\n",
    "            #         'default_mapping': {\n",
    "            #             'enabled': False\n",
    "            #         }\n",
    "            #     }\n",
    "            # }\n",
    "            logger.info(f\"Loaded index definition from {index_definition_path}, ensuring name is '{index_name}' and source is '{bucket_name}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading index definition: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        # Use the SearchIndexManager from the Cluster object for cluster-level indexes\n",
    "        # Or use scope-level if the index JSON is structured for that\n",
    "        # Assuming cluster level based on original script structure for upsert\n",
    "        search_index_manager = cluster.search_indexes()\n",
    "\n",
    "        # Create SearchIndex object from potentially modified JSON definition\n",
    "        search_index = SearchIndex.from_json(index_definition)\n",
    "\n",
    "        # Upsert the index (create if not exists, update if exists)\n",
    "        logger.info(f\"Upserting search index '{index_name}'...\")\n",
    "        search_index_manager.upsert_index(search_index)\n",
    "\n",
    "        # Wait for indexing\n",
    "        logger.info(f\"Index '{index_name}' upsert operation submitted. Waiting for indexing (10s)...\")\n",
    "        time.sleep(10)\n",
    "\n",
    "        logger.info(f\"Search index '{index_name}' setup complete.\")\n",
    "\n",
    "    except QueryIndexAlreadyExistsException:\n",
    "        # This exception might not be correct for SearchIndexManager\n",
    "        # Upsert should handle exists cases, but log potential specific errors\n",
    "        logger.warning(f\"Search index '{index_name}' likely already existed (caught QueryIndexAlreadyExistsException, check if applicable). Upsert attempted.\")\n",
    "    except CouchbaseException as e:\n",
    "        logger.error(f\"Couchbase error during search index setup for '{index_name}': {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during search index setup for '{index_name}': {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 clear_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_collection(cluster, bucket_name, scope_name, collection_name):\n",
    "    \"\"\"Delete all documents from the specified collection (Original Logic).\"\"\"\n",
    "    try:\n",
    "        logger.warning(f\"Attempting to clear all documents from `{bucket_name}`.`{scope_name}`.`{collection_name}`...\")\n",
    "        query = f\"DELETE FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
    "        result = cluster.query(query).execute()\n",
    "        # Try to get mutation count, handle if not available\n",
    "        mutation_count = 0\n",
    "        try:\n",
    "             metrics_data = result.meta_data().metrics()\n",
    "             if metrics_data:\n",
    "                  mutation_count = metrics_data.mutation_count()\n",
    "        except Exception as metrics_e:\n",
    "             logger.warning(f\"Could not retrieve mutation count after delete: {metrics_e}\")\n",
    "        logger.info(f\"Successfully cleared documents from the collection (approx. {mutation_count} mutations).\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error clearing documents from collection: {e}. Collection might be empty or index not ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 create_agent_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent_role(iam_client, role_name, aws_account_id):\n",
    "    \"\"\"Creates or gets the IAM role for the Bedrock Agent Lambda functions.\"\"\"\n",
    "    logger.info(f\"Checking/Creating IAM role: {role_name}\")\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": [\n",
    "                        \"lambda.amazonaws.com\",\n",
    "                        \"bedrock.amazonaws.com\" \n",
    "                    ]\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    role_arn = None\n",
    "    try:\n",
    "        # Check if role exists\n",
    "        get_role_response = iam_client.get_role(RoleName=role_name)\n",
    "        role_arn = get_role_response['Role']['Arn']\n",
    "        logger.info(f\"IAM role '{role_name}' already exists with ARN: {role_arn}\")\n",
    "        \n",
    "        # Ensure trust policy is up-to-date\n",
    "        logger.info(f\"Updating trust policy for existing role '{role_name}'...\")\n",
    "        iam_client.update_assume_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyDocument=json.dumps(assume_role_policy_document)\n",
    "        )\n",
    "        logger.info(f\"Trust policy updated for role '{role_name}'.\")\n",
    "\n",
    "    except iam_client.exceptions.NoSuchEntityException:\n",
    "        logger.info(f\"IAM role '{role_name}' not found. Creating...\")\n",
    "        try:\n",
    "            create_role_response = iam_client.create_role(\n",
    "                RoleName=role_name,\n",
    "                AssumeRolePolicyDocument=json.dumps(assume_role_policy_document),\n",
    "                Description='IAM role for Bedrock Agent Lambda functions (Experiment)',\n",
    "                MaxSessionDuration=3600\n",
    "            )\n",
    "            role_arn = create_role_response['Role']['Arn']\n",
    "            logger.info(f\"Successfully created IAM role '{role_name}' with ARN: {role_arn}\")\n",
    "            # Wait after role creation before attaching policies\n",
    "            logger.info(\"Waiting 15s for role creation propagation...\")\n",
    "            time.sleep(15)\n",
    "        except ClientError as e:\n",
    "            logger.error(f\"Error creating IAM role '{role_name}': {e}\")\n",
    "            raise\n",
    "            \n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error getting/updating IAM role '{role_name}': {e}\")\n",
    "        raise\n",
    "        \n",
    "    # Attach basic execution policy (idempotent)\n",
    "    try:\n",
    "        logger.info(f\"Attaching basic Lambda execution policy to role '{role_name}'...\")\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    "        )\n",
    "        logger.info(\"Attached basic Lambda execution policy.\")\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error attaching basic Lambda execution policy: {e}\")\n",
    "        # Don't necessarily raise, might already be attached or other issue\n",
    "        \n",
    "    # Add minimal inline policy for logging (can be expanded later if needed)\n",
    "    basic_inline_policy_name = \"LambdaBasicLoggingPermissions\"\n",
    "    basic_inline_policy_doc = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"logs:CreateLogGroup\",\n",
    "                    \"logs:CreateLogStream\",\n",
    "                    \"logs:PutLogEvents\"\n",
    "                ],\n",
    "                \"Resource\": f\"arn:aws:logs:{AWS_REGION}:{aws_account_id}:log-group:/aws/lambda/*:*\" # Scope down logs if possible\n",
    "            }\n",
    "            # Add S3 permissions here ONLY if Lambda code explicitly needs it\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Add Bedrock permissions policy\n",
    "    bedrock_policy_name = \"BedrockAgentPermissions\"\n",
    "    bedrock_policy_doc = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"bedrock:*\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"  # You can scope this down to specific agents/models if needed\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    try:\n",
    "        logger.info(f\"Putting basic inline policy '{basic_inline_policy_name}' for role '{role_name}'...\")\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=basic_inline_policy_name,\n",
    "            PolicyDocument=json.dumps(basic_inline_policy_doc)\n",
    "        )\n",
    "        logger.info(f\"Successfully put inline policy '{basic_inline_policy_name}'.\")\n",
    "        \n",
    "        # Add Bedrock permissions policy\n",
    "        logger.info(f\"Putting Bedrock permissions policy '{bedrock_policy_name}' for role '{role_name}'...\")\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=bedrock_policy_name,\n",
    "            PolicyDocument=json.dumps(bedrock_policy_doc)\n",
    "        )\n",
    "        logger.info(f\"Successfully put inline policy '{bedrock_policy_name}'.\")\n",
    "        \n",
    "        logger.info(\"Waiting 10s for policy changes to propagate...\")\n",
    "        time.sleep(10)\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error putting inline policy: {e}\")\n",
    "        # Decide if this is fatal\n",
    "        \n",
    "    if not role_arn:\n",
    "         raise Exception(f\"Failed to create or retrieve ARN for role {role_name}\")\n",
    "         \n",
    "    return role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Lambda Deployment Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.1 delete_lambda_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_lambda_function(lambda_client, function_name):\n",
    "    \"\"\"Delete Lambda function if it exists, attempting to remove permissions first.\"\"\"\n",
    "    logger.info(f\"Attempting to delete Lambda function: {function_name}...\")\n",
    "    try:\n",
    "        # Use a predictable statement ID added by create_lambda_function\n",
    "        statement_id = f\"AllowBedrockInvokeBasic-{function_name}\"\n",
    "        try:\n",
    "            logger.info(f\"Attempting to remove permission {statement_id} from {function_name}...\")\n",
    "            lambda_client.remove_permission(\n",
    "                FunctionName=function_name,\n",
    "                StatementId=statement_id\n",
    "            )\n",
    "            logger.info(f\"Successfully removed permission {statement_id} from {function_name}.\")\n",
    "            time.sleep(2) # Allow time for permission removal\n",
    "        except lambda_client.exceptions.ResourceNotFoundException:\n",
    "            logger.info(f\"Permission {statement_id} not found on {function_name}. Skipping removal.\")\n",
    "        except ClientError as perm_e:\n",
    "            # Log error but continue with deletion attempt\n",
    "            logger.warning(f\"Error removing permission {statement_id} from {function_name}: {str(perm_e)}\")\n",
    "\n",
    "        # Check if function exists before attempting deletion\n",
    "        lambda_client.get_function(FunctionName=function_name)\n",
    "        logger.info(f\"Function {function_name} exists. Deleting...\")\n",
    "        lambda_client.delete_function(FunctionName=function_name)\n",
    "\n",
    "        # Wait for deletion to complete using a waiter\n",
    "        logger.info(f\"Waiting for {function_name} to be deleted...\")\n",
    "        time.sleep(10) # Simple delay after delete call\n",
    "        logger.info(f\"Function {function_name} deletion initiated.\")\n",
    "\n",
    "        return True # Indicates deletion was attempted/occurred\n",
    "\n",
    "    except lambda_client.exceptions.ResourceNotFoundException:\n",
    "        logger.info(f\"Lambda function '{function_name}' does not exist. No need to delete.\")\n",
    "        return False # Indicates function didn't exist\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during deletion process for Lambda function '{function_name}': {str(e)}\")\n",
    "        # Depending on severity, might want to raise or just return False\n",
    "        return False # Indicates an error occurred beyond not found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.2 upload_to_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(zip_file, region, bucket_name=None):\n",
    "    \"\"\"Upload zip file to S3 with retry logic and return S3 location.\"\"\"\n",
    "    logger.info(f\"Preparing to upload {zip_file} to S3 in region {region}...\")\n",
    "    # Configure the client with increased timeouts\n",
    "    config = Config(\n",
    "        connect_timeout=60,\n",
    "        read_timeout=300,\n",
    "        retries={'max_attempts': 3, 'mode': 'adaptive'}\n",
    "    )\n",
    "\n",
    "    s3_client = boto3.client('s3', region_name=region, config=config)\n",
    "    sts_client = boto3.client('sts', region_name=region, config=config)\n",
    "\n",
    "    # Determine bucket name\n",
    "    if bucket_name is None:\n",
    "        try:\n",
    "            account_id = sts_client.get_caller_identity().get('Account')\n",
    "            timestamp = int(time.time())\n",
    "            bucket_name = f\"lambda-deployment-{account_id}-{timestamp}\"\n",
    "            logger.info(f\"Generated unique S3 bucket name: {bucket_name}\")\n",
    "        except Exception as e:\n",
    "            fallback_id = uuid.uuid4().hex[:12]\n",
    "            bucket_name = f\"lambda-deployment-{fallback_id}\"\n",
    "            logger.warning(f\"Error getting account ID ({e}). Using fallback bucket name: {bucket_name}\")\n",
    "\n",
    "    # Create bucket if needed\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        logger.info(f\"Using existing S3 bucket: {bucket_name}\")\n",
    "    except ClientError as e:\n",
    "        error_code = int(e.response['Error']['Code'])\n",
    "        if error_code == 404:\n",
    "            logger.info(f\"Creating S3 bucket: {bucket_name}...\")\n",
    "            try:\n",
    "                if region == 'us-east-1':\n",
    "                    s3_client.create_bucket(Bucket=bucket_name)\n",
    "                else:\n",
    "                    s3_client.create_bucket(\n",
    "                        Bucket=bucket_name,\n",
    "                        CreateBucketConfiguration={'LocationConstraint': region}\n",
    "                    )\n",
    "                logger.info(f\"Created S3 bucket: {bucket_name}. Waiting for availability...\")\n",
    "                waiter = s3_client.get_waiter('bucket_exists')\n",
    "                waiter.wait(Bucket=bucket_name, WaiterConfig={'Delay': 5, 'MaxAttempts': 12})\n",
    "                logger.info(f\"Bucket {bucket_name} is available.\")\n",
    "            except Exception as create_e:\n",
    "                logger.error(f\"Error creating bucket '{bucket_name}': {create_e}\")\n",
    "                raise\n",
    "        else:\n",
    "            logger.error(f\"Error checking bucket '{bucket_name}': {e}\")\n",
    "            raise\n",
    "\n",
    "    # Upload file\n",
    "    s3_key = f\"lambda/{os.path.basename(zip_file)}-{uuid.uuid4().hex[:8]}\"\n",
    "    try:\n",
    "        logger.info(f\"Uploading {zip_file} to s3://{bucket_name}/{s3_key}...\")\n",
    "        file_size = os.path.getsize(zip_file)\n",
    "        if file_size > 100 * 1024 * 1024:  # Use multipart for files > 100MB\n",
    "            logger.info(\"Using multipart upload for large file...\")\n",
    "            transfer_config = boto3.s3.transfer.TransferConfig(\n",
    "                multipart_threshold=10 * 1024 * 1024, max_concurrency=10,\n",
    "                multipart_chunksize=10 * 1024 * 1024, use_threads=True\n",
    "            )\n",
    "            s3_transfer = boto3.s3.transfer.S3Transfer(client=s3_client, config=transfer_config)\n",
    "            s3_transfer.upload_file(zip_file, bucket_name, s3_key)\n",
    "        else:\n",
    "            with open(zip_file, 'rb') as f:\n",
    "                s3_client.put_object(Bucket=bucket_name, Key=s3_key, Body=f)\n",
    "\n",
    "        logger.info(f\"Successfully uploaded to s3://{bucket_name}/{s3_key}\")\n",
    "        return {'S3Bucket': bucket_name, 'S3Key': s3_key}\n",
    "\n",
    "    except Exception as upload_e:\n",
    "        logger.error(f\"S3 upload failed: {upload_e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.3 package_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def package_function(function_name, source_dir, build_dir):\n",
    "    \"\"\"Package Lambda function using Makefile found in source_dir.\"\"\"\n",
    "    # source_dir is where the .py, requirements.txt, Makefile live (e.g., lambda_functions)\n",
    "    # build_dir is where packaging happens and final zip ends up (e.g., lambda-experiments)\n",
    "    makefile_path = os.path.join(source_dir, 'Makefile')\n",
    "    # Temp build dir inside source_dir, as Makefile expects relative paths\n",
    "    temp_package_dir = os.path.join(source_dir, 'package_dir') \n",
    "    # Requirements file is in source_dir\n",
    "    source_req_path = os.path.join(source_dir, 'requirements.txt') \n",
    "    # Target requirements path inside source_dir (needed for Makefile)\n",
    "    # target_req_path = os.path.join(source_dir, 'requirements.txt') # No copy needed if running make in source_dir\n",
    "    source_func_script_path = os.path.join(source_dir, f'{function_name}.py')\n",
    "    # Target function script path inside source_dir, renamed for Makefile install_deps copy\n",
    "    target_func_script_path = os.path.join(source_dir, 'lambda_function.py') \n",
    "    # Make output zip is created inside source_dir\n",
    "    make_output_zip = os.path.join(source_dir, 'lambda_package.zip') \n",
    "    # Final zip path is in the build_dir (one level up from source_dir)\n",
    "    final_zip_path = os.path.join(build_dir, f'{function_name}.zip')\n",
    "\n",
    "    logger.info(f\"--- Packaging function {function_name} --- \")\n",
    "    logger.info(f\"Source Dir (Makefile location & make cwd): {source_dir}\")\n",
    "    logger.info(f\"Build Dir (Final zip location): {build_dir}\")\n",
    "\n",
    "    if not os.path.exists(source_func_script_path):\n",
    "        raise FileNotFoundError(f\"Source function script not found: {source_func_script_path}\")\n",
    "    if not os.path.exists(source_req_path):\n",
    "        raise FileNotFoundError(f\"Source requirements file not found: {source_req_path}\")\n",
    "    if not os.path.exists(makefile_path):\n",
    "        raise FileNotFoundError(f\"Makefile not found at: {makefile_path}\")\n",
    "\n",
    "    # Ensure no leftover target script from previous failed run\n",
    "    if os.path.exists(target_func_script_path):\n",
    "        logger.warning(f\"Removing existing target script: {target_func_script_path}\")\n",
    "        os.remove(target_func_script_path)\n",
    "\n",
    "    try:\n",
    "        # 1. No need to create lambda subdir in build_dir\n",
    "\n",
    "        # 2. Copy source function script to source_dir as lambda_function.py\n",
    "        logger.info(f\"Copying {source_func_script_path} to {target_func_script_path}\")\n",
    "        shutil.copy(source_func_script_path, target_func_script_path)\n",
    "        # Requirements file is already in source_dir, no copy needed.\n",
    "\n",
    "        # 3. Run make command (execute from source_dir where Makefile is)\n",
    "        make_command = [\n",
    "            'make',\n",
    "            '-f', makefile_path, # Still specify Makefile path explicitly\n",
    "            'clean', # Clean first\n",
    "            'package',\n",
    "            # 'PYTHON_VERSION=python3.9' # Let Makefile use its default or system default\n",
    "        ]\n",
    "        logger.info(f\"Running make command: {' '.join(make_command)} (in {source_dir})\")\n",
    "        # Run make from source_dir; relative paths in Makefile should now work\n",
    "        subprocess.check_call(make_command, cwd=source_dir, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n",
    "        logger.info(\"Make command completed successfully.\")\n",
    "\n",
    "        # 4. Check for output zip in source_dir and rename/move to build_dir\n",
    "        if not os.path.exists(make_output_zip):\n",
    "            raise FileNotFoundError(f\"Makefile did not produce expected output: {make_output_zip}\")\n",
    "\n",
    "        logger.info(f\"Moving and renaming {make_output_zip} to {final_zip_path}\")\n",
    "        if os.path.exists(final_zip_path):\n",
    "             logger.warning(f\"Removing existing final zip: {final_zip_path}\")\n",
    "             os.remove(final_zip_path)\n",
    "        # Use shutil.move for cross-filesystem safety if needed, os.rename is fine here\n",
    "        os.rename(make_output_zip, final_zip_path) \n",
    "        logger.info(f\"Zip file ready: {final_zip_path}\")\n",
    "\n",
    "        return final_zip_path\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.error(f\"Error running Makefile for {function_name}: {e}\")\n",
    "        stderr_output = \"(No stderr captured)\"\n",
    "        if e.stderr:\n",
    "             try:\n",
    "                  stderr_output = e.stderr.decode()\n",
    "             except Exception:\n",
    "                  stderr_output = \"(Could not decode stderr)\"\n",
    "        logger.error(f\"Make stderr: {stderr_output}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error packaging function {function_name} using Makefile: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "    finally:\n",
    "        # 5. Clean up intermediate files in source_dir\n",
    "        if os.path.exists(target_func_script_path):\n",
    "            logger.info(f\"Cleaning up temporary script: {target_func_script_path}\")\n",
    "            os.remove(target_func_script_path)\n",
    "        if os.path.exists(make_output_zip): # If rename failed\n",
    "            logger.warning(f\"Cleaning up intermediate zip in source dir: {make_output_zip}\")\n",
    "            os.remove(make_output_zip)\n",
    "        # Consider cleaning temp_package_dir if make clean fails\n",
    "        # if os.path.exists(temp_package_dir):\n",
    "        #     logger.info(f\"Cleaning up temporary package dir: {temp_package_dir}\")\n",
    "        #     shutil.rmtree(temp_package_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.4 create_lambda_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lambda_function(lambda_client, function_name, handler, role_arn, zip_file, region):\n",
    "    \"\"\"Create or update Lambda function with retry logic.\"\"\"\n",
    "    logger.info(f\"Deploying Lambda function {function_name} from {zip_file}...\")\n",
    "\n",
    "    # Configure the client with increased timeouts for potentially long creation\n",
    "    config = Config(\n",
    "        connect_timeout=120,\n",
    "        read_timeout=300,\n",
    "        retries={'max_attempts': 5, 'mode': 'adaptive'}\n",
    "    )\n",
    "    lambda_client_local = boto3.client('lambda', region_name=region, config=config)\n",
    "\n",
    "    # Check zip file size\n",
    "    zip_size_mb = 0\n",
    "    try:\n",
    "        zip_size_bytes = os.path.getsize(zip_file)\n",
    "        zip_size_mb = zip_size_bytes / (1024 * 1024)\n",
    "        logger.info(f\"Zip file size: {zip_size_mb:.2f} MB\")\n",
    "    except OSError as e:\n",
    "         logger.error(f\"Could not get size of zip file {zip_file}: {e}\")\n",
    "         raise # Cannot proceed without zip file\n",
    "\n",
    "    use_s3 = zip_size_mb > 45 # Use S3 for packages over ~45MB\n",
    "    s3_location = None\n",
    "    zip_content = None\n",
    "\n",
    "    if use_s3:\n",
    "        logger.info(f\"Package size ({zip_size_mb:.2f} MB) requires S3 deployment.\")\n",
    "        s3_location = upload_to_s3(zip_file, region)\n",
    "        if not s3_location:\n",
    "             raise Exception(\"Failed to upload Lambda package to S3.\")\n",
    "    else:\n",
    "         logger.info(\"Deploying package directly.\")\n",
    "         try:\n",
    "             with open(zip_file, 'rb') as f:\n",
    "                 zip_content = f.read()\n",
    "         except OSError as e:\n",
    "              logger.error(f\"Could not read zip file {zip_file}: {e}\")\n",
    "              raise\n",
    "\n",
    "    # Define common create/update args\n",
    "    common_args = {\n",
    "        'FunctionName': function_name,\n",
    "        'Runtime': 'python3.9',\n",
    "        'Role': role_arn,\n",
    "        'Handler': handler,\n",
    "        'Timeout': 180,\n",
    "        'MemorySize': 1536, # Adjust as needed\n",
    "        # Env vars loaded from main script env or .env\n",
    "        'Environment': {\n",
    "            'Variables': {\n",
    "                'CB_HOST': os.getenv('CB_HOST', 'couchbase://localhost'),\n",
    "                'CB_USERNAME': os.getenv('CB_USERNAME', 'Administrator'),\n",
    "                'CB_PASSWORD': os.getenv('CB_PASSWORD', 'password'),\n",
    "                'CB_BUCKET_NAME': os.getenv('CB_BUCKET_NAME', 'vector-search-exp'),\n",
    "                'SCOPE_NAME': os.getenv('SCOPE_NAME', 'bedrock_exp'),\n",
    "                'COLLECTION_NAME': os.getenv('COLLECTION_NAME', 'docs_exp'),\n",
    "                'INDEX_NAME': os.getenv('INDEX_NAME', 'vector_search_bedrock_exp'),\n",
    "                'EMBEDDING_MODEL_ID': os.getenv('EMBEDDING_MODEL_ID', EMBEDDING_MODEL_ID),\n",
    "                'AGENT_MODEL_ID': os.getenv('AGENT_MODEL_ID', AGENT_MODEL_ID)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if use_s3:\n",
    "        code_arg = {'S3Bucket': s3_location['S3Bucket'], 'S3Key': s3_location['S3Key']}\n",
    "    else:\n",
    "        code_arg = {'ZipFile': zip_content}\n",
    "\n",
    "    max_retries = 3\n",
    "    base_delay = 10\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            logger.info(f\"Creating function '{function_name}' (attempt {attempt}/{max_retries})...\")\n",
    "            create_args = common_args.copy()\n",
    "            create_args['Code'] = code_arg\n",
    "            create_args['Publish'] = True # Publish a version\n",
    "\n",
    "            create_response = lambda_client_local.create_function(**create_args)\n",
    "            function_arn = create_response['FunctionArn']\n",
    "            logger.info(f\"Successfully created function '{function_name}' with ARN: {function_arn}\")\n",
    "\n",
    "            # Add basic invoke permission after creation\n",
    "            time.sleep(5) # Give function time to be fully created before adding policy\n",
    "            statement_id = f\"AllowBedrockInvokeBasic-{function_name}\"\n",
    "            try:\n",
    "                logger.info(f\"Adding basic invoke permission ({statement_id}) to {function_name}...\")\n",
    "                lambda_client_local.add_permission(\n",
    "                    FunctionName=function_name,\n",
    "                    StatementId=statement_id,\n",
    "                    Action='lambda:InvokeFunction',\n",
    "                    Principal='bedrock.amazonaws.com'\n",
    "                )\n",
    "                logger.info(f\"Successfully added basic invoke permission {statement_id}.\")\n",
    "            except lambda_client_local.exceptions.ResourceConflictException:\n",
    "                 logger.info(f\"Permission {statement_id} already exists for {function_name}. Skipping add.\")\n",
    "            except ClientError as perm_e:\n",
    "                logger.warning(f\"Failed to add basic invoke permission {statement_id} to {function_name}: {perm_e}\")\n",
    "\n",
    "            # Wait for function to be Active\n",
    "            logger.info(f\"Waiting for function '{function_name}' to become active...\")\n",
    "            waiter = lambda_client_local.get_waiter('function_active_v2')\n",
    "            waiter.wait(FunctionName=function_name, WaiterConfig={'Delay': 5, 'MaxAttempts': 24})\n",
    "            logger.info(f\"Function '{function_name}' is active.\")\n",
    "\n",
    "            return function_arn # Return ARN upon successful creation\n",
    "\n",
    "        except lambda_client_local.exceptions.ResourceConflictException:\n",
    "             logger.warning(f\"Function '{function_name}' already exists. Attempting to update code...\")\n",
    "             try:\n",
    "                 if use_s3:\n",
    "                     update_response = lambda_client_local.update_function_code(\n",
    "                         FunctionName=function_name,\n",
    "                         S3Bucket=s3_location['S3Bucket'],\n",
    "                         S3Key=s3_location['S3Key'],\n",
    "                         Publish=True\n",
    "                     )\n",
    "                 else:\n",
    "                     update_response = lambda_client_local.update_function_code(\n",
    "                         FunctionName=function_name,\n",
    "                         ZipFile=zip_content,\n",
    "                         Publish=True\n",
    "                     )\n",
    "                 function_arn = update_response['FunctionArn']\n",
    "                 logger.info(f\"Successfully updated function code for '{function_name}'. New version ARN: {function_arn}\")\n",
    "                 \n",
    "                 # Also update configuration just in case\n",
    "                 try:\n",
    "                      logger.info(f\"Updating configuration for '{function_name}'...\")\n",
    "                      lambda_client_local.update_function_configuration(**common_args)\n",
    "                      logger.info(f\"Configuration updated for '{function_name}'.\")\n",
    "                 except ClientError as conf_e:\n",
    "                      logger.warning(f\"Could not update configuration for '{function_name}': {conf_e}\")\n",
    "                 \n",
    "                 # Re-verify invoke permission after update\n",
    "                 time.sleep(5)\n",
    "                 statement_id = f\"AllowBedrockInvokeBasic-{function_name}\"\n",
    "                 try:\n",
    "                     logger.info(f\"Verifying/Adding basic invoke permission ({statement_id}) after update...\")\n",
    "                     lambda_client_local.add_permission(\n",
    "                         FunctionName=function_name,\n",
    "                         StatementId=statement_id,\n",
    "                         Action='lambda:InvokeFunction',\n",
    "                         Principal='bedrock.amazonaws.com'\n",
    "                     )\n",
    "                     logger.info(f\"Successfully added/verified basic invoke permission {statement_id}.\")\n",
    "                 except lambda_client_local.exceptions.ResourceConflictException:\n",
    "                     logger.info(f\"Permission {statement_id} already exists for {function_name}. Skipping add.\")\n",
    "                 except ClientError as perm_e:\n",
    "                     logger.warning(f\"Failed to add/verify basic invoke permission {statement_id} after update: {perm_e}\")\n",
    "\n",
    "                 # Wait for function to be Active after update\n",
    "                 logger.info(f\"Waiting for function '{function_name}' update to complete...\")\n",
    "                 waiter = lambda_client_local.get_waiter('function_updated_v2')\n",
    "                 waiter.wait(FunctionName=function_name, WaiterConfig={'Delay': 5, 'MaxAttempts': 24})\n",
    "                 logger.info(f\"Function '{function_name}' update complete.\")\n",
    "                 \n",
    "                 return function_arn # Return ARN after successful update\n",
    "\n",
    "             except ClientError as update_e:\n",
    "                 logger.error(f\"Failed to update function '{function_name}': {update_e}\")\n",
    "                 if attempt < max_retries:\n",
    "                      delay = base_delay * (2 ** (attempt - 1))\n",
    "                      logger.info(f\"Retrying update in {delay} seconds...\")\n",
    "                      time.sleep(delay)\n",
    "                 else:\n",
    "                      logger.error(\"Maximum update retries reached. Deployment failed.\")\n",
    "                      raise update_e\n",
    "                      \n",
    "        except ClientError as e:\n",
    "            # Handle throttling or other retryable errors\n",
    "            error_code = e.response.get('Error', {}).get('Code')\n",
    "            if error_code in ['ThrottlingException', 'ProvisionedConcurrencyConfigNotFoundException', 'EC2ThrottledException'] or 'Rate exceeded' in str(e):\n",
    "                logger.warning(f\"Retryable error on attempt {attempt}: {e}\")\n",
    "                if attempt < max_retries:\n",
    "                    delay = base_delay * (2 ** (attempt - 1)) + (uuid.uuid4().int % 5)\n",
    "                    logger.info(f\"Retrying in {delay} seconds...\")\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    logger.error(\"Maximum retries reached after retryable error. Deployment failed.\")\n",
    "                    raise e\n",
    "            else:\n",
    "                logger.error(f\"Error creating/updating Lambda '{function_name}': {e}\")\n",
    "                logger.error(traceback.format_exc()) # Log full traceback for unexpected errors\n",
    "                raise e # Re-raise non-retryable or unexpected errors\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Unexpected error during Lambda deployment: {e}\")\n",
    "             logger.error(traceback.format_exc())\n",
    "             raise e\n",
    "             \n",
    "    # If loop completes without returning, something went wrong\n",
    "    raise Exception(f\"Failed to deploy Lambda function {function_name} after {max_retries} attempts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Agent Resource Deletion Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9.1 get_agent_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_by_name(agent_client, agent_name):\n",
    "    \"\"\"Find an agent ID by its name using list_agents.\"\"\"\n",
    "    logger.info(f\"Attempting to find agent by name: {agent_name}\")\n",
    "    try:\n",
    "        paginator = agent_client.get_paginator('list_agents')\n",
    "        for page in paginator.paginate():\n",
    "            for agent_summary in page.get('agentSummaries', []):\n",
    "                if agent_summary.get('agentName') == agent_name:\n",
    "                    agent_id = agent_summary.get('agentId')\n",
    "                    logger.info(f\"Found agent '{agent_name}' with ID: {agent_id}\")\n",
    "                    return agent_id\n",
    "        logger.info(f\"Agent '{agent_name}' not found.\")\n",
    "        return None\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error listing agents to find '{agent_name}': {e}\")\n",
    "        return None # Treat as not found if error occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9.2 delete_action_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_action_group(agent_client, agent_id, action_group_id):\n",
    "    \"\"\"Deletes a specific action group for an agent.\"\"\"\n",
    "    logger.info(f\"Attempting to delete action group {action_group_id} for agent {agent_id}...\")\n",
    "    try:\n",
    "        agent_client.delete_agent_action_group(\n",
    "            agentId=agent_id,\n",
    "            agentVersion='DRAFT', # Action groups are tied to the DRAFT version\n",
    "            actionGroupId=action_group_id,\n",
    "            skipResourceInUseCheck=True # Force deletion even if in use (e.g., during prepare)\n",
    "        )\n",
    "        logger.info(f\"Successfully deleted action group {action_group_id} for agent {agent_id}.\")\n",
    "        time.sleep(5) # Short pause after deletion\n",
    "        return True\n",
    "    except agent_client.exceptions.ResourceNotFoundException:\n",
    "        logger.info(f\"Action group {action_group_id} not found for agent {agent_id}. Skipping deletion.\")\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        # Handle potential throttling or conflict if prepare is happening\n",
    "        error_code = e.response.get('Error', {}).get('Code')\n",
    "        if error_code == 'ConflictException':\n",
    "            logger.warning(f\"Conflict deleting action group {action_group_id} (agent might be preparing/busy). Retrying once after delay...\")\n",
    "            time.sleep(15)\n",
    "            try:\n",
    "                agent_client.delete_agent_action_group(\n",
    "                    agentId=agent_id, agentVersion='DRAFT', actionGroupId=action_group_id, skipResourceInUseCheck=True\n",
    "                )\n",
    "                logger.info(f\"Successfully deleted action group {action_group_id} after retry.\")\n",
    "                return True\n",
    "            except Exception as retry_e:\n",
    "                 logger.error(f\"Error deleting action group {action_group_id} on retry: {retry_e}\")\n",
    "                 return False\n",
    "        else:\n",
    "            logger.error(f\"Error deleting action group {action_group_id} for agent {agent_id}: {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9.3 delete_agent_and_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_agent_and_resources(agent_client, agent_name):\n",
    "    \"\"\"Deletes the agent and its associated action groups.\"\"\"\n",
    "    agent_id = get_agent_by_name(agent_client, agent_name)\n",
    "    if not agent_id:\n",
    "        logger.info(f\"Agent '{agent_name}' not found, no deletion needed.\")\n",
    "        return\n",
    "\n",
    "    logger.warning(f\"--- Deleting Agent Resources for '{agent_name}' (ID: {agent_id}) ---\")\n",
    "\n",
    "    # 1. Delete Action Groups\n",
    "    try:\n",
    "        logger.info(f\"Listing action groups for agent {agent_id}...\")\n",
    "        action_groups = agent_client.list_agent_action_groups(\n",
    "            agentId=agent_id,\n",
    "            agentVersion='DRAFT' # List groups for the DRAFT version\n",
    "        ).get('actionGroupSummaries', [])\n",
    "\n",
    "        if action_groups:\n",
    "            logger.info(f\"Found {len(action_groups)} action groups to delete.\")\n",
    "            for ag in action_groups:\n",
    "                delete_action_group(agent_client, agent_id, ag['actionGroupId'])\n",
    "        else:\n",
    "            logger.info(\"No action groups found to delete.\")\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error listing action groups for agent {agent_id}: {e}\")\n",
    "        # Continue to agent deletion attempt even if listing fails\n",
    "\n",
    "    # 2. Delete the Agent\n",
    "    try:\n",
    "        logger.info(f\"Attempting to delete agent {agent_id} ('{agent_name}')...\")\n",
    "        agent_client.delete_agent(agentId=agent_id, skipResourceInUseCheck=True) # Force delete\n",
    "\n",
    "        # Wait for agent deletion (custom waiter logic might be needed if no standard waiter)\n",
    "        logger.info(f\"Waiting up to 2 minutes for agent {agent_id} deletion...\")\n",
    "        deleted = False\n",
    "        for _ in range(24): # Check every 5 seconds for 2 minutes\n",
    "            try:\n",
    "                agent_client.get_agent(agentId=agent_id)\n",
    "                time.sleep(5)\n",
    "            except agent_client.exceptions.ResourceNotFoundException:\n",
    "                logger.info(f\"Agent {agent_id} successfully deleted.\")\n",
    "                deleted = True\n",
    "                break\n",
    "            except ClientError as e:\n",
    "                 # Handle potential throttling during check\n",
    "                 error_code = e.response.get('Error', {}).get('Code')\n",
    "                 if error_code == 'ThrottlingException':\n",
    "                     logger.warning(\"Throttled while checking agent deletion status, continuing wait...\")\n",
    "                     time.sleep(10)\n",
    "                 else:\n",
    "                     logger.error(f\"Error checking agent deletion status: {e}\")\n",
    "                     # Break checking loop on unexpected error\n",
    "                     break\n",
    "        if not deleted:\n",
    "             logger.warning(f\"Agent {agent_id} deletion confirmation timed out.\")\n",
    "\n",
    "    except agent_client.exceptions.ResourceNotFoundException:\n",
    "        logger.info(f\"Agent {agent_id} ('{agent_name}') already deleted or not found.\")\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error deleting agent {agent_id}: {e}\")\n",
    "\n",
    "    logger.info(f\"--- Agent Resource Deletion Complete for '{agent_name}' ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 Agent Creation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.10.1 create_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent(agent_client, agent_name, agent_role_arn, foundation_model_id):\n",
    "    \"\"\"Creates a new Bedrock Agent.\"\"\"\n",
    "    logger.info(f\"--- Creating Agent: {agent_name} ---\")\n",
    "    try:\n",
    "        # Updated Instruction for single tool\n",
    "        instruction = (\n",
    "            \"You are a helpful research assistant. Your primary function is to use the SearchAndFormat tool \"\n",
    "            \"to find relevant documents based on user queries and format them. \" \n",
    "            \"Use the user's query for the search, and specify a formatting style if requested, otherwise use the default. \"\n",
    "            \"Present the formatted results returned by the tool directly to the user.\"\n",
    "            \"Only use the tool provided. Do not add your own knowledge.\"\n",
    "        )\n",
    "\n",
    "        response = agent_client.create_agent(\n",
    "            agentName=agent_name,\n",
    "            agentResourceRoleArn=agent_role_arn,\n",
    "            foundationModel=foundation_model_id,\n",
    "            instruction=instruction,\n",
    "            idleSessionTTLInSeconds=1800, # 30 minutes\n",
    "            description=f\"Experimental agent for Couchbase search and content formatting ({foundation_model_id})\"\n",
    "            # promptOverrideConfiguration={} # Optional: Add later if needed\n",
    "        )\n",
    "        agent_info = response.get('agent')\n",
    "        agent_id = agent_info.get('agentId')\n",
    "        agent_arn = agent_info.get('agentArn')\n",
    "        agent_status = agent_info.get('agentStatus')\n",
    "        logger.info(f\"Agent creation initiated. Name: {agent_name}, ID: {agent_id}, ARN: {agent_arn}, Status: {agent_status}\")\n",
    "\n",
    "        # Wait for agent to become NOT_PREPARED (initial state after creation)\n",
    "        # Using custom waiter logic as there might not be a standard one for this transition\n",
    "        logger.info(f\"Waiting for agent {agent_id} to reach initial state...\")\n",
    "        for _ in range(12): # Check for up to 1 minute\n",
    "             current_status = agent_client.get_agent(agentId=agent_id)['agent']['agentStatus']\n",
    "             logger.info(f\"Agent {agent_id} status: {current_status}\")\n",
    "             if current_status != 'CREATING': # Expect NOT_PREPARED or FAILED\n",
    "                  break\n",
    "             time.sleep(5)\n",
    "\n",
    "        final_status = agent_client.get_agent(agentId=agent_id)['agent']['agentStatus']\n",
    "        if final_status == 'FAILED':\n",
    "            logger.error(f\"Agent {agent_id} creation failed.\")\n",
    "            # Optionally retrieve failure reasons if API provides them\n",
    "            raise Exception(f\"Agent creation failed for {agent_name}\")\n",
    "        else:\n",
    "             logger.info(f\"Agent {agent_id} successfully created (Status: {final_status}).\")\n",
    "             \n",
    "        return agent_id, agent_arn\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error creating agent '{agent_name}': {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.10.2 create_action_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_action_group(agent_client, agent_id, action_group_name, function_arn, schema_path=None):\n",
    "    \"\"\"Creates an action group for the agent using Define with function details.\"\"\"\n",
    "    logger.info(f\"--- Creating/Updating Action Group (Function Details): {action_group_name} for Agent: {agent_id} ---\")\n",
    "    logger.info(f\"Lambda ARN: {function_arn}\")\n",
    "\n",
    "    # Define function schema details (for functionSchema parameter)\n",
    "    function_schema_details = {\n",
    "        'functions': [\n",
    "            {\n",
    "                'name': 'searchAndFormatDocuments', # Function name agent will call\n",
    "                'description': 'Performs vector search based on query, retrieves documents, and formats results using specified style.',\n",
    "                'parameters': {\n",
    "                    'query': {\n",
    "                        'description': 'The search query text.',\n",
    "                        'type': 'string',\n",
    "                        'required': True\n",
    "                    },\n",
    "                    'k': {\n",
    "                        'description': 'The maximum number of documents to retrieve.',\n",
    "                        'type': 'integer',\n",
    "                        'required': False # Making optional as Lambda has default\n",
    "                    },\n",
    "                    'style': {\n",
    "                        'description': 'The desired formatting style for the results (e.g., \\'bullet points\\', \\'paragraph\\', \\'summary\\').',\n",
    "                        'type': 'string',\n",
    "                        'required': False # Making optional as Lambda has default\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Check if Action Group already exists for the DRAFT version\n",
    "        try:\n",
    "             logger.info(f\"Checking if action group '{action_group_name}' already exists for agent {agent_id} DRAFT version...\")\n",
    "             paginator = agent_client.get_paginator('list_agent_action_groups')\n",
    "             existing_group = None\n",
    "             for page in paginator.paginate(agentId=agent_id, agentVersion='DRAFT'):\n",
    "                 for ag_summary in page.get('actionGroupSummaries', []):\n",
    "                      if ag_summary.get('actionGroupName') == action_group_name:\n",
    "                           existing_group = ag_summary\n",
    "                           break\n",
    "                 if existing_group:\n",
    "                      break\n",
    "             \n",
    "             if existing_group:\n",
    "                 ag_id = existing_group['actionGroupId']\n",
    "                 logger.warning(f\"Action Group '{action_group_name}' (ID: {ag_id}) already exists for agent {agent_id} DRAFT. Attempting update to Function Details.\")\n",
    "                 # Update existing action group - REMOVE apiSchema, ADD functionSchema\n",
    "                 response = agent_client.update_agent_action_group(\n",
    "                     agentId=agent_id,\n",
    "                     agentVersion='DRAFT',\n",
    "                     actionGroupId=ag_id,\n",
    "                     actionGroupName=action_group_name,\n",
    "                     actionGroupExecutor={'lambda': function_arn},\n",
    "                     functionSchema={ # Use functionSchema\n",
    "                         'functions': function_schema_details['functions'] # Pass the list with the correct key\n",
    "                     },\n",
    "                     actionGroupState='ENABLED'\n",
    "                 )\n",
    "                 ag_info = response.get('agentActionGroup')\n",
    "                 logger.info(f\"Successfully updated Action Group '{action_group_name}' (ID: {ag_info.get('actionGroupId')}) to use Function Details.\")\n",
    "                 return ag_info.get('actionGroupId')\n",
    "             else:\n",
    "                  logger.info(f\"Action group '{action_group_name}' does not exist. Creating new with Function Details.\")\n",
    "\n",
    "        except ClientError as e:\n",
    "             logger.error(f\"Error checking for existing action group '{action_group_name}': {e}. Proceeding with creation attempt.\")\n",
    "\n",
    "\n",
    "        # Create new action group if not found or update failed implicitly\n",
    "        response = agent_client.create_agent_action_group(\n",
    "            agentId=agent_id,\n",
    "            agentVersion='DRAFT', \n",
    "            actionGroupName=action_group_name,\n",
    "            actionGroupExecutor={\n",
    "                'lambda': function_arn \n",
    "            },\n",
    "            functionSchema={ # Use functionSchema\n",
    "                 'functions': function_schema_details['functions'] # Pass the list with the correct key\n",
    "            },\n",
    "            actionGroupState='ENABLED' \n",
    "        )\n",
    "        ag_info = response.get('agentActionGroup')\n",
    "        ag_id = ag_info.get('actionGroupId')\n",
    "        logger.info(f\"Successfully created Action Group '{action_group_name}' with ID: {ag_id} using Function Details.\")\n",
    "        time.sleep(5) # Pause after creation/update\n",
    "        return ag_id\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error creating/updating action group '{action_group_name}' using Function Details: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.10.3 prepare_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_agent(agent_client, agent_id):\n",
    "    \"\"\"Prepares the DRAFT version of the agent.\"\"\"\n",
    "    logger.info(f\"--- Preparing Agent: {agent_id} ---\")\n",
    "    try:\n",
    "        response = agent_client.prepare_agent(agentId=agent_id)\n",
    "        agent_version = response.get('agentVersion') # Should be DRAFT\n",
    "        prepared_at = response.get('preparedAt')\n",
    "        status = response.get('agentStatus') # Should be PREPARING\n",
    "        logger.info(f\"Agent preparation initiated for version '{agent_version}'. Status: {status}. Prepared At: {prepared_at}\")\n",
    "\n",
    "        # Wait for preparation to complete (PREPARED or FAILED)\n",
    "        logger.info(f\"Waiting for agent {agent_id} preparation to complete (up to 10 minutes)...\")\n",
    "        # Define a simple waiter config\n",
    "        waiter_config = {\n",
    "            'version': 2,\n",
    "            'waiters': {\n",
    "                'AgentPrepared': {\n",
    "                    'delay': 30, # Check every 30 seconds\n",
    "                    'operation': 'GetAgent',\n",
    "                    'maxAttempts': 20, # Max 10 minutes\n",
    "                    'acceptors': [\n",
    "                        {\n",
    "                            'matcher': 'path',\n",
    "                            'expected': 'PREPARED',\n",
    "                            'argument': 'agent.agentStatus',\n",
    "                            'state': 'success'\n",
    "                        },\n",
    "                        {\n",
    "                            'matcher': 'path',\n",
    "                            'expected': 'FAILED',\n",
    "                            'argument': 'agent.agentStatus',\n",
    "                            'state': 'failure'\n",
    "                        },\n",
    "                        {\n",
    "                            'matcher': 'path',\n",
    "                            'expected': 'UPDATING', # Can happen during prep? Treat as retryable\n",
    "                            'argument': 'agent.agentStatus',\n",
    "                            'state': 'retry'\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        waiter_model = WaiterModel(waiter_config)\n",
    "        custom_waiter = create_waiter_with_client('AgentPrepared', waiter_model, agent_client)\n",
    "\n",
    "        try: # Outer try for both preparation and alias handling\n",
    "             custom_waiter.wait(agentId=agent_id)\n",
    "             logger.info(f\"Agent {agent_id} successfully prepared.\")\n",
    "\n",
    "        except Exception as e: # Outer except catches prepare_agent wait errors OR unhandled alias errors\n",
    "            logger.error(f\"Agent {agent_id} preparation failed or timed out (or alias error): {e}\")\n",
    "            # Check final status if possible\n",
    "            try:\n",
    "                final_status = agent_client.get_agent(agentId=agent_id)['agent']['agentStatus']\n",
    "                logger.error(f\"Final agent status: {final_status}\")\n",
    "            except Exception as get_e:\n",
    "                logger.error(f\"Could not retrieve final agent status after wait failure: {get_e}\")\n",
    "            raise Exception(f\"Agent preparation or alias setup failed for {agent_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preparing agent {agent_id}: {e}\")\n",
    "        # Handle error, maybe exit\n",
    "        raise e # Re-raise the exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11 Agent Invocation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.11.1 test_agent_invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_invocation(agent_runtime_client, agent_id, agent_alias_id, session_id, prompt):\n",
    "    \"\"\"Invokes the agent and prints the response.\"\"\"\n",
    "    logger.info(f\"--- Testing Agent Invocation (Agent ID: {agent_id}, Alias: {agent_alias_id}) ---\")\n",
    "    logger.info(f\"Session ID: {session_id}\")\n",
    "    logger.info(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "\n",
    "    try:\n",
    "        response = agent_runtime_client.invoke_agent(\n",
    "            agentId=agent_id,\n",
    "            agentAliasId=agent_alias_id,\n",
    "            sessionId=session_id,\n",
    "            inputText=prompt,\n",
    "            enableTrace=True # Enable trace for debugging\n",
    "        )\n",
    "\n",
    "        logger.info(\"Agent invocation successful. Processing response...\")\n",
    "        # print(f\"Raw Response: {response}\") # Optional: Print raw response for deep debug\n",
    "        completion_text = \"\"\n",
    "        trace_events = []\n",
    "\n",
    "        # The response is a stream. Iterate through the chunks.\n",
    "        for event in response.get('completion', []):\n",
    "            # print(f\"Event: {event}\") # Optional: Print each event\n",
    "            if 'chunk' in event:\n",
    "                data = event['chunk'].get('bytes', b'')\n",
    "                decoded_chunk = data.decode('utf-8')\n",
    "                completion_text += decoded_chunk\n",
    "                # Log chunk as it arrives (can be verbose)\n",
    "                # print(decoded_chunk, end='') \n",
    "            elif 'trace' in event:\n",
    "                trace_part = event['trace'].get('trace')\n",
    "                if trace_part:\n",
    "                     # Log the full trace part object for detailed debugging\n",
    "                    #  print(f\"Trace Event: {json.dumps(trace_part)}\")\n",
    "                     trace_events.append(trace_part)\n",
    "            else:\n",
    "                 logger.warning(f\"Unhandled event type in stream: {event}\")\n",
    "\n",
    "        # Log final combined response\n",
    "        logger.info(f\"\\n--- Agent Final Response ---\\n{completion_text}\")\n",
    "        \n",
    "        # Keep trace summary log (optional, can be removed if too verbose)\n",
    "        if trace_events:\n",
    "             logger.info(\"\\n--- Invocation Trace Summary ---\")\n",
    "             for i, trace in enumerate(trace_events):\n",
    "                  trace_type = trace.get('type')\n",
    "                  step_type = trace.get('orchestration', {}).get('stepType')\n",
    "                  model_invocation_input = trace.get('modelInvocationInput')\n",
    "                  if model_invocation_input:\n",
    "                      fm_input = model_invocation_input.get('text',\n",
    "                          json.dumps(model_invocation_input.get('invocationInput',{}).get('toolConfiguration',{})) # Handle tool input\n",
    "                      )\n",
    "                      # logger.info(f\"  Model Input Snippet: {fm_input[:150]}...\") # Can be verbose\n",
    "                  observation = trace.get('observation')\n",
    "                  # if observation:\n",
    "                      # logger.info(f\"  Observation: {observation}\") # Can be verbose\n",
    "                  log_line = f\"Trace {i+1}: Type={trace_type}, Step={step_type}\"\n",
    "                  rationale = trace.get('rationale', {}).get('text')\n",
    "                  if rationale: log_line += f\", Rationale=\\\"{rationale[:100]}...\\\"\"\n",
    "                  logger.info(log_line) # Log summary line\n",
    "\n",
    "        return completion_text\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error invoking agent: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None\n",
    "    except Exception as e:\n",
    "         logger.error(f\"Unexpected error during agent invocation: {e}\")\n",
    "         logger.error(traceback.format_exc())\n",
    "         return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Execution Flow\n",
    "\n",
    "This section executes the main logic of the script:\n",
    "1.  Performs initial checks and setup.\n",
    "2.  Sets up the Couchbase environment (bucket, scope, collection, index).\n",
    "3.  Loads data into the Couchbase vector store.\n",
    "4.  Creates the necessary IAM role.\n",
    "5.  Deploys the Lambda function.\n",
    "6.  Sets up the Bedrock Agent (deleting any previous version).\n",
    "7.  Creates the Action Group linking the agent to the Lambda.\n",
    "8.  Prepares the agent and creates/updates an alias.\n",
    "9.  Invokes the agent with a test prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 19:46:44,330 - INFO - --- Starting Bedrock Agent Experiment Script ---\n",
      "2025-05-02 19:46:44,331 - INFO - All required environment variables are set.\n",
      "2025-05-02 19:46:44,332 - INFO - Initializing AWS clients in region: us-east-1\n",
      "2025-05-02 19:46:44,694 - INFO - AWS clients initialized successfully.\n",
      "2025-05-02 19:46:44,694 - INFO - Connecting to Couchbase cluster at couchbases://cb.hlcup4o4jmjr55yf.cloud.couchbase.com...\n",
      "2025-05-02 19:46:47,386 - INFO - Successfully connected to Couchbase.\n",
      "2025-05-02 19:46:47,387 - INFO - AWS clients and Couchbase connection initialized.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"--- Starting Bedrock Agent Experiment Script ---\")\n",
    "\n",
    "if not check_environment_variables():\n",
    "    # In a notebook, raising an exception might be better than exit(1)\n",
    "    raise EnvironmentError(\"Missing required environment variables. Check logs.\")\n",
    "\n",
    "# Initialize all clients, including the agent client\n",
    "try:\n",
    "    bedrock_runtime_client, iam_client, lambda_client, bedrock_agent_client, bedrock_agent_runtime_client = initialize_aws_clients()\n",
    "    cb_cluster = connect_couchbase()\n",
    "    logger.info(\"AWS clients and Couchbase connection initialized.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Initialization failed: {e}\")\n",
    "    raise # Re-raise the exception to stop execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Couchbase Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 19:46:47,393 - INFO - Setting up collection: vector-search-testing/shared/bedrock\n",
      "2025-05-02 19:46:48,567 - INFO - Bucket 'vector-search-testing' exists.\n",
      "2025-05-02 19:46:49,472 - INFO - Scope 'shared' already exists.\n",
      "2025-05-02 19:46:50,419 - INFO - Collection 'bedrock' already exists.\n",
      "2025-05-02 19:46:50,421 - INFO - Ensuring primary index exists on `vector-search-testing`.`shared`.`bedrock`...\n",
      "2025-05-02 19:46:51,477 - INFO - Primary index present or created successfully.\n",
      "2025-05-02 19:46:51,479 - INFO - Collection setup complete.\n",
      "2025-05-02 19:46:51,480 - INFO - Couchbase collection 'vector-search-testing.shared.bedrock' setup complete.\n",
      "2025-05-02 19:46:51,481 - INFO - Looking for index definition at: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/aws_index.json\n",
      "2025-05-02 19:46:51,482 - INFO - Loaded index definition from /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/aws_index.json, ensuring name is 'vector_search_bedrock' and source is 'vector-search-testing'.\n",
      "2025-05-02 19:46:51,482 - INFO - Upserting search index 'vector_search_bedrock'...\n",
      "2025-05-02 19:46:52,150 - WARNING - Search index 'vector_search_bedrock' likely already existed (caught QueryIndexAlreadyExistsException, check if applicable). Upsert attempted.\n",
      "2025-05-02 19:46:52,154 - INFO - Couchbase search index 'vector_search_bedrock' setup complete.\n",
      "2025-05-02 19:46:52,154 - WARNING - Attempting to clear all documents from `vector-search-testing`.`shared`.`bedrock`...\n",
      "2025-05-02 19:46:52,414 - WARNING - Could not retrieve mutation count after delete: 'list' object has no attribute 'meta_data'\n",
      "2025-05-02 19:46:52,415 - INFO - Successfully cleared documents from the collection (approx. 0 mutations).\n",
      "2025-05-02 19:46:52,421 - INFO - Cleared any existing documents from the collection.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Use the setup functions with the script's config variables\n",
    "    cb_collection = setup_collection(cb_cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)\n",
    "    logger.info(f\"Couchbase collection '{CB_BUCKET_NAME}.{SCOPE_NAME}.{COLLECTION_NAME}' setup complete.\")\n",
    "\n",
    "    # Pass required args to setup_search_index\n",
    "    setup_search_index(cb_cluster, INDEX_NAME, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME, INDEX_JSON_PATH)\n",
    "    logger.info(f\"Couchbase search index '{INDEX_NAME}' setup complete.\")\n",
    "\n",
    "    # Clear any existing documents from previous runs\n",
    "    clear_collection(cb_cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)\n",
    "    logger.info(\"Cleared any existing documents from the collection.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Couchbase setup failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Vector Store Initialization and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 19:46:52,429 - INFO - Initializing Bedrock Embeddings client with model: amazon.titan-embed-text-v2:0\n",
      "2025-05-02 19:46:52,430 - INFO - Successfully created Bedrock embeddings client.\n",
      "2025-05-02 19:46:52,430 - INFO - Initializing CouchbaseSearchVectorStore with index: vector_search_bedrock\n",
      "2025-05-02 19:46:56,647 - INFO - Successfully created Couchbase vector store.\n",
      "2025-05-02 19:46:56,647 - INFO - Looking for documents at: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/documents.json\n",
      "2025-05-02 19:46:56,648 - INFO - Loaded 7 documents from /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/documents.json\n",
      "2025-05-02 19:46:56,649 - INFO - Adding 7 documents to vector store...\n",
      "2025-05-02 19:47:00,417 - INFO - Successfully added 7 documents to the vector store.\n",
      "2025-05-02 19:47:00,418 - INFO - --- Couchbase Setup and Data Loading Complete ---\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logger.info(f\"Initializing Bedrock Embeddings client with model: {EMBEDDING_MODEL_ID}\")\n",
    "    embeddings = BedrockEmbeddings(\n",
    "        client=bedrock_runtime_client,\n",
    "        model_id=EMBEDDING_MODEL_ID\n",
    "    )\n",
    "    logger.info(\"Successfully created Bedrock embeddings client.\")\n",
    "\n",
    "    logger.info(f\"Initializing CouchbaseSearchVectorStore with index: {INDEX_NAME}\")\n",
    "    vector_store = CouchbaseSearchVectorStore(\n",
    "        cluster=cb_cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=embeddings,\n",
    "        index_name=INDEX_NAME\n",
    "    )\n",
    "    logger.info(\"Successfully created Couchbase vector store.\")\n",
    "\n",
    "    # Load documents from JSON file\n",
    "    logger.info(f\"Looking for documents at: {DOCS_JSON_PATH}\")\n",
    "    if not os.path.exists(DOCS_JSON_PATH):\n",
    "         logger.error(f\"Documents file not found: {DOCS_JSON_PATH}\")\n",
    "         raise FileNotFoundError(f\"Documents file not found: {DOCS_JSON_PATH}\")\n",
    "\n",
    "    with open(DOCS_JSON_PATH, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        documents_to_load = data.get('documents', [])\n",
    "    logger.info(f\"Loaded {len(documents_to_load)} documents from {DOCS_JSON_PATH}\")\n",
    "\n",
    "    # Add documents to vector store\n",
    "    if documents_to_load:\n",
    "        logger.info(f\"Adding {len(documents_to_load)} documents to vector store...\")\n",
    "        texts = [doc.get('text', '') for doc in documents_to_load]\n",
    "        metadatas = []\n",
    "        for i, doc in enumerate(documents_to_load):\n",
    "            metadata_raw = doc.get('metadata', {})\n",
    "            if isinstance(metadata_raw, str):\n",
    "                try:\n",
    "                    metadata = json.loads(metadata_raw)\n",
    "                    if not isinstance(metadata, dict):\n",
    "                         logger.warning(f\"Metadata for doc {i} parsed from string is not a dict: {metadata}. Using empty dict.\")\n",
    "                         metadata = {}\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.warning(f\"Could not parse metadata string for doc {i}: {metadata_raw}. Using empty dict.\")\n",
    "                    metadata = {}\n",
    "            elif isinstance(metadata_raw, dict):\n",
    "                metadata = metadata_raw\n",
    "            else:\n",
    "                logger.warning(f\"Metadata for doc {i} is not a string or dict: {metadata_raw}. Using empty dict.\")\n",
    "                metadata = {}\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "        inserted_ids = vector_store.add_texts(texts=texts, metadatas=metadatas)\n",
    "        logger.info(f\"Successfully added {len(inserted_ids)} documents to the vector store.\")\n",
    "    else:\n",
    "         logger.warning(\"No documents found in the JSON file to add.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "     logger.error(f\"Setup failed: {e}\")\n",
    "     raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during vector store setup or data loading: {e}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise \n",
    "\n",
    "logger.info(\"--- Couchbase Setup and Data Loading Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Create IAM Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 19:47:00,427 - INFO - Checking/Creating IAM role: bedrock_agent_lambda_exp_role\n",
      "2025-05-02 19:47:01,422 - INFO - IAM role 'bedrock_agent_lambda_exp_role' already exists with ARN: arn:aws:iam::598307997273:role/bedrock_agent_lambda_exp_role\n",
      "2025-05-02 19:47:01,422 - INFO - Updating trust policy for existing role 'bedrock_agent_lambda_exp_role'...\n",
      "2025-05-02 19:47:01,693 - INFO - Trust policy updated for role 'bedrock_agent_lambda_exp_role'.\n",
      "2025-05-02 19:47:01,694 - INFO - Attaching basic Lambda execution policy to role 'bedrock_agent_lambda_exp_role'...\n",
      "2025-05-02 19:47:01,969 - INFO - Attached basic Lambda execution policy.\n",
      "2025-05-02 19:47:01,971 - INFO - Putting basic inline policy 'LambdaBasicLoggingPermissions' for role 'bedrock_agent_lambda_exp_role'...\n",
      "2025-05-02 19:47:02,240 - INFO - Successfully put inline policy 'LambdaBasicLoggingPermissions'.\n",
      "2025-05-02 19:47:02,241 - INFO - Putting Bedrock permissions policy 'BedrockAgentPermissions' for role 'bedrock_agent_lambda_exp_role'...\n",
      "2025-05-02 19:47:02,508 - INFO - Successfully put inline policy 'BedrockAgentPermissions'.\n",
      "2025-05-02 19:47:02,509 - INFO - Waiting 10s for policy changes to propagate...\n",
      "2025-05-02 19:47:12,515 - INFO - Agent IAM Role ARN: arn:aws:iam::598307997273:role/bedrock_agent_lambda_exp_role\n"
     ]
    }
   ],
   "source": [
    "agent_role_name = \"bedrock_agent_lambda_exp_role\"\n",
    "try:\n",
    "    # Ensure AWS_ACCOUNT_ID is loaded correctly\n",
    "    if not AWS_ACCOUNT_ID:\n",
    "        logger.info(\"Attempting to fetch AWS Account ID...\")\n",
    "        sts_client = boto3.client('sts', region_name=AWS_REGION)\n",
    "        AWS_ACCOUNT_ID = sts_client.get_caller_identity().get('Account')\n",
    "        if not AWS_ACCOUNT_ID:\n",
    "            raise ValueError(\"AWS Account ID could not be determined. Please set the AWS_ACCOUNT_ID environment variable.\")\n",
    "        logger.info(f\"Fetched AWS Account ID: {AWS_ACCOUNT_ID}\")\n",
    "        \n",
    "    agent_role_arn = create_agent_role(iam_client, agent_role_name, AWS_ACCOUNT_ID)\n",
    "    logger.info(f\"Agent IAM Role ARN: {agent_role_arn}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create/verify IAM role: {e}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Deploy Lambda Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 19:47:12,526 - INFO - --- Starting Lambda Deployment (Single Function) --- \n",
      "2025-05-02 19:47:12,527 - INFO - Deleting potentially conflicting old Lambda functions...\n",
      "2025-05-02 19:47:12,527 - INFO - Attempting to delete Lambda function: bedrock_agent_researcher_exp...\n",
      "2025-05-02 19:47:12,527 - INFO - Attempting to remove permission AllowBedrockInvokeBasic-bedrock_agent_researcher_exp from bedrock_agent_researcher_exp...\n",
      "2025-05-02 19:47:13,395 - INFO - Permission AllowBedrockInvokeBasic-bedrock_agent_researcher_exp not found on bedrock_agent_researcher_exp. Skipping removal.\n",
      "2025-05-02 19:47:13,654 - INFO - Lambda function 'bedrock_agent_researcher_exp' does not exist. No need to delete.\n",
      "2025-05-02 19:47:13,655 - INFO - Attempting to delete Lambda function: bedrock_agent_writer_exp...\n",
      "2025-05-02 19:47:13,656 - INFO - Attempting to remove permission AllowBedrockInvokeBasic-bedrock_agent_writer_exp from bedrock_agent_writer_exp...\n",
      "2025-05-02 19:47:13,893 - INFO - Permission AllowBedrockInvokeBasic-bedrock_agent_writer_exp not found on bedrock_agent_writer_exp. Skipping removal.\n",
      "2025-05-02 19:47:14,185 - INFO - Lambda function 'bedrock_agent_writer_exp' does not exist. No need to delete.\n",
      "2025-05-02 19:47:14,186 - INFO - Attempting to delete Lambda function: bedrock_agent_search_format_exp...\n",
      "2025-05-02 19:47:14,187 - INFO - Attempting to remove permission AllowBedrockInvokeBasic-bedrock_agent_search_format_exp from bedrock_agent_search_format_exp...\n",
      "2025-05-02 19:47:14,431 - INFO - Successfully removed permission AllowBedrockInvokeBasic-bedrock_agent_search_format_exp from bedrock_agent_search_format_exp.\n",
      "2025-05-02 19:47:16,754 - INFO - Function bedrock_agent_search_format_exp exists. Deleting...\n",
      "2025-05-02 19:47:17,239 - INFO - Waiting for bedrock_agent_search_format_exp to be deleted...\n",
      "2025-05-02 19:47:27,244 - INFO - Function bedrock_agent_search_format_exp deletion initiated.\n",
      "2025-05-02 19:47:27,247 - INFO - Old Lambda deletion checks complete.\n",
      "2025-05-02 19:47:27,248 - INFO - Packaging Lambda function 'bedrock_agent_search_format_exp'...\n",
      "2025-05-02 19:47:27,249 - INFO - --- Packaging function bedrock_agent_search_and_format --- \n",
      "2025-05-02 19:47:27,249 - INFO - Source Dir (Makefile location & make cwd): /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions\n",
      "2025-05-02 19:47:27,250 - INFO - Build Dir (Final zip location): /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments\n",
      "2025-05-02 19:47:27,250 - INFO - Copying /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/bedrock_agent_search_and_format.py to /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/lambda_function.py\n",
      "2025-05-02 19:47:27,253 - INFO - Running make command: make -f /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/Makefile clean package (in /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions)\n",
      "2025-05-02 19:47:52,445 - INFO - Make command completed successfully.\n",
      "2025-05-02 19:47:52,447 - INFO - Moving and renaming /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/lambda_package.zip to /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_search_and_format.zip\n",
      "2025-05-02 19:47:52,447 - INFO - Zip file ready: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_search_and_format.zip\n",
      "2025-05-02 19:47:52,448 - INFO - Cleaning up temporary script: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/lambda_function.py\n",
      "2025-05-02 19:47:52,448 - INFO - Lambda function packaged at: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_search_and_format.zip\n",
      "2025-05-02 19:47:52,448 - INFO - Creating/Updating Lambda function 'bedrock_agent_search_format_exp'...\n",
      "2025-05-02 19:47:52,449 - INFO - Deploying Lambda function bedrock_agent_search_format_exp from /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_search_and_format.zip...\n",
      "2025-05-02 19:47:52,459 - INFO - Found credentials in environment variables.\n",
      "2025-05-02 19:47:52,561 - INFO - Zip file size: 50.57 MB\n",
      "2025-05-02 19:47:52,562 - INFO - Package size (50.57 MB) requires S3 deployment.\n",
      "2025-05-02 19:47:52,562 - INFO - Preparing to upload /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_search_and_format.zip to S3 in region us-east-1...\n",
      "2025-05-02 19:47:53,517 - INFO - Generated unique S3 bucket name: lambda-deployment-598307997273-1746195473\n",
      "2025-05-02 19:47:54,257 - INFO - Creating S3 bucket: lambda-deployment-598307997273-1746195473...\n",
      "2025-05-02 19:47:54,642 - INFO - Created S3 bucket: lambda-deployment-598307997273-1746195473. Waiting for availability...\n",
      "2025-05-02 19:47:54,917 - INFO - Bucket lambda-deployment-598307997273-1746195473 is available.\n",
      "2025-05-02 19:47:54,917 - INFO - Uploading /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_search_and_format.zip to s3://lambda-deployment-598307997273-1746195473/lambda/bedrock_agent_search_and_format.zip-86de95de...\n",
      "2025-05-02 19:50:25,766 - INFO - Successfully uploaded to s3://lambda-deployment-598307997273-1746195473/lambda/bedrock_agent_search_and_format.zip-86de95de\n",
      "2025-05-02 19:50:25,771 - INFO - Creating function 'bedrock_agent_search_format_exp' (attempt 1/3)...\n",
      "2025-05-02 19:50:29,443 - INFO - Successfully created function 'bedrock_agent_search_format_exp' with ARN: arn:aws:lambda:us-east-1:598307997273:function:bedrock_agent_search_format_exp\n",
      "2025-05-02 19:50:34,448 - INFO - Adding basic invoke permission (AllowBedrockInvokeBasic-bedrock_agent_search_format_exp) to bedrock_agent_search_format_exp...\n",
      "2025-05-02 19:50:34,889 - INFO - Successfully added basic invoke permission AllowBedrockInvokeBasic-bedrock_agent_search_format_exp.\n",
      "2025-05-02 19:50:34,890 - INFO - Waiting for function 'bedrock_agent_search_format_exp' to become active...\n",
      "2025-05-02 19:50:35,228 - INFO - Function 'bedrock_agent_search_format_exp' is active.\n",
      "2025-05-02 19:50:35,231 - INFO - Search/Format Lambda Deployed: arn:aws:lambda:us-east-1:598307997273:function:bedrock_agent_search_format_exp\n",
      "2025-05-02 19:50:35,231 - INFO - Cleaning up deployment zip file...\n",
      "2025-05-02 19:50:35,233 - INFO - Removed zip file: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_search_and_format.zip\n",
      "2025-05-02 19:50:35,234 - INFO - --- Lambda Deployment Complete --- \n"
     ]
    }
   ],
   "source": [
    "search_format_lambda_name = \"bedrock_agent_search_format_exp\"\n",
    "# Adjust source/build dirs for notebook context if necessary\n",
    "lambda_source_dir = os.path.join(SCRIPT_DIR, 'lambda_functions') \n",
    "lambda_build_dir = SCRIPT_DIR # Final zip ends up in the notebook's directory\n",
    "\n",
    "logger.info(\"--- Starting Lambda Deployment (Single Function) --- \")\n",
    "search_format_lambda_arn = None\n",
    "search_format_zip_path = None\n",
    "\n",
    "try:\n",
    "    # Delete old lambdas if they exist (optional, but good cleanup)\n",
    "    logger.info(\"Deleting potentially conflicting old Lambda functions...\")\n",
    "    delete_lambda_function(lambda_client, \"bedrock_agent_researcher_exp\")\n",
    "    delete_lambda_function(lambda_client, \"bedrock_agent_writer_exp\")\n",
    "    # Delete the new lambda if it exists from a previous run\n",
    "    delete_lambda_function(lambda_client, search_format_lambda_name)\n",
    "    logger.info(\"Old Lambda deletion checks complete.\")\n",
    "\n",
    "    logger.info(f\"Packaging Lambda function '{search_format_lambda_name}'...\")\n",
    "    search_format_zip_path = package_function(\"bedrock_agent_search_and_format\", lambda_source_dir, lambda_build_dir)\n",
    "    logger.info(f\"Lambda function packaged at: {search_format_zip_path}\")\n",
    "\n",
    "    logger.info(f\"Creating/Updating Lambda function '{search_format_lambda_name}'...\")\n",
    "    search_format_lambda_arn = create_lambda_function(\n",
    "        lambda_client=lambda_client, function_name=search_format_lambda_name,\n",
    "        handler='lambda_function.lambda_handler', role_arn=agent_role_arn,\n",
    "        zip_file=search_format_zip_path, region=AWS_REGION\n",
    "    )\n",
    "    logger.info(f\"Search/Format Lambda Deployed: {search_format_lambda_arn}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "     logger.error(f\"Lambda packaging failed: Required file not found. {e}\")\n",
    "     raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Lambda deployment failed: {e}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise\n",
    "finally:\n",
    "    logger.info(\"Cleaning up deployment zip file...\")\n",
    "    if search_format_zip_path and os.path.exists(search_format_zip_path):\n",
    "        try:\n",
    "            os.remove(search_format_zip_path)\n",
    "            logger.info(f\"Removed zip file: {search_format_zip_path}\")\n",
    "        except OSError as e:\n",
    "            logger.warning(f\"Could not remove zip file {search_format_zip_path}: {e}\")\n",
    "\n",
    "logger.info(\"--- Lambda Deployment Complete --- \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 19:50:35,243 - INFO - Checking for and deleting existing agent: couchbase_search_format_agent_exp\n",
      "2025-05-02 19:50:35,244 - INFO - Attempting to find agent by name: couchbase_search_format_agent_exp\n",
      "2025-05-02 19:50:36,151 - INFO - Found agent 'couchbase_search_format_agent_exp' with ID: FYEP37WCIK\n",
      "2025-05-02 19:50:36,152 - WARNING - --- Deleting Agent Resources for 'couchbase_search_format_agent_exp' (ID: FYEP37WCIK) ---\n",
      "2025-05-02 19:50:36,152 - INFO - Listing action groups for agent FYEP37WCIK...\n",
      "2025-05-02 19:50:36,478 - INFO - Found 1 action groups to delete.\n",
      "2025-05-02 19:50:36,479 - INFO - Attempting to delete action group UNXIVAWF9V for agent FYEP37WCIK...\n",
      "2025-05-02 19:50:36,821 - INFO - Successfully deleted action group UNXIVAWF9V for agent FYEP37WCIK.\n",
      "2025-05-02 19:50:41,827 - INFO - Attempting to delete agent FYEP37WCIK ('couchbase_search_format_agent_exp')...\n",
      "2025-05-02 19:50:42,166 - INFO - Waiting up to 2 minutes for agent FYEP37WCIK deletion...\n",
      "2025-05-02 19:50:48,517 - INFO - Agent FYEP37WCIK successfully deleted.\n",
      "2025-05-02 19:50:48,517 - INFO - --- Agent Resource Deletion Complete for 'couchbase_search_format_agent_exp' ---\n",
      "2025-05-02 19:50:48,517 - INFO - Deletion process completed for any existing agent named couchbase_search_format_agent_exp.\n",
      "2025-05-02 19:50:48,518 - INFO - --- Creating Agent: couchbase_search_format_agent_exp ---\n",
      "2025-05-02 19:50:48,518 - INFO - --- Creating Agent: couchbase_search_format_agent_exp ---\n",
      "2025-05-02 19:50:49,063 - INFO - Agent creation initiated. Name: couchbase_search_format_agent_exp, ID: X2C1OD9EAT, ARN: arn:aws:bedrock:us-east-1:598307997273:agent/X2C1OD9EAT, Status: CREATING\n",
      "2025-05-02 19:50:49,064 - INFO - Waiting for agent X2C1OD9EAT to reach initial state...\n",
      "2025-05-02 19:50:49,514 - INFO - Agent X2C1OD9EAT status: CREATING\n",
      "2025-05-02 19:50:54,958 - INFO - Agent X2C1OD9EAT status: NOT_PREPARED\n",
      "2025-05-02 19:50:55,364 - INFO - Agent X2C1OD9EAT successfully created (Status: NOT_PREPARED).\n",
      "2025-05-02 19:50:55,365 - INFO - Agent created successfully. ID: X2C1OD9EAT, ARN: arn:aws:bedrock:us-east-1:598307997273:agent/X2C1OD9EAT\n"
     ]
    }
   ],
   "source": [
    "agent_name = f\"couchbase_search_format_agent_exp\"\n",
    "agent_id = None\n",
    "agent_arn = None\n",
    "alias_name = \"prod\" # Define alias name here\n",
    "# agent_alias_id_to_use will be set later after preparation\n",
    "\n",
    "# 1. Attempt to find and delete existing agent to ensure a clean state\n",
    "logger.info(f\"Checking for and deleting existing agent: {agent_name}\")\n",
    "try:\n",
    "    delete_agent_and_resources(bedrock_agent_client, agent_name) # Handles finding and deleting\n",
    "    logger.info(f\"Deletion process completed for any existing agent named {agent_name}.\")\n",
    "except Exception as e:\n",
    "    # Log error during find/delete but proceed to creation attempt\n",
    "    logger.error(f\"Error during agent finding/deletion phase: {e}. Proceeding to creation attempt.\")\n",
    "\n",
    "# 2. Always attempt to create the agent after the delete phase\n",
    "logger.info(f\"--- Creating Agent: {agent_name} ---\")\n",
    "try:\n",
    "    agent_id, agent_arn = create_agent(\n",
    "        agent_client=bedrock_agent_client,\n",
    "        agent_name=agent_name,\n",
    "        agent_role_arn=agent_role_arn,\n",
    "        foundation_model_id=AGENT_MODEL_ID\n",
    "    )\n",
    "    if not agent_id:\n",
    "        raise Exception(\"create_agent function did not return a valid agent ID.\")\n",
    "    logger.info(f\"Agent created successfully. ID: {agent_id}, ARN: {agent_arn}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create agent '{agent_name}': {e}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Action Group Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 19:50:55,371 - INFO - Creating/Updating Action Group 'SearchAndFormatActionGroup' for agent X2C1OD9EAT...\n",
      "2025-05-02 19:50:55,372 - INFO - --- Creating/Updating Action Group (Function Details): SearchAndFormatActionGroup for Agent: X2C1OD9EAT ---\n",
      "2025-05-02 19:50:55,372 - INFO - Lambda ARN: arn:aws:lambda:us-east-1:598307997273:function:bedrock_agent_search_format_exp\n",
      "2025-05-02 19:50:55,373 - INFO - Checking if action group 'SearchAndFormatActionGroup' already exists for agent X2C1OD9EAT DRAFT version...\n",
      "2025-05-02 19:50:55,760 - INFO - Action group 'SearchAndFormatActionGroup' does not exist. Creating new with Function Details.\n",
      "2025-05-02 19:50:56,228 - INFO - Successfully created Action Group 'SearchAndFormatActionGroup' with ID: PLC2S67Z8C using Function Details.\n",
      "2025-05-02 19:51:01,232 - INFO - Action Group 'SearchAndFormatActionGroup' created/updated with ID: PLC2S67Z8C\n",
      "2025-05-02 19:51:01,233 - INFO - Waiting 30s after action group setup before preparing agent...\n"
     ]
    }
   ],
   "source": [
    "# --- Action Group Creation/Update (Now assumes agent_id is valid) ---\n",
    "action_group_name = \"SearchAndFormatActionGroup\"\n",
    "action_group_id = None\n",
    "try:\n",
    "    if not agent_id:\n",
    "        raise ValueError(\"Agent ID is not set. Cannot create action group.\")\n",
    "    if not search_format_lambda_arn:\n",
    "        raise ValueError(\"Lambda ARN is not set. Cannot create action group.\")\n",
    "        \n",
    "    logger.info(f\"Creating/Updating Action Group '{action_group_name}' for agent {agent_id}...\")\n",
    "    action_group_id = create_action_group(\n",
    "        agent_client=bedrock_agent_client,\n",
    "        agent_id=agent_id,\n",
    "        action_group_name=action_group_name,\n",
    "        function_arn=search_format_lambda_arn,\n",
    "        # schema_path=None # No longer needed explicitly if default is None\n",
    "    )\n",
    "    if not action_group_id:\n",
    "        raise Exception(\"create_action_group did not return a valid ID.\")\n",
    "    logger.info(f\"Action Group '{action_group_name}' created/updated with ID: {action_group_id}\")\n",
    "\n",
    "    # Add a slightly longer wait after action group modification/creation\n",
    "    logger.info(\"Waiting 30s after action group setup before preparing agent...\")\n",
    "    time.sleep(30)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to set up action group: {e}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Prepare Agent and Handle Alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 19:51:31,249 - INFO - --- Preparing Agent: X2C1OD9EAT ---\n",
      "2025-05-02 19:51:31,250 - INFO - --- Preparing Agent: X2C1OD9EAT ---\n",
      "2025-05-02 19:51:31,775 - INFO - Agent preparation initiated for version 'DRAFT'. Status: PREPARING. Prepared At: 2025-05-02 14:21:31.718556+00:00\n",
      "2025-05-02 19:51:31,776 - INFO - Waiting for agent X2C1OD9EAT preparation to complete (up to 10 minutes)...\n",
      "2025-05-02 19:52:02,737 - INFO - Agent X2C1OD9EAT successfully prepared.\n",
      "2025-05-02 19:52:02,738 - INFO - Agent X2C1OD9EAT preparation seems complete (waiter succeeded).\n",
      "2025-05-02 19:52:02,739 - INFO - --- Setting up Alias 'prod' for Agent X2C1OD9EAT ---\n",
      "2025-05-02 19:52:02,739 - INFO - Checking for alias 'prod' for agent X2C1OD9EAT...\n",
      "2025-05-02 19:52:03,039 - INFO - Alias 'prod' not found. Creating new alias...\n",
      "2025-05-02 19:52:03,425 - INFO - Successfully created alias 'prod' with ID: V4OM2MRUSG. (Defaults to latest prepared version - DRAFT)\n",
      "2025-05-02 19:52:03,425 - INFO - Waiting 10s for alias 'prod' changes to propagate...\n",
      "2025-05-02 19:52:13,431 - INFO - Agent X2C1OD9EAT preparation and alias 'prod' (V4OM2MRUSG) setup complete.\n"
     ]
    }
   ],
   "source": [
    "agent_alias_id_to_use = None # Initialize alias ID\n",
    "alias_name = \"prod\" # Make sure alias_name is defined\n",
    "if agent_id:\n",
    "    logger.info(f\"--- Preparing Agent: {agent_id} ---\")\n",
    "    preparation_successful = False\n",
    "    try:\n",
    "        # prepare_agent now ONLY prepares, doesn't handle alias or return its ID\n",
    "        prepare_agent(bedrock_agent_client, agent_id) \n",
    "        logger.info(f\"Agent {agent_id} preparation seems complete (waiter succeeded).\")\n",
    "        preparation_successful = True # Flag success\n",
    "\n",
    "    except Exception as e: # Catch errors from preparation\n",
    "        logger.error(f\"Error during agent preparation for {agent_id}: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise # Stop execution if preparation fails\n",
    "\n",
    "    # --- Alias Handling (runs only if preparation succeeded) ---\n",
    "    if preparation_successful:\n",
    "        logger.info(f\"--- Setting up Alias '{alias_name}' for Agent {agent_id} ---\") # Add log\n",
    "        try:\n",
    "            # --- Alias Creation/Update Logic (Copied/adapted from main.py's __main__) ---\n",
    "            logger.info(f\"Checking for alias '{alias_name}' for agent {agent_id}...\")\n",
    "            existing_alias = None\n",
    "            paginator = bedrock_agent_client.get_paginator('list_agent_aliases')\n",
    "            for page in paginator.paginate(agentId=agent_id):\n",
    "                for alias_summary in page.get('agentAliasSummaries', []):\n",
    "                    if alias_summary.get('agentAliasName') == alias_name:\n",
    "                        existing_alias = alias_summary\n",
    "                        break\n",
    "                if existing_alias:\n",
    "                    break\n",
    "            \n",
    "            if existing_alias:\n",
    "                agent_alias_id_to_use = existing_alias['agentAliasId']\n",
    "                logger.info(f\"Using existing alias '{alias_name}' with ID: {agent_alias_id_to_use}.\")\n",
    "                # Optional: Update alias to point to DRAFT if needed, \n",
    "                # but create_agent_alias defaults to latest prepared (DRAFT) so just checking existence is often enough.\n",
    "            else:\n",
    "                logger.info(f\"Alias '{alias_name}' not found. Creating new alias...\")\n",
    "                create_alias_response = bedrock_agent_client.create_agent_alias(\n",
    "                    agentId=agent_id,\n",
    "                    agentAliasName=alias_name\n",
    "                    # routingConfiguration removed - defaults to latest prepared (DRAFT)\n",
    "                )\n",
    "                agent_alias_id_to_use = create_alias_response.get('agentAlias', {}).get('agentAliasId')\n",
    "                logger.info(f\"Successfully created alias '{alias_name}' with ID: {agent_alias_id_to_use}. (Defaults to latest prepared version - DRAFT)\")\n",
    "\n",
    "            if not agent_alias_id_to_use:\n",
    "                    raise ValueError(f\"Failed to get a valid alias ID for '{alias_name}'\")\n",
    "\n",
    "            logger.info(f\"Waiting 10s for alias '{alias_name}' changes to propagate...\")\n",
    "            time.sleep(10)\n",
    "            logger.info(f\"Agent {agent_id} preparation and alias '{alias_name}' ({agent_alias_id_to_use}) setup complete.\")\n",
    "\n",
    "\n",
    "        except Exception as alias_e: # Catch errors from alias logic\n",
    "            logger.error(f\"Failed to create/update alias '{alias_name}' for agent {agent_id}: {alias_e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            raise # Use raise for notebook context\n",
    "        # End of Alias specific try/except\n",
    "    # End of if preparation_successful\n",
    "else:\n",
    "    logger.error(\"Agent ID not available, skipping preparation and alias setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Test Agent Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 19:52:13,440 - INFO - --- Invoking Agent X2C1OD9EAT using Alias 'prod' (V4OM2MRUSG) ---\n",
      "2025-05-02 19:52:13,440 - INFO - --- Testing Agent Invocation (Agent ID: X2C1OD9EAT, Alias: V4OM2MRUSG) ---\n",
      "2025-05-02 19:52:13,441 - INFO - Session ID: cdbb2c41-0b44-4878-92ff-27e16910f9e2\n",
      "2025-05-02 19:52:13,441 - INFO - Prompt: \"Search for information about Project Chimera and format the results using bullet points.\"\n",
      "2025-05-02 19:52:14,496 - INFO - Agent invocation successful. Processing response...\n",
      "2025-05-02 19:52:31,303 - INFO - \n",
      "--- Agent Final Response ---\n",
      " Project Chimera combines quantum entanglement communication with neural networks for secure, real-time data analysis across distributed nodes. Lead developer: Dr. Aris Thorne.\n",
      "\n",
      " Chimera operates in two modes:\n",
      "    - 'Quantum Sync' for high-fidelity data transfer\n",
      "    - 'Neural Inference' for localized edge processing based on the synced data.\n",
      "\n",
      " A key aspect of Chimera is its \"Ephemeral Key Protocol\" (EKP), which generates one-time quantum keys for each transmission, ensuring absolute forward secrecy.\n",
      "2025-05-02 19:52:31,304 - INFO - \n",
      "--- Invocation Trace Summary ---\n",
      "2025-05-02 19:52:31,305 - INFO - Trace 1: Type=None, Step=None\n",
      "2025-05-02 19:52:31,305 - INFO - Trace 2: Type=None, Step=None\n",
      "2025-05-02 19:52:31,308 - INFO - Trace 3: Type=None, Step=None\n",
      "2025-05-02 19:52:31,308 - INFO - Trace 4: Type=None, Step=None\n",
      "2025-05-02 19:52:31,309 - INFO - Trace 5: Type=None, Step=None\n",
      "2025-05-02 19:52:31,309 - INFO - Trace 6: Type=None, Step=None\n",
      "2025-05-02 19:52:31,309 - INFO - Trace 7: Type=None, Step=None\n",
      "2025-05-02 19:52:31,309 - INFO - Trace 8: Type=None, Step=None\n"
     ]
    }
   ],
   "source": [
    "# --- Test Invocation ---\n",
    "# Agent ID and custom alias ID should be valid here\n",
    "if agent_id and agent_alias_id_to_use: # Check both are set\n",
    "     session_id = str(uuid.uuid4()) \n",
    "     test_prompt = \"Search for information about Project Chimera and format the results using bullet points.\"\n",
    "     logger.info(f\"--- Invoking Agent {agent_id} using Alias '{alias_name}' ({agent_alias_id_to_use}) ---\") # Updated log\n",
    "     try:\n",
    "         completion = test_agent_invocation(\n",
    "             agent_runtime_client=bedrock_agent_runtime_client,\n",
    "             agent_id=agent_id,\n",
    "             agent_alias_id=agent_alias_id_to_use, \n",
    "             session_id=session_id,\n",
    "             prompt=test_prompt\n",
    "         )\n",
    "         if completion is None:\n",
    "              logger.error(\"Agent invocation failed.\")\n",
    "         # The function already logs the final response\n",
    "     except Exception as e:\n",
    "          logger.error(f\"Error during test invocation: {e}\")\n",
    "          logger.error(traceback.format_exc())\n",
    "else:\n",
    "     logger.error(\"Agent ID or Alias ID not available, skipping invocation test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 Script Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 19:52:31,314 - INFO - --- Script Execution Finished (Agent Setup & Test Complete) --- \n"
     ]
    }
   ],
   "source": [
    "logger.info(\"--- Script Execution Finished (Agent Setup & Test Complete) --- \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
