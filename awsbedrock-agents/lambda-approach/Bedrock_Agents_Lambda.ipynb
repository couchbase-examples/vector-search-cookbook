{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Bedrock Agents with Couchbase Vector Search - Lambda Approach\n",
    "\n",
    "This notebook demonstrates the Lambda approach for implementing AWS Bedrock agents with Couchbase Vector Search. In this approach, the agent invokes AWS Lambda functions to execute operations.\n",
    "\n",
    "We'll implement a multi-agent architecture with specialized agents for different tasks:\n",
    "- **Researcher Agent**: Searches for relevant documents in the vector store\n",
    "- **Writer Agent**: Formats and presents the research findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Approaches\n",
    "\n",
    "This notebook demonstrates the Lambda Approach for AWS Bedrock Agents. For comparison, you might also want to check out the Custom Control Approach, which handles agent tools directly in your application code instead of using AWS Lambda functions.\n",
    "\n",
    "The Custom Control approach offers simpler setup and more direct control, but may not scale as well. You can find that implementation here: [Custom Control Approach Notebook](https://github.com/couchbase-examples/vector-search-cookbook/blob/main/awsbedrock-agents/custom-control-approach/Bedrock_Agents_Custom_Control.ipynb)\n",
    "\n",
    "Note: If the link above doesn't work in your Jupyter environment, you can navigate to the file manually in the `awsbedrock-agents/custom-control-approach/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The Lambda approach for AWS Bedrock Agents delegates the execution of an agent's defined functions (tools) to backend AWS Lambda functions. When the agent decides to use a tool, Bedrock directly invokes the corresponding Lambda function that you've specified in the agent's action group configuration. This Lambda function receives the parameters from the agent, executes the necessary logic (e.g., querying a Couchbase vector store, calling other APIs, performing computations), and then returns the result to the Bedrock Agent. The agent can then use this result to continue its reasoning process or formulate a final response to the user. This architecture promotes a clean separation of concerns, allows tool logic to be developed and scaled independently, and leverages the serverless capabilities of AWS Lambda.\n",
    "\n",
    "## Key Steps & Concepts\n",
    "\n",
    "1.  **Define Agent Instructions & Tool Schema:**\n",
    "    *   **Instructions:** Craft a clear prompt that tells the agent its purpose, capabilities, and how it should behave (e.g., \"You are a research assistant that uses the SearchAndFormat tool...\").\n",
    "    *   **Function Schema:** Define the structure of the tool(s) the agent can use. In this notebook, we define a single tool (e.g., `searchAndFormatDocuments`) that the agent will call. This schema specifies the function name, description, and its input parameters (e.g., `query`, `k`, `style`). This schema acts as the contract between the agent and the Lambda function.\n",
    "\n",
    "2.  **Implement Lambda Handler Function:**\n",
    "    *   Create an AWS Lambda function (e.g., `bedrock_agent_search_and_format.py`) that contains the actual Python code to execute the tool's logic. \n",
    "    *   **Event Handling:** The Lambda handler receives an event payload from Bedrock. This payload includes details like the API path (which corresponds to the function name in the schema), HTTP method, and the parameters supplied by the agent.\n",
    "    *   **Business Logic:** Inside the Lambda, parse the incoming event, extract parameters, and perform the required actions. For this notebook, this involves: \n",
    "        *   Connecting to Couchbase.\n",
    "        *   Initializing the Bedrock Embeddings client.\n",
    "        *   Performing a vector similarity search using the provided query and `k` value.\n",
    "        *   Optionally, formatting the search results based on the `style` parameter (though in this specific example, the formatting is largely illustrative and the LLM does the heavy lifting of presentation).\n",
    "    *   **Response Structure:** The Lambda must return a JSON response in a specific format that Bedrock expects. This response typically includes the `actionGroup`, `apiPath`, `httpMethod`, `httpStatusCode`, and a `responseBody` containing the result of the tool execution (e.g., the search results as a string). \n",
    "    *   **Deployment:** Package the Lambda function with its dependencies (e.g., `requirements.txt`) into a .zip file. This notebook includes helper functions to automate packaging (using a `Makefile`) and deployment, including uploading to S3 if the package is large. The Lambda also needs an IAM role with permissions to run, write logs, and interact with Bedrock and any other required AWS services.\n",
    "    *   **Environment Variables:** The Lambda function is configured with environment variables (e.g., Couchbase connection details, Bedrock model IDs) to allow it to connect to necessary services without hardcoding credentials. These are set during the Lambda creation/update process in the notebook.\n",
    "\n",
    "3.  **Create Agent in AWS Bedrock:**\n",
    "    *   Use the `bedrock_agent_client.create_agent` SDK call. Provide the agent name, the ARN of the IAM role it will assume, the foundation model ID (e.g., Claude Sonnet), and the instructions defined in step 1.\n",
    "\n",
    "4.  **Create Agent Action Group (Linking to Lambda):**\n",
    "    *   Use `bedrock_agent_client.create_agent_action_group`.\n",
    "    *   **`actionGroupExecutor`:** This is the crucial part for the Lambda approach. Set it to `{'lambda': 'arn:aws:lambda:<region>:<account_id>:function:<lambda_function_name>'}`. This tells Bedrock to invoke your specific Lambda function when this action group is triggered.\n",
    "    *   **`functionSchema`:** Provide the function schema defined in step 1. This allows the agent to understand how to call the Lambda function (i.e., what parameters to send).\n",
    "    *   Give the action group a name (e.g., `SearchAndFormatActionGroup`).\n",
    "\n",
    "5.  **Prepare Agent:**\n",
    "    *   Call `bedrock_agent_client.prepare_agent` with the `agentId`. This makes the DRAFT version of the agent (with its newly configured action group) ready for use. The notebook includes a custom waiter to poll until the agent status is `PREPARED`.\n",
    "\n",
    "6.  **Create or Update Agent Alias:**\n",
    "    *   An alias (e.g., `prod`) is used to invoke a specific version of the agent. The notebook checks if an alias exists and creates one if not, pointing to the latest prepared (DRAFT) version. Use `bedrock_agent_client.create_agent_alias` or `update_agent_alias`.\n",
    "\n",
    "7.  **Invoke Agent:**\n",
    "    *   Use `bedrock_agent_runtime_client.invoke_agent`, providing the `agentId`, `agentAliasId`, a unique `sessionId`, and the user's `inputText` (prompt). \n",
    "    *   Bedrock takes over: when the agent decides to use the tool from the action group, Bedrock transparently calls the configured Lambda function with the necessary parameters.\n",
    "    *   The Lambda executes, returns its result to the agent, and the agent uses this result to generate its final response.\n",
    "    *   Your application code simply waits for and processes the final streaming response from the `invoke_agent` call. Unlike the Custom Control approach, there's no `returnControl` event for the application to handle for tool execution; Bedrock manages the Lambda invocation directly.\n",
    "\n",
    "## Pros\n",
    "\n",
    "*   **Decoupling & Modularity:** Tool execution logic is encapsulated within Lambda functions, separate from the main application code. This allows for independent development, deployment, and scaling of tools.\n",
    "*   **Scalability & Serverless:** Leverages the inherent scalability, concurrency, and pay-per-use benefits of AWS Lambda for tool execution.\n",
    "*   **Managed Execution Environment:** AWS manages the underlying infrastructure, runtime environment, and invocation mechanism for the Lambda functions.\n",
    "*   **Simpler Application-Level Code:** The application that invokes the Bedrock agent doesn't need to implement the tool's logic itself or handle `returnControl` events for function execution, as Bedrock orchestrates the call to Lambda directly.\n",
    "\n",
    "## Cons\n",
    "\n",
    "*   **Deployment & Configuration Overhead:** Requires setting up, packaging, configuring dependencies, and deploying separate Lambda functions. IAM roles and permissions for Lambdas also need careful management.\n",
    "*   **State Management:** If tools need to share state or complex context with the Lambda function, this must be explicitly passed, often via environment variables or by including necessary lookup logic within the Lambda itself.\n",
    "*   **Cold Starts:** AWS Lambda cold starts can introduce latency the first time a function is invoked after a period of inactivity, potentially affecting agent response time.\n",
    "*   **Debugging Complexity:** Troubleshooting can be more involved as it spans across the Bedrock Agent service, the Lambda service, and potentially other services the Lambda interacts with (like Couchbase). Centralized logging (e.g., CloudWatch Logs for Lambda) is essential.\n",
    "*   **Cost:** Incurs costs associated with Lambda invocations, execution duration, and any resources used by the Lambda (e.g., data transfer, provisioned concurrency if used)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "This section imports all necessary Python libraries. These include:\n",
    "- Standard libraries: `json` for data handling, `logging` for progress and error messages, `os` for interacting with the operating system (e.g., file paths), `subprocess` for running external commands (like `make` for Lambda packaging), `time` for delays, `traceback` for detailed error reporting, `uuid` for generating unique identifiers, and `shutil` for file operations.\n",
    "- `boto3` and `botocore`: The AWS SDK for Python, used to interact with AWS services like Bedrock, IAM, Lambda, and S3. Specific configurations (`Config`) and waiters are also imported for robust client interactions.\n",
    "- `couchbase`: The official Couchbase SDK for Python, used for connecting to and interacting with the Couchbase cluster, including managing buckets, collections, and search indexes. Specific exception classes are imported for error handling.\n",
    "- `dotenv`: For loading environment variables from a `.env` file, which helps manage configuration settings like API keys and connection strings securely.\n",
    "- `langchain_aws` and `langchain_couchbase`: Libraries from the LangChain ecosystem. `BedrockEmbeddings` is used to generate text embeddings via Amazon Bedrock, and `CouchbaseSearchVectorStore` provides an interface for using Couchbase as a vector store in LangChain applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "import uuid\n",
    "from datetime import timedelta\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "from botocore.waiter import WaiterModel, create_waiter_with_client\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.exceptions import \\\n",
    "    SearchIndexNotFoundException  # Added more specific exceptions\n",
    "from couchbase.exceptions import (BucketNotFoundException,\n",
    "                                  CollectionNotFoundException,\n",
    "                                  CouchbaseException,\n",
    "                                  InternalServerFailureException,\n",
    "                                  QueryIndexAlreadyExistsException,\n",
    "                                  ScopeNotFoundException,\n",
    "                                  ServiceUnavailableException)\n",
    "from couchbase.management.buckets import (BucketSettings, BucketType,\n",
    "                                          CreateBucketSettings)\n",
    "from couchbase.management.collections import CollectionSpec\n",
    "from couchbase.management.search import SearchIndex, SearchIndexManager\n",
    "from couchbase.options import ClusterOptions, QueryOptions\n",
    "from dotenv import load_dotenv\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_couchbase.vectorstores import CouchbaseSearchVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "This section handles the initial setup of essential configurations for the notebook:\n",
    "- **Logging:** Configures the `logging` module to output messages with a specific format (timestamp, level, message), which helps in tracking the script's execution and diagnosing issues.\n",
    "- **Environment Variables:** Attempts to load environment variables from a `.env` file located either in the current directory or the parent directory. This is a common practice to keep sensitive information like credentials and hostnames out of the codebase. If the `.env` file is not found, the script will rely on variables already set in the execution environment.\n",
    "- **Couchbase Settings:** Defines variables for connecting to Couchbase, including the host, username, password, and the names for the bucket, scope, collection, and search index that will be used for this experiment. Default values are provided if specific environment variables are not set.\n",
    "- **AWS Settings:** Defines variables for AWS configuration, such as the region, access key ID, secret access key, and AWS account ID. These are crucial for `boto3` to interact with AWS services.\n",
    "- **Bedrock Model IDs:** Specifies the model identifiers for the Amazon Bedrock text embedding model (e.g., `amazon.titan-embed-text-v2:0`) and the foundation model to be used by the agent (e.g., `anthropic.claude-3-sonnet-20240229-v1:0`).\n",
    "- **File Paths:** Sets up variables for various file paths used throughout the notebook, such as the directory for schemas, the path to the Couchbase search index JSON definition, and the path to the JSON file containing documents to be loaded into the vector store. Using `os.getcwd()` makes these paths relative to the notebook's current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 16:33:49,779 - INFO - Attempting to load .env file from: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/.env\n",
      "2025-05-08 16:33:49,783 - INFO - .env file loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables from project root .env\n",
    "# In a notebook environment, '__file__' is not defined. Use a relative path or absolute path directly.\n",
    "# Assuming the notebook is run from the 'lambda-experiments' directory\n",
    "dotenv_path = os.path.join(os.getcwd(), '.env') # Or specify the full path if needed\n",
    "logger.info(f\"Attempting to load .env file from: {dotenv_path}\")\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    logger.info(\".env file loaded successfully.\")\n",
    "else:\n",
    "    # Try loading from parent directory if not found in current\n",
    "    parent_dotenv_path = os.path.join(os.path.dirname(os.getcwd()), '.env')\n",
    "    if os.path.exists(parent_dotenv_path):\n",
    "        load_dotenv(dotenv_path=parent_dotenv_path)\n",
    "        logger.info(f\".env file loaded successfully from parent directory: {parent_dotenv_path}\")\n",
    "    else:\n",
    "        logger.warning(f\".env file not found at {dotenv_path} or {parent_dotenv_path}. Relying on environment variables.\")\n",
    "\n",
    "\n",
    "# Couchbase Configuration\n",
    "CB_HOST = os.getenv(\"CB_HOST\", \"couchbase://localhost\")\n",
    "CB_USERNAME = os.getenv(\"CB_USERNAME\", \"Administrator\")\n",
    "CB_PASSWORD = os.getenv(\"CB_PASSWORD\", \"password\")\n",
    "# Using a new bucket/scope/collection for experiments to avoid conflicts\n",
    "CB_BUCKET_NAME = os.getenv(\"CB_BUCKET_NAME\", \"vector-search-exp\")\n",
    "SCOPE_NAME = os.getenv(\"SCOPE_NAME\", \"bedrock_exp\")\n",
    "COLLECTION_NAME = os.getenv(\"COLLECTION_NAME\", \"docs_exp\")\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\", \"vector_search_bedrock_exp\")\n",
    "\n",
    "# AWS Configuration\n",
    "AWS_REGION = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_ACCOUNT_ID = os.getenv(\"AWS_ACCOUNT_ID\")\n",
    "\n",
    "# Bedrock Model IDs\n",
    "EMBEDDING_MODEL_ID = \"amazon.titan-embed-text-v2:0\"\n",
    "AGENT_MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\" # Using Sonnet for the agent\n",
    "\n",
    "# Paths (relative to the notebook's execution directory)\n",
    "SCRIPT_DIR = os.getcwd() # Use current working directory for notebook context\n",
    "SCHEMAS_DIR = os.path.join(SCRIPT_DIR, 'schemas') # New Schemas Dir\n",
    "SEARCH_FORMAT_SCHEMA_PATH = os.path.join(SCHEMAS_DIR, 'search_and_format_schema.json') # Added\n",
    "INDEX_JSON_PATH = os.path.join(SCRIPT_DIR, 'aws_index.json') # Keep\n",
    "DOCS_JSON_PATH = os.path.join(SCRIPT_DIR, 'documents.json') # Changed to load from script's directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions\n",
    "\n",
    "This section defines a comprehensive suite of helper functions to modularize the various operations required throughout the notebook. These functions encapsulate specific tasks, making the main execution flow cleaner and easier to understand. The categories of helper functions include:\n",
    "\n",
    "*   **Environment and Client Initialization:** Checking for necessary environment variables and setting up AWS SDK (`boto3`) clients for services like IAM, Lambda, Bedrock, and S3.\n",
    "*   **Couchbase Interaction:** Connecting to the Couchbase cluster, and robustly setting up buckets, scopes, collections, and search indexes. Includes functions to clear data from collections for clean experimental runs.\n",
    "*   **IAM Role Management:** Creating or retrieving the necessary IAM roles with appropriate trust policies and permissions that allow Bedrock Agents and Lambda functions to operate and interact with other AWS services securely.\n",
    "*   **Lambda Function Deployment:** A set of functions to manage the lifecycle of the Lambda function that the agent will invoke. This includes packaging the Lambda code and its dependencies (using a `Makefile`), uploading the deployment package (to S3 if it's large), creating or updating the Lambda function in AWS, and deleting it for cleanup.\n",
    "*   **Bedrock Agent Resource Management:** Functions for creating the Bedrock Agent itself, defining its action groups (which link the agent to the Lambda function via its ARN and define the tool schema), preparing the agent to make it invocable, and managing agent aliases. Also includes functions to delete these agent resources for cleanup.\n",
    "*   **Agent Invocation:** A function to test the fully configured agent by sending it a prompt and processing its streamed response, including any trace information for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 check_environment_variables\n",
    "\n",
    "This function verifies that all critical environment variables required for the script to run (e.g., AWS credentials, Couchbase password, AWS Account ID) are set. It logs an error and returns `False` if any are missing, otherwise logs success and returns `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_environment_variables():\n",
    "    \"\"\"Check if required environment variables are set.\"\"\"\n",
    "    required_vars = [\"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\", \"AWS_ACCOUNT_ID\", \"CB_PASSWORD\"]\n",
    "    missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
    "    if missing_vars:\n",
    "        logger.error(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "        logger.error(\"Please set these variables in your environment or .env file\")\n",
    "        return False\n",
    "    logger.info(\"All required environment variables are set.\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 initialize_aws_clients\n",
    "\n",
    "This function sets up and returns the necessary AWS SDK (`boto3`) clients for interacting with various AWS services. It initializes clients for Bedrock Runtime (for embeddings and agent invocation), IAM (for managing roles and policies), Lambda (for deploying and managing Lambda functions), Bedrock Agent (for creating and managing agents), and Bedrock Agent Runtime (for invoking agents). It uses credentials and region from the environment configuration and includes a custom configuration (`agent_config`) with longer timeouts and retries, which is particularly important for Bedrock Agent operations that can take more time, like agent preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_aws_clients():\n",
    "    \"\"\"Initialize required AWS clients.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Initializing AWS clients in region: {AWS_REGION}\")\n",
    "        session = boto3.Session(\n",
    "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "            region_name=AWS_REGION\n",
    "        )\n",
    "        # Use a config with longer timeouts for agent operations\n",
    "        agent_config = Config(\n",
    "            connect_timeout=120,\n",
    "            read_timeout=600, # Agent preparation can take time\n",
    "            retries={'max_attempts': 5, 'mode': 'adaptive'}\n",
    "        )\n",
    "        bedrock_runtime = session.client('bedrock-runtime', region_name=AWS_REGION)\n",
    "        iam_client = session.client('iam', region_name=AWS_REGION) \n",
    "        lambda_client = session.client('lambda', region_name=AWS_REGION)\n",
    "        bedrock_agent_client = session.client('bedrock-agent', region_name=AWS_REGION, config=agent_config) # Add agent client\n",
    "        bedrock_agent_runtime_client = session.client('bedrock-agent-runtime', region_name=AWS_REGION, config=agent_config) # Add agent runtime client\n",
    "        logger.info(\"AWS clients initialized successfully.\")\n",
    "        return bedrock_runtime, iam_client, lambda_client, bedrock_agent_client, bedrock_agent_runtime_client # Return agent runtime client\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing AWS clients: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 connect_couchbase\n",
    "\n",
    "This function establishes a connection to the Couchbase cluster using the connection string (`CB_HOST`), username, and password from the environment configuration. It uses `PasswordAuthenticator` for authentication and `ClusterOptions` for potentially customizing connection parameters (though commented out in the example, it shows where timeouts could be set). It waits for the cluster to be ready before returning the `Cluster` object, ensuring that subsequent operations can be performed reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_couchbase():\n",
    "    \"\"\"Connect to Couchbase cluster.\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Connecting to Couchbase cluster at {CB_HOST}...\")\n",
    "        auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "        # Use robust options\n",
    "        options = ClusterOptions(\n",
    "             auth,\n",
    "             # query_timeout=timedelta(seconds=75), # Example: longer timeout\n",
    "             # kv_timeout=timedelta(seconds=10)\n",
    "        )\n",
    "        cluster = Cluster(CB_HOST, options)\n",
    "        cluster.wait_until_ready(timedelta(seconds=10)) # Wait longer if needed\n",
    "        logger.info(\"Successfully connected to Couchbase.\")\n",
    "        return cluster\n",
    "    except CouchbaseException as e:\n",
    "        logger.error(f\"Couchbase connection error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error connecting to Couchbase: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 setup_collection\n",
    "\n",
    "This comprehensive function is responsible for ensuring that the required Couchbase bucket, scope, and collection are available for the agent's vector store. It performs the following steps idempotently:\n",
    "- Checks if the specified bucket (`bucket_name`) exists. If not, it creates the bucket with defined settings (e.g., RAM quota, flush enabled). It includes a pause to allow the bucket to become ready.\n",
    "- Checks if the specified scope (`scope_name`) exists within the bucket. If not, it creates the scope and includes a brief pause.\n",
    "- Checks if the specified collection (`collection_name`) exists within the scope. If not, it creates the collection using a `CollectionSpec` and pauses.\n",
    "- Ensures that a primary N1QL index exists on the collection, creating it if it's missing. This is often useful for administrative queries or simpler lookups, though not strictly for vector search itself.\n",
    "Finally, it returns a `Collection` object representing the target collection for further operations.\n",
    "\n",
    "> Note: Bucket Creation will not work on Capella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_collection(cluster, bucket_name, scope_name, collection_name):\n",
    "    \"\"\"Set up Couchbase collection (Original Logic from lamda-approach)\"\"\"\n",
    "    logger.info(f\"Setting up collection: {bucket_name}/{scope_name}/{collection_name}\")\n",
    "    try:\n",
    "        # Check if bucket exists, create if it doesn't\n",
    "        try:\n",
    "            bucket = cluster.bucket(bucket_name)\n",
    "            logger.info(f\"Bucket '{bucket_name}' exists.\")\n",
    "        except BucketNotFoundException:\n",
    "            logger.info(f\"Bucket '{bucket_name}' does not exist. Creating it...\")\n",
    "            # Use BucketSettings with potentially lower RAM for experiment\n",
    "            bucket_settings = BucketSettings(\n",
    "                name=bucket_name,\n",
    "                bucket_type=BucketType.COUCHBASE,\n",
    "                ram_quota_mb=256, # Adjusted from 1024\n",
    "                flush_enabled=True,\n",
    "                num_replicas=0\n",
    "            )\n",
    "            try:\n",
    "                 cluster.buckets().create_bucket(bucket_settings)\n",
    "                 # Wait longer after bucket creation\n",
    "                 logger.info(f\"Bucket '{bucket_name}' created. Waiting for ready state (10s)...\")\n",
    "                 time.sleep(10) \n",
    "                 bucket = cluster.bucket(bucket_name) # Re-assign bucket object\n",
    "            except Exception as create_e:\n",
    "                 logger.error(f\"Failed to create bucket '{bucket_name}': {create_e}\")\n",
    "                 raise\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error getting bucket '{bucket_name}': {e}\")\n",
    "             raise\n",
    "\n",
    "        bucket_manager = bucket.collections()\n",
    "\n",
    "        # Check if scope exists, create if it doesn't\n",
    "        scopes = bucket_manager.get_all_scopes()\n",
    "        scope_exists = any(s.name == scope_name for s in scopes)\n",
    "\n",
    "        if not scope_exists:\n",
    "            logger.info(f\"Scope '{scope_name}' does not exist. Creating it...\")\n",
    "            try:\n",
    "                 bucket_manager.create_scope(scope_name)\n",
    "                 logger.info(f\"Scope '{scope_name}' created. Waiting (2s)...\")\n",
    "                 time.sleep(2)\n",
    "            except CouchbaseException as e:\n",
    "                 # Handle potential race condition or already exists error more robustly\n",
    "                 if \"already exists\" in str(e).lower() or \"scope_exists\" in str(e).lower():\n",
    "                      logger.info(f\"Scope '{scope_name}' likely already exists (caught during creation attempt).\")\n",
    "                 else:\n",
    "                      logger.error(f\"Failed to create scope '{scope_name}': {e}\")\n",
    "                      raise\n",
    "        else:\n",
    "             logger.info(f\"Scope '{scope_name}' already exists.\")\n",
    "\n",
    "        # Check if collection exists, create if it doesn't\n",
    "        # Re-fetch scopes in case it was just created\n",
    "        scopes = bucket_manager.get_all_scopes()\n",
    "        collection_exists = False\n",
    "        for s in scopes:\n",
    "             if s.name == scope_name:\n",
    "                  if any(c.name == collection_name for c in s.collections):\n",
    "                       collection_exists = True\n",
    "                       break\n",
    "        \n",
    "        if not collection_exists:\n",
    "            logger.info(f\"Collection '{collection_name}' does not exist in scope '{scope_name}'. Creating it...\")\n",
    "            try:\n",
    "                # Use CollectionSpec\n",
    "                collection_spec = CollectionSpec(collection_name, scope_name)\n",
    "                bucket_manager.create_collection(collection_spec)\n",
    "                logger.info(f\"Collection '{collection_name}' created. Waiting (2s)...\")\n",
    "                time.sleep(2)\n",
    "            except CouchbaseException as e:\n",
    "                 if \"already exists\" in str(e).lower() or \"collection_exists\" in str(e).lower():\n",
    "                     logger.info(f\"Collection '{collection_name}' likely already exists (caught during creation attempt).\")\n",
    "                 else:\n",
    "                     logger.error(f\"Failed to create collection '{collection_name}': {e}\")\n",
    "                     raise\n",
    "        else:\n",
    "            logger.info(f\"Collection '{collection_name}' already exists.\")\n",
    "\n",
    "        # Ensure primary index exists\n",
    "        try:\n",
    "            logger.info(f\"Ensuring primary index exists on `{bucket_name}`.`{scope_name}`.`{collection_name}`...\")\n",
    "            cluster.query(f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{bucket_name}`.`{scope_name}`.`{collection_name}`\").execute()\n",
    "            logger.info(\"Primary index present or created successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating primary index: {str(e)}\")\n",
    "            # Decide if this is fatal\n",
    "\n",
    "        logger.info(\"Collection setup complete.\")\n",
    "        # Return the collection object for use\n",
    "        return cluster.bucket(bucket_name).scope(scope_name).collection(collection_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up collection: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 setup_search_index\n",
    "\n",
    "This function is responsible for creating or updating the Couchbase Search (FTS) index required for vector similarity search. Key operations include:\n",
    "- Loading the index definition from a specified JSON file (`index_definition_path`).\n",
    "- Dynamically updating the loaded index definition to use the correct `index_name` and `sourceName` (bucket name) provided as arguments. This allows for a template index definition file to be reused.\n",
    "- Using the `SearchIndexManager` (obtained from the cluster object) to `upsert_index`. Upserting means the index will be created if it doesn't exist, or updated if an index with the same name already exists. This makes the operation idempotent.\n",
    "- After submitting the upsert operation, it includes a pause (`time.sleep`) to allow Couchbase some time to start the indexing process in the background.\n",
    "\n",
    "> Note: The index has been written for this specific bucket,scope,collection. For a different configuration, please change the same in index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_search_index(cluster, index_name, bucket_name, scope_name, collection_name, index_definition_path):\n",
    "    \"\"\"Set up search indexes (Original Logic, adapted) \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Looking for index definition at: {index_definition_path}\")\n",
    "        if not os.path.exists(index_definition_path):\n",
    "             logger.error(f\"Index definition file not found: {index_definition_path}\")\n",
    "             raise FileNotFoundError(f\"Index definition file not found: {index_definition_path}\")\n",
    "\n",
    "        with open(index_definition_path, 'r') as file:\n",
    "            index_definition = json.load(file)\n",
    "            # Update name and source based on function arguments\n",
    "            index_definition['name'] = index_name\n",
    "            index_definition['sourceName'] = bucket_name\n",
    "            # Optional: update params to explicitly target scope.collection if needed\n",
    "            # index_definition['planParams']['indexPartitions'] = 1 # Example\n",
    "            # index_definition['params'] = {\n",
    "            #     'mapping': {\n",
    "            #         'types': {\n",
    "            #             f'{scope_name}.{collection_name}': {\n",
    "            #                 'enabled': True,\n",
    "            #                 'dynamic': True # Or specify fields\n",
    "            #             }\n",
    "            #         },\n",
    "            #         'default_mapping': {\n",
    "            #             'enabled': False\n",
    "            #         }\n",
    "            #     }\n",
    "            # }\n",
    "            logger.info(f\"Loaded index definition from {index_definition_path}, ensuring name is '{index_name}' and source is '{bucket_name}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading index definition: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        # Use the SearchIndexManager from the Cluster object for cluster-level indexes\n",
    "        # Or use scope-level if the index JSON is structured for that\n",
    "        # Assuming cluster level based on original script structure for upsert\n",
    "        search_index_manager = cluster.search_indexes()\n",
    "\n",
    "        # Create SearchIndex object from potentially modified JSON definition\n",
    "        search_index = SearchIndex.from_json(index_definition)\n",
    "\n",
    "        # Upsert the index (create if not exists, update if exists)\n",
    "        logger.info(f\"Upserting search index '{index_name}'...\")\n",
    "        search_index_manager.upsert_index(search_index)\n",
    "\n",
    "        # Wait for indexing\n",
    "        logger.info(f\"Index '{index_name}' upsert operation submitted. Waiting for indexing (10s)...\")\n",
    "        time.sleep(10)\n",
    "\n",
    "        logger.info(f\"Search index '{index_name}' setup complete.\")\n",
    "\n",
    "    except QueryIndexAlreadyExistsException:\n",
    "        # This exception might not be correct for SearchIndexManager\n",
    "        # Upsert should handle exists cases, but log potential specific errors\n",
    "        logger.warning(f\"Search index '{index_name}' likely already existed (caught QueryIndexAlreadyExistsException, check if applicable). Upsert attempted.\")\n",
    "    except CouchbaseException as e:\n",
    "        logger.error(f\"Couchbase error during search index setup for '{index_name}': {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during search index setup for '{index_name}': {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 clear_collection\n",
    "\n",
    "This utility function is used to delete all documents from a specified Couchbase collection. It constructs and executes a N1QL `DELETE` query targeting the given bucket, scope, and collection. This is useful for ensuring a clean state before loading new data for an experiment, preventing interference from previous runs. It also attempts to log the number of mutations (deleted documents) if the query metrics are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_collection(cluster, bucket_name, scope_name, collection_name):\n",
    "    \"\"\"Delete all documents from the specified collection (Original Logic).\"\"\"\n",
    "    try:\n",
    "        logger.warning(f\"Attempting to clear all documents from `{bucket_name}`.`{scope_name}`.`{collection_name}`...\")\n",
    "        query = f\"DELETE FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
    "        result = cluster.query(query).execute()\n",
    "        # Try to get mutation count, handle if not available\n",
    "        mutation_count = 0\n",
    "        try:\n",
    "             metrics_data = result.meta_data().metrics()\n",
    "             if metrics_data:\n",
    "                  mutation_count = metrics_data.mutation_count()\n",
    "        except Exception as metrics_e:\n",
    "             logger.warning(f\"Could not retrieve mutation count after delete: {metrics_e}\")\n",
    "        logger.info(f\"Successfully cleared documents from the collection (approx. {mutation_count} mutations).\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error clearing documents from collection: {e}. Collection might be empty or index not ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 create_agent_role\n",
    "\n",
    "This function creates or updates the necessary IAM (Identity and Access Management) role that the Bedrock Agent and its associated Lambda function will assume. The role needs permissions to interact with AWS services on your behalf. Key aspects of this function are:\n",
    "- **Assume Role Policy:** Defines which AWS services (principals) are allowed to assume this role. In this case, it allows both `lambda.amazonaws.com` (for the Lambda function execution) and `bedrock.amazonaws.com` (for the Bedrock Agent service itself).\n",
    "- **Idempotency:** It first checks if a role with the specified `role_name` already exists. \n",
    "    - If it exists, the function retrieves its ARN and updates its trust policy to ensure it matches the required configuration.\n",
    "    - If it doesn't exist, it creates a new IAM role with the defined assume role policy and description.\n",
    "- **Permissions Policies:**\n",
    "    - Attaches the AWS managed policy `AWSLambdaBasicExecutionRole`, which grants the Lambda function permissions to write logs to CloudWatch.\n",
    "    - Creates and attaches an inline policy (`LambdaBasicLoggingPermissions`) for more specific logging permissions if needed, scoped to the Lambda log group.\n",
    "    - Creates and attaches an inline policy (`BedrockAgentPermissions`) granting broad `bedrock:*` permissions. For production, these permissions should be scoped down to the minimum required.\n",
    "- **Propagation Delays:** Includes `time.sleep` calls after creating the role and after attaching policies to allow time for the changes to propagate within AWS, which helps prevent subsequent operations from failing due to eventual consistency issues.\n",
    "It returns the ARN (Amazon Resource Name) of the created or updated IAM role, which is then used when creating the Bedrock Agent and the Lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent_role(iam_client, role_name, aws_account_id):\n",
    "    \"\"\"Creates or gets the IAM role for the Bedrock Agent Lambda functions.\"\"\"\n",
    "    logger.info(f\"Checking/Creating IAM role: {role_name}\")\n",
    "    assume_role_policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": [\n",
    "                        \"lambda.amazonaws.com\",\n",
    "                        \"bedrock.amazonaws.com\" \n",
    "                    ]\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    role_arn = None\n",
    "    try:\n",
    "        # Check if role exists\n",
    "        get_role_response = iam_client.get_role(RoleName=role_name)\n",
    "        role_arn = get_role_response['Role']['Arn']\n",
    "        logger.info(f\"IAM role '{role_name}' already exists with ARN: {role_arn}\")\n",
    "        \n",
    "        # Ensure trust policy is up-to-date\n",
    "        logger.info(f\"Updating trust policy for existing role '{role_name}'...\")\n",
    "        iam_client.update_assume_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyDocument=json.dumps(assume_role_policy_document)\n",
    "        )\n",
    "        logger.info(f\"Trust policy updated for role '{role_name}'.\")\n",
    "\n",
    "    except iam_client.exceptions.NoSuchEntityException:\n",
    "        logger.info(f\"IAM role '{role_name}' not found. Creating...\")\n",
    "        try:\n",
    "            create_role_response = iam_client.create_role(\n",
    "                RoleName=role_name,\n",
    "                AssumeRolePolicyDocument=json.dumps(assume_role_policy_document),\n",
    "                Description='IAM role for Bedrock Agent Lambda functions (Experiment)',\n",
    "                MaxSessionDuration=3600\n",
    "            )\n",
    "            role_arn = create_role_response['Role']['Arn']\n",
    "            logger.info(f\"Successfully created IAM role '{role_name}' with ARN: {role_arn}\")\n",
    "            # Wait after role creation before attaching policies\n",
    "            logger.info(\"Waiting 15s for role creation propagation...\")\n",
    "            time.sleep(15)\n",
    "        except ClientError as e:\n",
    "            logger.error(f\"Error creating IAM role '{role_name}': {e}\")\n",
    "            raise\n",
    "            \n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error getting/updating IAM role '{role_name}': {e}\")\n",
    "        raise\n",
    "        \n",
    "    # Attach basic execution policy (idempotent)\n",
    "    try:\n",
    "        logger.info(f\"Attaching basic Lambda execution policy to role '{role_name}'...\")\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn='arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'\n",
    "        )\n",
    "        logger.info(\"Attached basic Lambda execution policy.\")\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error attaching basic Lambda execution policy: {e}\")\n",
    "        # Don't necessarily raise, might already be attached or other issue\n",
    "        \n",
    "    # Add minimal inline policy for logging (can be expanded later if needed)\n",
    "    basic_inline_policy_name = \"LambdaBasicLoggingPermissions\"\n",
    "    basic_inline_policy_doc = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"logs:CreateLogGroup\",\n",
    "                    \"logs:CreateLogStream\",\n",
    "                    \"logs:PutLogEvents\"\n",
    "                ],\n",
    "                \"Resource\": f\"arn:aws:logs:{AWS_REGION}:{aws_account_id}:log-group:/aws/lambda/*:*\" # Scope down logs if possible\n",
    "            }\n",
    "            # Add S3 permissions here ONLY if Lambda code explicitly needs it\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Add Bedrock permissions policy\n",
    "    bedrock_policy_name = \"BedrockAgentPermissions\"\n",
    "    bedrock_policy_doc = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"bedrock:*\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"  # You can scope this down to specific agents/models if needed\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    try:\n",
    "        logger.info(f\"Putting basic inline policy '{basic_inline_policy_name}' for role '{role_name}'...\")\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=basic_inline_policy_name,\n",
    "            PolicyDocument=json.dumps(basic_inline_policy_doc)\n",
    "        )\n",
    "        logger.info(f\"Successfully put inline policy '{basic_inline_policy_name}'.\")\n",
    "        \n",
    "        # Add Bedrock permissions policy\n",
    "        logger.info(f\"Putting Bedrock permissions policy '{bedrock_policy_name}' for role '{role_name}'...\")\n",
    "        iam_client.put_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyName=bedrock_policy_name,\n",
    "            PolicyDocument=json.dumps(bedrock_policy_doc)\n",
    "        )\n",
    "        logger.info(f\"Successfully put inline policy '{bedrock_policy_name}'.\")\n",
    "        \n",
    "        logger.info(\"Waiting 10s for policy changes to propagate...\")\n",
    "        time.sleep(10)\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error putting inline policy: {e}\")\n",
    "        # Decide if this is fatal\n",
    "        \n",
    "    if not role_arn:\n",
    "         raise Exception(f\"Failed to create or retrieve ARN for role {role_name}\")\n",
    "         \n",
    "    return role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Lambda Deployment Functions\n",
    "\n",
    "This subsection groups together several helper functions dedicated to managing the deployment lifecycle of the AWS Lambda function that will serve as the tool executor for the Bedrock Agent. These functions handle packaging the Lambda code, managing its dependencies, deploying it to AWS, and cleaning up resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.1 delete_lambda_function\n",
    "\n",
    "This function is designed to safely delete an AWS Lambda function. Before attempting to delete the function itself, it tries to remove any permissions associated with it (specifically, the permission allowing Bedrock to invoke it, using a predictable statement ID). It then checks if the function exists and, if so, proceeds with the deletion. The function includes a brief pause after initiating deletion, as the process is asynchronous. It returns `True` if deletion was attempted/occurred and `False` if the function didn't exist or if an error occurred during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_lambda_function(lambda_client, function_name):\n",
    "    \"\"\"Delete Lambda function if it exists, attempting to remove permissions first.\"\"\"\n",
    "    logger.info(f\"Attempting to delete Lambda function: {function_name}...\")\n",
    "    try:\n",
    "        # Use a predictable statement ID added by create_lambda_function\n",
    "        statement_id = f\"AllowBedrockInvokeBasic-{function_name}\"\n",
    "        try:\n",
    "            logger.info(f\"Attempting to remove permission {statement_id} from {function_name}...\")\n",
    "            lambda_client.remove_permission(\n",
    "                FunctionName=function_name,\n",
    "                StatementId=statement_id\n",
    "            )\n",
    "            logger.info(f\"Successfully removed permission {statement_id} from {function_name}.\")\n",
    "            time.sleep(2) # Allow time for permission removal\n",
    "        except lambda_client.exceptions.ResourceNotFoundException:\n",
    "            logger.info(f\"Permission {statement_id} not found on {function_name}. Skipping removal.\")\n",
    "        except ClientError as perm_e:\n",
    "            # Log error but continue with deletion attempt\n",
    "            logger.warning(f\"Error removing permission {statement_id} from {function_name}: {str(perm_e)}\")\n",
    "\n",
    "        # Check if function exists before attempting deletion\n",
    "        lambda_client.get_function(FunctionName=function_name)\n",
    "        logger.info(f\"Function {function_name} exists. Deleting...\")\n",
    "        lambda_client.delete_function(FunctionName=function_name)\n",
    "\n",
    "        # Wait for deletion to complete using a waiter\n",
    "        logger.info(f\"Waiting for {function_name} to be deleted...\")\n",
    "        time.sleep(10) # Simple delay after delete call\n",
    "        logger.info(f\"Function {function_name} deletion initiated.\")\n",
    "\n",
    "        return True # Indicates deletion was attempted/occurred\n",
    "\n",
    "    except lambda_client.exceptions.ResourceNotFoundException:\n",
    "        logger.info(f\"Lambda function '{function_name}' does not exist. No need to delete.\")\n",
    "        return False # Indicates function didn't exist\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during deletion process for Lambda function '{function_name}': {str(e)}\")\n",
    "        # Depending on severity, might want to raise or just return False\n",
    "        return False # Indicates an error occurred beyond not found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.2 upload_to_s3\n",
    "\n",
    "This function handles uploading a Lambda deployment package (a .zip file) to Amazon S3. This is necessary when the package size exceeds the direct upload limit for Lambda. Key features include:\n",
    "- **Bucket Management:** It generates a unique S3 bucket name (prefixed with `lambda-deployment-`) using the AWS account ID and a timestamp, or a fallback UUID if the account ID isn't available. It checks if this bucket exists and creates it if not, ensuring the correct region is specified for bucket creation. It also uses a waiter to ensure the bucket is available before proceeding.\n",
    "- **S3 Key Generation:** Creates a unique S3 key (object path) for the uploaded file, incorporating the original filename and a UUID to prevent collisions.\n",
    "- **Multipart Upload:** For large files (currently > 100MB in the code, though the Lambda direct upload limit is typically around 50MB for the zip, and 250MB unzipped including layers, so S3 is used for packages over ~45-50MB in this notebook), it uses `boto3.s3.transfer.S3Transfer` for robust multipart uploads. For smaller files, it uses a standard `put_object` call.\n",
    "- **Retry Configuration:** Initializes the S3 and STS clients with a configuration that includes increased timeouts and retries for better resilience.\n",
    "It returns a dictionary containing the `S3Bucket` and `S3Key` of the uploaded package, which is then used by the `create_lambda_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(zip_file, region, bucket_name=None):\n",
    "    \"\"\"Upload zip file to S3 with retry logic and return S3 location.\"\"\"\n",
    "    logger.info(f\"Preparing to upload {zip_file} to S3 in region {region}...\")\n",
    "    # Configure the client with increased timeouts\n",
    "    config = Config(\n",
    "        connect_timeout=60,\n",
    "        read_timeout=300,\n",
    "        retries={'max_attempts': 3, 'mode': 'adaptive'}\n",
    "    )\n",
    "\n",
    "    s3_client = boto3.client('s3', region_name=region, config=config)\n",
    "    sts_client = boto3.client('sts', region_name=region, config=config)\n",
    "\n",
    "    # Determine bucket name\n",
    "    if bucket_name is None:\n",
    "        try:\n",
    "            account_id = sts_client.get_caller_identity().get('Account')\n",
    "            timestamp = int(time.time())\n",
    "            bucket_name = f\"lambda-deployment-{account_id}-{timestamp}\"\n",
    "            logger.info(f\"Generated unique S3 bucket name: {bucket_name}\")\n",
    "        except Exception as e:\n",
    "            fallback_id = uuid.uuid4().hex[:12]\n",
    "            bucket_name = f\"lambda-deployment-{fallback_id}\"\n",
    "            logger.warning(f\"Error getting account ID ({e}). Using fallback bucket name: {bucket_name}\")\n",
    "\n",
    "    # Create bucket if needed\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "        logger.info(f\"Using existing S3 bucket: {bucket_name}\")\n",
    "    except ClientError as e:\n",
    "        error_code = int(e.response['Error']['Code'])\n",
    "        if error_code == 404:\n",
    "            logger.info(f\"Creating S3 bucket: {bucket_name}...\")\n",
    "            try:\n",
    "                if region == 'us-east-1':\n",
    "                    s3_client.create_bucket(Bucket=bucket_name)\n",
    "                else:\n",
    "                    s3_client.create_bucket(\n",
    "                        Bucket=bucket_name,\n",
    "                        CreateBucketConfiguration={'LocationConstraint': region}\n",
    "                    )\n",
    "                logger.info(f\"Created S3 bucket: {bucket_name}. Waiting for availability...\")\n",
    "                waiter = s3_client.get_waiter('bucket_exists')\n",
    "                waiter.wait(Bucket=bucket_name, WaiterConfig={'Delay': 5, 'MaxAttempts': 12})\n",
    "                logger.info(f\"Bucket {bucket_name} is available.\")\n",
    "            except Exception as create_e:\n",
    "                logger.error(f\"Error creating bucket '{bucket_name}': {create_e}\")\n",
    "                raise\n",
    "        else:\n",
    "            logger.error(f\"Error checking bucket '{bucket_name}': {e}\")\n",
    "            raise\n",
    "\n",
    "    # Upload file\n",
    "    s3_key = f\"lambda/{os.path.basename(zip_file)}-{uuid.uuid4().hex[:8]}\"\n",
    "    try:\n",
    "        logger.info(f\"Uploading {zip_file} to s3://{bucket_name}/{s3_key}...\")\n",
    "        file_size = os.path.getsize(zip_file)\n",
    "        if file_size > 100 * 1024 * 1024:  # Use multipart for files > 100MB\n",
    "            logger.info(\"Using multipart upload for large file...\")\n",
    "            transfer_config = boto3.s3.transfer.TransferConfig(\n",
    "                multipart_threshold=10 * 1024 * 1024, max_concurrency=10,\n",
    "                multipart_chunksize=10 * 1024 * 1024, use_threads=True\n",
    "            )\n",
    "            s3_transfer = boto3.s3.transfer.S3Transfer(client=s3_client, config=transfer_config)\n",
    "            s3_transfer.upload_file(zip_file, bucket_name, s3_key)\n",
    "        else:\n",
    "            with open(zip_file, 'rb') as f:\n",
    "                s3_client.put_object(Bucket=bucket_name, Key=s3_key, Body=f)\n",
    "\n",
    "        logger.info(f\"Successfully uploaded to s3://{bucket_name}/{s3_key}\")\n",
    "        return {'S3Bucket': bucket_name, 'S3Key': s3_key}\n",
    "\n",
    "    except Exception as upload_e:\n",
    "        logger.error(f\"S3 upload failed: {upload_e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.3 package_function\n",
    "\n",
    "This function automates the process of packaging the Lambda function code and its dependencies into a .zip file, ready for deployment. It relies on a `Makefile` located in the `source_dir` (which is `lambda_functions` in this notebook). The steps are:\n",
    "1.  **Path Setup:** Defines various paths for source files, the temporary packaging directory, the `Makefile`, and the final output .zip file.\n",
    "2.  **File Preparation:** It copies the specific Lambda handler script (e.g., `bedrock_agent_search_and_format.py`) to `lambda_function.py` within the `source_dir` because the `Makefile` is likely configured to look for a generic `lambda_function.py`.\n",
    "3.  **Execute Makefile:** It runs a `make clean package` command using `subprocess.check_call`. The `make` command is executed with the `source_dir` as its current working directory. The Makefile is responsible for creating a virtual environment, installing dependencies from `requirements.txt` into a temporary `package_dir`, and then zipping the contents of this directory along with `lambda_function.py` into `lambda_package.zip` within the `source_dir`.\n",
    "4.  **Output Handling:** After the `make` command successfully completes, it moves and renames the generated `lambda_package.zip` from the `source_dir` to the specified `build_dir` (the notebook's current directory in this case) with a name like `function_name.zip`.\n",
    "5.  **Cleanup:** In a `finally` block, it cleans up the temporary `lambda_function.py` copied earlier and any intermediate `lambda_package.zip` left in the `source_dir` (e.g., if the rename/move failed).\n",
    "The function returns the path to the final .zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def package_function(function_name, source_dir, build_dir):\n",
    "    \"\"\"Package Lambda function using Makefile found in source_dir.\"\"\"\n",
    "    # source_dir is where the .py, requirements.txt, Makefile live (e.g., lambda_functions)\n",
    "    # build_dir is where packaging happens and final zip ends up (e.g., lambda-experiments)\n",
    "    makefile_path = os.path.join(source_dir, 'Makefile')\n",
    "    # Temp build dir inside source_dir, as Makefile expects relative paths\n",
    "    temp_package_dir = os.path.join(source_dir, 'package_dir') \n",
    "    # Requirements file is in source_dir\n",
    "    source_req_path = os.path.join(source_dir, 'requirements.txt') \n",
    "    # Target requirements path inside source_dir (needed for Makefile)\n",
    "    # target_req_path = os.path.join(source_dir, 'requirements.txt') # No copy needed if running make in source_dir\n",
    "    source_func_script_path = os.path.join(source_dir, f'{function_name}.py')\n",
    "    # Target function script path inside source_dir, renamed for Makefile install_deps copy\n",
    "    target_func_script_path = os.path.join(source_dir, 'lambda_function.py') \n",
    "    # Make output zip is created inside source_dir\n",
    "    make_output_zip = os.path.join(source_dir, 'lambda_package.zip') \n",
    "    # Final zip path is in the build_dir (one level up from source_dir)\n",
    "    final_zip_path = os.path.join(build_dir, f'{function_name}.zip')\n",
    "\n",
    "    logger.info(f\"--- Packaging function {function_name} --- \")\n",
    "    logger.info(f\"Source Dir (Makefile location & make cwd): {source_dir}\")\n",
    "    logger.info(f\"Build Dir (Final zip location): {build_dir}\")\n",
    "\n",
    "    if not os.path.exists(source_func_script_path):\n",
    "        raise FileNotFoundError(f\"Source function script not found: {source_func_script_path}\")\n",
    "    if not os.path.exists(source_req_path):\n",
    "        raise FileNotFoundError(f\"Source requirements file not found: {source_req_path}\")\n",
    "    if not os.path.exists(makefile_path):\n",
    "        raise FileNotFoundError(f\"Makefile not found at: {makefile_path}\")\n",
    "\n",
    "    # Ensure no leftover target script from previous failed run\n",
    "    if os.path.exists(target_func_script_path):\n",
    "        logger.warning(f\"Removing existing target script: {target_func_script_path}\")\n",
    "        os.remove(target_func_script_path)\n",
    "\n",
    "    try:\n",
    "        # 1. No need to create lambda subdir in build_dir\n",
    "\n",
    "        # 2. Copy source function script to source_dir as lambda_function.py\n",
    "        logger.info(f\"Copying {source_func_script_path} to {target_func_script_path}\")\n",
    "        shutil.copy(source_func_script_path, target_func_script_path)\n",
    "        # Requirements file is already in source_dir, no copy needed.\n",
    "\n",
    "        # 3. Run make command (execute from source_dir where Makefile is)\n",
    "        make_command = [\n",
    "            'make',\n",
    "            '-f', makefile_path, # Still specify Makefile path explicitly\n",
    "            'clean', # Clean first\n",
    "            'package',\n",
    "            # 'PYTHON_VERSION=python3.9' # Let Makefile use its default or system default\n",
    "        ]\n",
    "        logger.info(f\"Running make command: {' '.join(make_command)} (in {source_dir})\")\n",
    "        # Run make from source_dir; relative paths in Makefile should now work\n",
    "        subprocess.check_call(make_command, cwd=source_dir, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)\n",
    "        logger.info(\"Make command completed successfully.\")\n",
    "\n",
    "        # 4. Check for output zip in source_dir and rename/move to build_dir\n",
    "        if not os.path.exists(make_output_zip):\n",
    "            raise FileNotFoundError(f\"Makefile did not produce expected output: {make_output_zip}\")\n",
    "\n",
    "        logger.info(f\"Moving and renaming {make_output_zip} to {final_zip_path}\")\n",
    "        if os.path.exists(final_zip_path):\n",
    "             logger.warning(f\"Removing existing final zip: {final_zip_path}\")\n",
    "             os.remove(final_zip_path)\n",
    "        # Use shutil.move for cross-filesystem safety if needed, os.rename is fine here\n",
    "        os.rename(make_output_zip, final_zip_path) \n",
    "        logger.info(f\"Zip file ready: {final_zip_path}\")\n",
    "\n",
    "        return final_zip_path\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.error(f\"Error running Makefile for {function_name}: {e}\")\n",
    "        stderr_output = \"(No stderr captured)\"\n",
    "        if e.stderr:\n",
    "             try:\n",
    "                  stderr_output = e.stderr.decode()\n",
    "             except Exception:\n",
    "                  stderr_output = \"(Could not decode stderr)\"\n",
    "        logger.error(f\"Make stderr: {stderr_output}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error packaging function {function_name} using Makefile: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "    finally:\n",
    "        # 5. Clean up intermediate files in source_dir\n",
    "        if os.path.exists(target_func_script_path):\n",
    "            logger.info(f\"Cleaning up temporary script: {target_func_script_path}\")\n",
    "            os.remove(target_func_script_path)\n",
    "        if os.path.exists(make_output_zip): # If rename failed\n",
    "            logger.warning(f\"Cleaning up intermediate zip in source dir: {make_output_zip}\")\n",
    "            os.remove(make_output_zip)\n",
    "        # Consider cleaning temp_package_dir if make clean fails\n",
    "        # if os.path.exists(temp_package_dir):\n",
    "        #     logger.info(f\"Cleaning up temporary package dir: {temp_package_dir}\")\n",
    "        #     shutil.rmtree(temp_package_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8.4 create_lambda_function\n",
    "\n",
    "This is a key function that handles the creation or update of the AWS Lambda function. It incorporates several important aspects for robustness and proper configuration:\n",
    "- **Package Handling:** It checks the size of the deployment .zip file. If it's over a threshold (45MB in this code, as Lambda has limits for direct uploads), it calls `upload_to_s3` to upload the package to S3 and uses the S3 location for deployment. Otherwise, it reads the .zip file content directly for deployment.\n",
    "- **Configuration:** Defines common arguments for Lambda creation/update, including the function name, runtime (`python3.9`), IAM role ARN, handler name, timeout, memory size, and crucial environment variables (Couchbase details, Bedrock model IDs) that the Lambda will need at runtime.\n",
    "- **Idempotency & Retry Logic:** It first attempts to create the Lambda function. \n",
    "    - If it encounters a `ResourceConflictException` (meaning the function already exists), it then attempts to update the function's code and configuration.\n",
    "    - It includes a retry loop for both creation and update operations to handle potential throttling or other transient AWS issues, with an exponential backoff strategy.\n",
    "- **Permissions:** After successfully creating or updating the Lambda, it adds a resource-based policy (permission) to the Lambda function. This permission specifically allows the Bedrock service (`bedrock.amazonaws.com`) to invoke this Lambda function. It uses a predictable `StatementId` and handles potential conflicts if the permission already exists.\n",
    "- **Waiters:** It uses `boto3` waiters (`function_active_v2` after creation, `function_updated_v2` after update) to pause execution until the Lambda function becomes fully active and ready, preventing issues where subsequent operations might target a Lambda that isn't fully initialized.\n",
    "The function returns the ARN of the successfully created or updated Lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lambda_function(lambda_client, function_name, handler, role_arn, zip_file, region):\n",
    "    \"\"\"Create or update Lambda function with retry logic.\"\"\"\n",
    "    logger.info(f\"Deploying Lambda function {function_name} from {zip_file}...\")\n",
    "\n",
    "    # Configure the client with increased timeouts for potentially long creation\n",
    "    config = Config(\n",
    "        connect_timeout=120,\n",
    "        read_timeout=300,\n",
    "        retries={'max_attempts': 5, 'mode': 'adaptive'}\n",
    "    )\n",
    "    lambda_client_local = boto3.client('lambda', region_name=region, config=config)\n",
    "\n",
    "    # Check zip file size\n",
    "    zip_size_mb = 0\n",
    "    try:\n",
    "        zip_size_bytes = os.path.getsize(zip_file)\n",
    "        zip_size_mb = zip_size_bytes / (1024 * 1024)\n",
    "        logger.info(f\"Zip file size: {zip_size_mb:.2f} MB\")\n",
    "    except OSError as e:\n",
    "         logger.error(f\"Could not get size of zip file {zip_file}: {e}\")\n",
    "         raise # Cannot proceed without zip file\n",
    "\n",
    "    use_s3 = zip_size_mb > 45 # Use S3 for packages over ~45MB\n",
    "    s3_location = None\n",
    "    zip_content = None\n",
    "\n",
    "    if use_s3:\n",
    "        logger.info(f\"Package size ({zip_size_mb:.2f} MB) requires S3 deployment.\")\n",
    "        s3_location = upload_to_s3(zip_file, region)\n",
    "        if not s3_location:\n",
    "             raise Exception(\"Failed to upload Lambda package to S3.\")\n",
    "    else:\n",
    "         logger.info(\"Deploying package directly.\")\n",
    "         try:\n",
    "             with open(zip_file, 'rb') as f:\n",
    "                 zip_content = f.read()\n",
    "         except OSError as e:\n",
    "              logger.error(f\"Could not read zip file {zip_file}: {e}\")\n",
    "              raise\n",
    "\n",
    "    # Define common create/update args\n",
    "    common_args = {\n",
    "        'FunctionName': function_name,\n",
    "        'Runtime': 'python3.9',\n",
    "        'Role': role_arn,\n",
    "        'Handler': handler,\n",
    "        'Timeout': 180,\n",
    "        'MemorySize': 1536, # Adjust as needed\n",
    "        # Env vars loaded from main script env or .env\n",
    "        'Environment': {\n",
    "            'Variables': {\n",
    "                'CB_HOST': os.getenv('CB_HOST', 'couchbase://localhost'),\n",
    "                'CB_USERNAME': os.getenv('CB_USERNAME', 'Administrator'),\n",
    "                'CB_PASSWORD': os.getenv('CB_PASSWORD', 'password'),\n",
    "                'CB_BUCKET_NAME': os.getenv('CB_BUCKET_NAME', 'vector-search-exp'),\n",
    "                'SCOPE_NAME': os.getenv('SCOPE_NAME', 'bedrock_exp'),\n",
    "                'COLLECTION_NAME': os.getenv('COLLECTION_NAME', 'docs_exp'),\n",
    "                'INDEX_NAME': os.getenv('INDEX_NAME', 'vector_search_bedrock_exp'),\n",
    "                'EMBEDDING_MODEL_ID': os.getenv('EMBEDDING_MODEL_ID', EMBEDDING_MODEL_ID),\n",
    "                'AGENT_MODEL_ID': os.getenv('AGENT_MODEL_ID', AGENT_MODEL_ID)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if use_s3:\n",
    "        code_arg = {'S3Bucket': s3_location['S3Bucket'], 'S3Key': s3_location['S3Key']}\n",
    "    else:\n",
    "        code_arg = {'ZipFile': zip_content}\n",
    "\n",
    "    max_retries = 3\n",
    "    base_delay = 10\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            logger.info(f\"Creating function '{function_name}' (attempt {attempt}/{max_retries})...\")\n",
    "            create_args = common_args.copy()\n",
    "            create_args['Code'] = code_arg\n",
    "            create_args['Publish'] = True # Publish a version\n",
    "\n",
    "            create_response = lambda_client_local.create_function(**create_args)\n",
    "            function_arn = create_response['FunctionArn']\n",
    "            logger.info(f\"Successfully created function '{function_name}' with ARN: {function_arn}\")\n",
    "\n",
    "            # Add basic invoke permission after creation\n",
    "            time.sleep(5) # Give function time to be fully created before adding policy\n",
    "            statement_id = f\"AllowBedrockInvokeBasic-{function_name}\"\n",
    "            try:\n",
    "                logger.info(f\"Adding basic invoke permission ({statement_id}) to {function_name}...\")\n",
    "                lambda_client_local.add_permission(\n",
    "                    FunctionName=function_name,\n",
    "                    StatementId=statement_id,\n",
    "                    Action='lambda:InvokeFunction',\n",
    "                    Principal='bedrock.amazonaws.com'\n",
    "                )\n",
    "                logger.info(f\"Successfully added basic invoke permission {statement_id}.\")\n",
    "            except lambda_client_local.exceptions.ResourceConflictException:\n",
    "                 logger.info(f\"Permission {statement_id} already exists for {function_name}. Skipping add.\")\n",
    "            except ClientError as perm_e:\n",
    "                logger.warning(f\"Failed to add basic invoke permission {statement_id} to {function_name}: {perm_e}\")\n",
    "\n",
    "            # Wait for function to be Active\n",
    "            logger.info(f\"Waiting for function '{function_name}' to become active...\")\n",
    "            waiter = lambda_client_local.get_waiter('function_active_v2')\n",
    "            waiter.wait(FunctionName=function_name, WaiterConfig={'Delay': 5, 'MaxAttempts': 24})\n",
    "            logger.info(f\"Function '{function_name}' is active.\")\n",
    "\n",
    "            return function_arn # Return ARN upon successful creation\n",
    "\n",
    "        except lambda_client_local.exceptions.ResourceConflictException:\n",
    "             logger.warning(f\"Function '{function_name}' already exists. Attempting to update code...\")\n",
    "             try:\n",
    "                 if use_s3:\n",
    "                     update_response = lambda_client_local.update_function_code(\n",
    "                         FunctionName=function_name,\n",
    "                         S3Bucket=s3_location['S3Bucket'],\n",
    "                         S3Key=s3_location['S3Key'],\n",
    "                         Publish=True\n",
    "                     )\n",
    "                 else:\n",
    "                     update_response = lambda_client_local.update_function_code(\n",
    "                         FunctionName=function_name,\n",
    "                         ZipFile=zip_content,\n",
    "                         Publish=True\n",
    "                     )\n",
    "                 function_arn = update_response['FunctionArn']\n",
    "                 logger.info(f\"Successfully updated function code for '{function_name}'. New version ARN: {function_arn}\")\n",
    "                 \n",
    "                 # Also update configuration just in case\n",
    "                 try:\n",
    "                      logger.info(f\"Updating configuration for '{function_name}'...\")\n",
    "                      lambda_client_local.update_function_configuration(**common_args)\n",
    "                      logger.info(f\"Configuration updated for '{function_name}'.\")\n",
    "                 except ClientError as conf_e:\n",
    "                      logger.warning(f\"Could not update configuration for '{function_name}': {conf_e}\")\n",
    "                 \n",
    "                 # Re-verify invoke permission after update\n",
    "                 time.sleep(5)\n",
    "                 statement_id = f\"AllowBedrockInvokeBasic-{function_name}\"\n",
    "                 try:\n",
    "                     logger.info(f\"Verifying/Adding basic invoke permission ({statement_id}) after update...\")\n",
    "                     lambda_client_local.add_permission(\n",
    "                         FunctionName=function_name,\n",
    "                         StatementId=statement_id,\n",
    "                         Action='lambda:InvokeFunction',\n",
    "                         Principal='bedrock.amazonaws.com'\n",
    "                     )\n",
    "                     logger.info(f\"Successfully added/verified basic invoke permission {statement_id}.\")\n",
    "                 except lambda_client_local.exceptions.ResourceConflictException:\n",
    "                     logger.info(f\"Permission {statement_id} already exists for {function_name}. Skipping add.\")\n",
    "                 except ClientError as perm_e:\n",
    "                     logger.warning(f\"Failed to add/verify basic invoke permission {statement_id} after update: {perm_e}\")\n",
    "\n",
    "                 # Wait for function to be Active after update\n",
    "                 logger.info(f\"Waiting for function '{function_name}' update to complete...\")\n",
    "                 waiter = lambda_client_local.get_waiter('function_updated_v2')\n",
    "                 waiter.wait(FunctionName=function_name, WaiterConfig={'Delay': 5, 'MaxAttempts': 24})\n",
    "                 logger.info(f\"Function '{function_name}' update complete.\")\n",
    "                 \n",
    "                 return function_arn # Return ARN after successful update\n",
    "\n",
    "             except ClientError as update_e:\n",
    "                 logger.error(f\"Failed to update function '{function_name}': {update_e}\")\n",
    "                 if attempt < max_retries:\n",
    "                      delay = base_delay * (2 ** (attempt - 1))\n",
    "                      logger.info(f\"Retrying update in {delay} seconds...\")\n",
    "                      time.sleep(delay)\n",
    "                 else:\n",
    "                      logger.error(\"Maximum update retries reached. Deployment failed.\")\n",
    "                      raise update_e\n",
    "                      \n",
    "        except ClientError as e:\n",
    "            # Handle throttling or other retryable errors\n",
    "            error_code = e.response.get('Error', {}).get('Code')\n",
    "            if error_code in ['ThrottlingException', 'ProvisionedConcurrencyConfigNotFoundException', 'EC2ThrottledException'] or 'Rate exceeded' in str(e):\n",
    "                logger.warning(f\"Retryable error on attempt {attempt}: {e}\")\n",
    "                if attempt < max_retries:\n",
    "                    delay = base_delay * (2 ** (attempt - 1)) + (uuid.uuid4().int % 5)\n",
    "                    logger.info(f\"Retrying in {delay} seconds...\")\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    logger.error(\"Maximum retries reached after retryable error. Deployment failed.\")\n",
    "                    raise e\n",
    "            else:\n",
    "                logger.error(f\"Error creating/updating Lambda '{function_name}': {e}\")\n",
    "                logger.error(traceback.format_exc()) # Log full traceback for unexpected errors\n",
    "                raise e # Re-raise non-retryable or unexpected errors\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Unexpected error during Lambda deployment: {e}\")\n",
    "             logger.error(traceback.format_exc())\n",
    "             raise e\n",
    "             \n",
    "    # If loop completes without returning, something went wrong\n",
    "    raise Exception(f\"Failed to deploy Lambda function {function_name} after {max_retries} attempts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Agent Resource Deletion Functions\n",
    "\n",
    "This subsection provides helper functions to manage the cleanup of AWS Bedrock Agent resources. Creating agents, action groups, and aliases results in persistent configurations in AWS. These functions are essential for maintaining a clean environment, especially during experimentation and development, by allowing for the removal of these resources when they are no longer needed or before recreating them in a subsequent run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9.1 get_agent_by_name\n",
    "\n",
    "This utility function searches for an existing Bedrock Agent by its name. Since the AWS SDK's `get_agent` requires an `agentId`, and you often work with human-readable names, this function bridges that gap. It uses the `list_agents` operation (with a paginator to handle potentially many agents in an account) and iterates through the summaries, comparing the `agentName` field. If a match is found, it returns the corresponding `agentId`. If no agent with the given name is found or an error occurs during listing, it returns `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_by_name(agent_client, agent_name):\n",
    "    \"\"\"Find an agent ID by its name using list_agents.\"\"\"\n",
    "    logger.info(f\"Attempting to find agent by name: {agent_name}\")\n",
    "    try:\n",
    "        paginator = agent_client.get_paginator('list_agents')\n",
    "        for page in paginator.paginate():\n",
    "            for agent_summary in page.get('agentSummaries', []):\n",
    "                if agent_summary.get('agentName') == agent_name:\n",
    "                    agent_id = agent_summary.get('agentId')\n",
    "                    logger.info(f\"Found agent '{agent_name}' with ID: {agent_id}\")\n",
    "                    return agent_id\n",
    "        logger.info(f\"Agent '{agent_name}' not found.\")\n",
    "        return None\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error listing agents to find '{agent_name}': {e}\")\n",
    "        return None # Treat as not found if error occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9.2 delete_action_group\n",
    "\n",
    "This function handles the deletion of a specific action group associated with a Bedrock Agent. Action groups are always tied to the `DRAFT` version of an agent. It calls `delete_agent_action_group`, providing the `agentId`, `agentVersion='DRAFT'`, and the `actionGroupId`. It uses `skipResourceInUseCheck=True` to force deletion, which can be useful if the agent is in a state (like `PREPARING`) that might otherwise prevent immediate deletion. The function includes error handling for cases where the action group is not found or if a conflict occurs (e.g., agent is busy), attempting a retry after a delay in case of a conflict. It returns `True` if deletion was successful or the group was not found, and `False` if an unrecoverable error occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_action_group(agent_client, agent_id, action_group_id):\n",
    "    \"\"\"Deletes a specific action group for an agent.\"\"\"\n",
    "    logger.info(f\"Attempting to delete action group {action_group_id} for agent {agent_id}...\")\n",
    "    try:\n",
    "        agent_client.delete_agent_action_group(\n",
    "            agentId=agent_id,\n",
    "            agentVersion='DRAFT', # Action groups are tied to the DRAFT version\n",
    "            actionGroupId=action_group_id,\n",
    "            skipResourceInUseCheck=True # Force deletion even if in use (e.g., during prepare)\n",
    "        )\n",
    "        logger.info(f\"Successfully deleted action group {action_group_id} for agent {agent_id}.\")\n",
    "        time.sleep(5) # Short pause after deletion\n",
    "        return True\n",
    "    except agent_client.exceptions.ResourceNotFoundException:\n",
    "        logger.info(f\"Action group {action_group_id} not found for agent {agent_id}. Skipping deletion.\")\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        # Handle potential throttling or conflict if prepare is happening\n",
    "        error_code = e.response.get('Error', {}).get('Code')\n",
    "        if error_code == 'ConflictException':\n",
    "            logger.warning(f\"Conflict deleting action group {action_group_id} (agent might be preparing/busy). Retrying once after delay...\")\n",
    "            time.sleep(15)\n",
    "            try:\n",
    "                agent_client.delete_agent_action_group(\n",
    "                    agentId=agent_id, agentVersion='DRAFT', actionGroupId=action_group_id, skipResourceInUseCheck=True\n",
    "                )\n",
    "                logger.info(f\"Successfully deleted action group {action_group_id} after retry.\")\n",
    "                return True\n",
    "            except Exception as retry_e:\n",
    "                 logger.error(f\"Error deleting action group {action_group_id} on retry: {retry_e}\")\n",
    "                 return False\n",
    "        else:\n",
    "            logger.error(f\"Error deleting action group {action_group_id} for agent {agent_id}: {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9.3 delete_agent_and_resources\n",
    "\n",
    "This function orchestrates the complete cleanup of a Bedrock Agent and its associated components. Its process is:\n",
    "1.  **Find Agent:** It first calls `get_agent_by_name` to retrieve the `agentId` for the specified `agent_name`. If the agent isn't found, it exits gracefully.\n",
    "2.  **Delete Action Groups:** It lists all action groups associated with the `DRAFT` version of the agent. For each action group found, it calls `delete_action_group` to remove it.\n",
    "3.  **Delete Agent:** After attempting to delete all action groups, it proceeds to delete the agent itself using `delete_agent` with `skipResourceInUseCheck=True` to force the deletion.\n",
    "4.  **Wait for Deletion:** It includes a custom polling loop to wait for the agent to be fully deleted by repeatedly calling `get_agent` and checking for a `ResourceNotFoundException`. This ensures that subsequent operations (like recreating an agent with the same name) are less likely to encounter conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_agent_and_resources(agent_client, agent_name):\n",
    "    \"\"\"Deletes the agent and its associated action groups.\"\"\"\n",
    "    agent_id = get_agent_by_name(agent_client, agent_name)\n",
    "    if not agent_id:\n",
    "        logger.info(f\"Agent '{agent_name}' not found, no deletion needed.\")\n",
    "        return\n",
    "\n",
    "    logger.warning(f\"--- Deleting Agent Resources for '{agent_name}' (ID: {agent_id}) ---\")\n",
    "\n",
    "    # 1. Delete Action Groups\n",
    "    try:\n",
    "        logger.info(f\"Listing action groups for agent {agent_id}...\")\n",
    "        action_groups = agent_client.list_agent_action_groups(\n",
    "            agentId=agent_id,\n",
    "            agentVersion='DRAFT' # List groups for the DRAFT version\n",
    "        ).get('actionGroupSummaries', [])\n",
    "\n",
    "        if action_groups:\n",
    "            logger.info(f\"Found {len(action_groups)} action groups to delete.\")\n",
    "            for ag in action_groups:\n",
    "                delete_action_group(agent_client, agent_id, ag['actionGroupId'])\n",
    "        else:\n",
    "            logger.info(\"No action groups found to delete.\")\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error listing action groups for agent {agent_id}: {e}\")\n",
    "        # Continue to agent deletion attempt even if listing fails\n",
    "\n",
    "    # 2. Delete the Agent\n",
    "    try:\n",
    "        logger.info(f\"Attempting to delete agent {agent_id} ('{agent_name}')...\")\n",
    "        agent_client.delete_agent(agentId=agent_id, skipResourceInUseCheck=True) # Force delete\n",
    "\n",
    "        # Wait for agent deletion (custom waiter logic might be needed if no standard waiter)\n",
    "        logger.info(f\"Waiting up to 2 minutes for agent {agent_id} deletion...\")\n",
    "        deleted = False\n",
    "        for _ in range(24): # Check every 5 seconds for 2 minutes\n",
    "            try:\n",
    "                agent_client.get_agent(agentId=agent_id)\n",
    "                time.sleep(5)\n",
    "            except agent_client.exceptions.ResourceNotFoundException:\n",
    "                logger.info(f\"Agent {agent_id} successfully deleted.\")\n",
    "                deleted = True\n",
    "                break\n",
    "            except ClientError as e:\n",
    "                 # Handle potential throttling during check\n",
    "                 error_code = e.response.get('Error', {}).get('Code')\n",
    "                 if error_code == 'ThrottlingException':\n",
    "                     logger.warning(\"Throttled while checking agent deletion status, continuing wait...\")\n",
    "                     time.sleep(10)\n",
    "                 else:\n",
    "                     logger.error(f\"Error checking agent deletion status: {e}\")\n",
    "                     # Break checking loop on unexpected error\n",
    "                     break\n",
    "        if not deleted:\n",
    "             logger.warning(f\"Agent {agent_id} deletion confirmation timed out.\")\n",
    "\n",
    "    except agent_client.exceptions.ResourceNotFoundException:\n",
    "        logger.info(f\"Agent {agent_id} ('{agent_name}') already deleted or not found.\")\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error deleting agent {agent_id}: {e}\")\n",
    "\n",
    "    logger.info(f\"--- Agent Resource Deletion Complete for '{agent_name}' ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 Agent Creation Functions\n",
    "\n",
    "This subsection contains functions dedicated to the setup and configuration of the Bedrock Agent itself, including its core definition, action groups that link it to tools (Lambda functions), and the preparation process that makes it ready for invocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.10.1 create_agent\n",
    "\n",
    "This function creates a new Bedrock Agent. It takes the desired `agent_name`, the `agent_role_arn` (obtained from `create_agent_role`), and the `foundation_model_id` (e.g., for Claude Sonnet) as input. Key configurations include:\n",
    "- **Instruction:** A detailed prompt that defines the agent's persona, capabilities, and how it should use its tools. The instruction in this notebook guides the agent to use a single \"SearchAndFormat\" tool and present results directly.\n",
    "- **`idleSessionTTLInSeconds`:** Sets a timeout for how long an agent session can remain idle.\n",
    "- **Description:** A brief description for the agent.\n",
    "After calling `create_agent`, the function logs the initial response details (ID, ARN, status). It then enters a polling loop to wait until the agent's status moves out of the `CREATING` state, typically to `NOT_PREPARED`. If the agent creation fails and enters a `FAILED` state, it raises an exception. It returns the `agent_id` and `agent_arn` upon successful initiation of creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent(agent_client, agent_name, agent_role_arn, foundation_model_id):\n",
    "    \"\"\"Creates a new Bedrock Agent.\"\"\"\n",
    "    logger.info(f\"--- Creating Agent: {agent_name} ---\")\n",
    "    try:\n",
    "        # Updated Instruction for single tool\n",
    "        instruction = (\n",
    "            \"You are a helpful research assistant. Your primary function is to use the SearchAndFormat tool \"\n",
    "            \"to find relevant documents based on user queries and format them. \" \n",
    "            \"Use the user's query for the search, and specify a formatting style if requested, otherwise use the default. \"\n",
    "            \"Present the formatted results returned by the tool directly to the user.\"\n",
    "            \"Only use the tool provided. Do not add your own knowledge.\"\n",
    "        )\n",
    "\n",
    "        response = agent_client.create_agent(\n",
    "            agentName=agent_name,\n",
    "            agentResourceRoleArn=agent_role_arn,\n",
    "            foundationModel=foundation_model_id,\n",
    "            instruction=instruction,\n",
    "            idleSessionTTLInSeconds=1800, # 30 minutes\n",
    "            description=f\"Experimental agent for Couchbase search and content formatting ({foundation_model_id})\"\n",
    "            # promptOverrideConfiguration={} # Optional: Add later if needed\n",
    "        )\n",
    "        agent_info = response.get('agent')\n",
    "        agent_id = agent_info.get('agentId')\n",
    "        agent_arn = agent_info.get('agentArn')\n",
    "        agent_status = agent_info.get('agentStatus')\n",
    "        logger.info(f\"Agent creation initiated. Name: {agent_name}, ID: {agent_id}, ARN: {agent_arn}, Status: {agent_status}\")\n",
    "\n",
    "        # Wait for agent to become NOT_PREPARED (initial state after creation)\n",
    "        # Using custom waiter logic as there might not be a standard one for this transition\n",
    "        logger.info(f\"Waiting for agent {agent_id} to reach initial state...\")\n",
    "        for _ in range(12): # Check for up to 1 minute\n",
    "             current_status = agent_client.get_agent(agentId=agent_id)['agent']['agentStatus']\n",
    "             logger.info(f\"Agent {agent_id} status: {current_status}\")\n",
    "             if current_status != 'CREATING': # Expect NOT_PREPARED or FAILED\n",
    "                  break\n",
    "             time.sleep(5)\n",
    "\n",
    "        final_status = agent_client.get_agent(agentId=agent_id)['agent']['agentStatus']\n",
    "        if final_status == 'FAILED':\n",
    "            logger.error(f\"Agent {agent_id} creation failed.\")\n",
    "            # Optionally retrieve failure reasons if API provides them\n",
    "            raise Exception(f\"Agent creation failed for {agent_name}\")\n",
    "        else:\n",
    "             logger.info(f\"Agent {agent_id} successfully created (Status: {final_status}).\")\n",
    "             \n",
    "        return agent_id, agent_arn\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error creating agent '{agent_name}': {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.10.2 create_action_group\n",
    "\n",
    "This function creates or updates an action group for the specified agent. Action groups define the tools an agent can use. In this Lambda-based approach, the action group links the agent to the Lambda function that implements the tool. Key steps include:\n",
    "- **Function Schema Definition:** It programmatically defines a `function_schema_details` dictionary. This schema describes the tool (`searchAndFormatDocuments`) that the Lambda function provides, including its name, description, and expected input parameters (`query`, `k`, `style`) with their types and whether they are required. This schema is what the agent uses to understand how to invoke the tool.\n",
    "- **Idempotency:** It first checks if an action group with the given `action_group_name` already exists for the `DRAFT` version of the agent. \n",
    "    - If it exists, it attempts to update the existing action group using `update_agent_action_group`, ensuring the `actionGroupExecutor` points to the correct Lambda ARN and that it uses the `functionSchema` (for defining the tool via its signature) rather than an OpenAPI schema.\n",
    "    - If it doesn't exist, it creates a new action group using `create_agent_action_group`.\n",
    "- **`actionGroupExecutor`:** This is set to `{'lambda': function_arn}`, where `function_arn` is the ARN of the deployed Lambda function. This tells Bedrock to invoke this Lambda when the agent decides to use a tool from this action group.\n",
    "- **`functionSchema` Parameter:** The `functionSchema` (containing the `function_schema_details`) is provided to the `create_agent_action_group` or `update_agent_action_group` call. This method of defining tools is simpler for single functions compared to providing a full OpenAPI schema, which is also an option for more complex APIs.\n",
    "- **State:** The action group is explicitly set to `ENABLED`.\n",
    "A brief pause is added after creation/update to allow changes to propagate. The function returns the `actionGroupId`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_action_group(agent_client, agent_id, action_group_name, function_arn, schema_path=None):\n",
    "    \"\"\"Creates an action group for the agent using Define with function details.\"\"\"\n",
    "    logger.info(f\"--- Creating/Updating Action Group (Function Details): {action_group_name} for Agent: {agent_id} ---\")\n",
    "    logger.info(f\"Lambda ARN: {function_arn}\")\n",
    "\n",
    "    # Define function schema details (for functionSchema parameter)\n",
    "    function_schema_details = {\n",
    "        'functions': [\n",
    "            {\n",
    "                'name': 'searchAndFormatDocuments', # Function name agent will call\n",
    "                'description': 'Performs vector search based on query, retrieves documents, and formats results using specified style.',\n",
    "                'parameters': {\n",
    "                    'query': {\n",
    "                        'description': 'The search query text.',\n",
    "                        'type': 'string',\n",
    "                        'required': True\n",
    "                    },\n",
    "                    'k': {\n",
    "                        'description': 'The maximum number of documents to retrieve.',\n",
    "                        'type': 'integer',\n",
    "                        'required': False # Making optional as Lambda has default\n",
    "                    },\n",
    "                    'style': {\n",
    "                        'description': 'The desired formatting style for the results (e.g., \\'bullet points\\', \\'paragraph\\', \\'summary\\').',\n",
    "                        'type': 'string',\n",
    "                        'required': False # Making optional as Lambda has default\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Check if Action Group already exists for the DRAFT version\n",
    "        try:\n",
    "             logger.info(f\"Checking if action group '{action_group_name}' already exists for agent {agent_id} DRAFT version...\")\n",
    "             paginator = agent_client.get_paginator('list_agent_action_groups')\n",
    "             existing_group = None\n",
    "             for page in paginator.paginate(agentId=agent_id, agentVersion='DRAFT'):\n",
    "                 for ag_summary in page.get('actionGroupSummaries', []):\n",
    "                      if ag_summary.get('actionGroupName') == action_group_name:\n",
    "                           existing_group = ag_summary\n",
    "                           break\n",
    "                 if existing_group:\n",
    "                      break\n",
    "             \n",
    "             if existing_group:\n",
    "                 ag_id = existing_group['actionGroupId']\n",
    "                 logger.warning(f\"Action Group '{action_group_name}' (ID: {ag_id}) already exists for agent {agent_id} DRAFT. Attempting update to Function Details.\")\n",
    "                 # Update existing action group - REMOVE apiSchema, ADD functionSchema\n",
    "                 response = agent_client.update_agent_action_group(\n",
    "                     agentId=agent_id,\n",
    "                     agentVersion='DRAFT',\n",
    "                     actionGroupId=ag_id,\n",
    "                     actionGroupName=action_group_name,\n",
    "                     actionGroupExecutor={'lambda': function_arn},\n",
    "                     functionSchema={ # Use functionSchema\n",
    "                         'functions': function_schema_details['functions'] # Pass the list with the correct key\n",
    "                     },\n",
    "                     actionGroupState='ENABLED'\n",
    "                 )\n",
    "                 ag_info = response.get('agentActionGroup')\n",
    "                 logger.info(f\"Successfully updated Action Group '{action_group_name}' (ID: {ag_info.get('actionGroupId')}) to use Function Details.\")\n",
    "                 return ag_info.get('actionGroupId')\n",
    "             else:\n",
    "                  logger.info(f\"Action group '{action_group_name}' does not exist. Creating new with Function Details.\")\n",
    "\n",
    "        except ClientError as e:\n",
    "             logger.error(f\"Error checking for existing action group '{action_group_name}': {e}. Proceeding with creation attempt.\")\n",
    "\n",
    "\n",
    "        # Create new action group if not found or update failed implicitly\n",
    "        response = agent_client.create_agent_action_group(\n",
    "            agentId=agent_id,\n",
    "            agentVersion='DRAFT', \n",
    "            actionGroupName=action_group_name,\n",
    "            actionGroupExecutor={\n",
    "                'lambda': function_arn \n",
    "            },\n",
    "            functionSchema={ # Use functionSchema\n",
    "                 'functions': function_schema_details['functions'] # Pass the list with the correct key\n",
    "            },\n",
    "            actionGroupState='ENABLED' \n",
    "        )\n",
    "        ag_info = response.get('agentActionGroup')\n",
    "        ag_id = ag_info.get('actionGroupId')\n",
    "        logger.info(f\"Successfully created Action Group '{action_group_name}' with ID: {ag_id} using Function Details.\")\n",
    "        time.sleep(5) # Pause after creation/update\n",
    "        return ag_id\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error creating/updating action group '{action_group_name}' using Function Details: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.10.3 prepare_agent\n",
    "\n",
    "This function initiates the preparation of the `DRAFT` version of the Bedrock Agent and waits for this process to complete. Preparation involves Bedrock compiling the agent's configuration (instructions, action groups, model settings) and making it ready for invocation. \n",
    "- It calls `bedrock_agent_client.prepare_agent(agentId=agent_id)`.\n",
    "- **Custom Waiter:** It then uses a custom-defined `boto3` waiter (`AgentPrepared`) to poll the agent's status. The waiter configuration specifies:\n",
    "    - `delay`: How often to check (e.g., every 30 seconds).\n",
    "    - `operation`: The SDK call to make for checking (`GetAgent`).\n",
    "    - `maxAttempts`: How many times to check before timing out (e.g., 20 attempts, for a total of up to 10 minutes).\n",
    "    - `acceptors`: Conditions that determine success, failure, or retry. It succeeds if `agent.agentStatus` becomes `PREPARED`, fails if it becomes `FAILED`, and retries if it's `UPDATING` (though `PREPARING` is the more typical intermediate state here).\n",
    "If the waiter times out or the agent preparation results in a `FAILED` status, an exception is raised. This step is crucial because an agent cannot be invoked (or an alias reliably pointed to its version) until it is successfully prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_agent(agent_client, agent_id):\n",
    "    \"\"\"Prepares the DRAFT version of the agent.\"\"\"\n",
    "    logger.info(f\"--- Preparing Agent: {agent_id} ---\")\n",
    "    try:\n",
    "        response = agent_client.prepare_agent(agentId=agent_id)\n",
    "        agent_version = response.get('agentVersion') # Should be DRAFT\n",
    "        prepared_at = response.get('preparedAt')\n",
    "        status = response.get('agentStatus') # Should be PREPARING\n",
    "        logger.info(f\"Agent preparation initiated for version '{agent_version}'. Status: {status}. Prepared At: {prepared_at}\")\n",
    "\n",
    "        # Wait for preparation to complete (PREPARED or FAILED)\n",
    "        logger.info(f\"Waiting for agent {agent_id} preparation to complete (up to 10 minutes)...\")\n",
    "        # Define a simple waiter config\n",
    "        waiter_config = {\n",
    "            'version': 2,\n",
    "            'waiters': {\n",
    "                'AgentPrepared': {\n",
    "                    'delay': 30, # Check every 30 seconds\n",
    "                    'operation': 'GetAgent',\n",
    "                    'maxAttempts': 20, # Max 10 minutes\n",
    "                    'acceptors': [\n",
    "                        {\n",
    "                            'matcher': 'path',\n",
    "                            'expected': 'PREPARED',\n",
    "                            'argument': 'agent.agentStatus',\n",
    "                            'state': 'success'\n",
    "                        },\n",
    "                        {\n",
    "                            'matcher': 'path',\n",
    "                            'expected': 'FAILED',\n",
    "                            'argument': 'agent.agentStatus',\n",
    "                            'state': 'failure'\n",
    "                        },\n",
    "                        {\n",
    "                            'matcher': 'path',\n",
    "                            'expected': 'UPDATING', # Can happen during prep? Treat as retryable\n",
    "                            'argument': 'agent.agentStatus',\n",
    "                            'state': 'retry'\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        waiter_model = WaiterModel(waiter_config)\n",
    "        custom_waiter = create_waiter_with_client('AgentPrepared', waiter_model, agent_client)\n",
    "\n",
    "        try: # Outer try for both preparation and alias handling\n",
    "             custom_waiter.wait(agentId=agent_id)\n",
    "             logger.info(f\"Agent {agent_id} successfully prepared.\")\n",
    "\n",
    "        except Exception as e: # Outer except catches prepare_agent wait errors OR unhandled alias errors\n",
    "            logger.error(f\"Agent {agent_id} preparation failed or timed out (or alias error): {e}\")\n",
    "            # Check final status if possible\n",
    "            try:\n",
    "                final_status = agent_client.get_agent(agentId=agent_id)['agent']['agentStatus']\n",
    "                logger.error(f\"Final agent status: {final_status}\")\n",
    "            except Exception as get_e:\n",
    "                logger.error(f\"Could not retrieve final agent status after wait failure: {get_e}\")\n",
    "            raise Exception(f\"Agent preparation or alias setup failed for {agent_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preparing agent {agent_id}: {e}\")\n",
    "        # Handle error, maybe exit\n",
    "        raise e # Re-raise the exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11 Agent Invocation Function\n",
    "\n",
    "This subsection provides the function used to interact with the prepared and aliased Bedrock Agent, sending it a prompt and processing its response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.11.1 test_agent_invocation\n",
    "\n",
    "This function is responsible for invoking the configured Bedrock Agent and handling its response. Key operations include:\n",
    "- **Invocation:** Calls `bedrock_agent_runtime_client.invoke_agent` with the `agentId`, `agentAliasId`, a unique `sessionId` (generated for each invocation in this script), the user's `prompt` (inputText), and `enableTrace=True` to get detailed trace information for debugging.\n",
    "- **Stream Processing:** The agent's response is a stream. The function iterates through the events in this stream (`response.get('completion', [])`).\n",
    "    - **`chunk` events:** These contain parts of the agent's textual response. The function decodes these byte chunks (UTF-8) and concatenates them to form the `completion_text`.\n",
    "    - **`trace` events:** If `enableTrace` was true, these events provide detailed insight into the agent's internal operations, such as which foundation model was called, the input to the model, any tool invocations (though in the Lambda approach, the tool invocation itself is handled by Bedrock calling Lambda, the trace might show the agent deciding to call it and the result from it), and rationale. The function collects these trace parts.\n",
    "- **Logging:** It logs the final combined `completion_text` and a summary of the trace events, which can be very helpful for understanding the agent's decision-making process and debugging any issues with tool invocation or response generation.\n",
    "It returns the final textual response from the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent_invocation(agent_runtime_client, agent_id, agent_alias_id, session_id, prompt):\n",
    "    \"\"\"Invokes the agent and prints the response.\"\"\"\n",
    "    logger.info(f\"--- Testing Agent Invocation (Agent ID: {agent_id}, Alias: {agent_alias_id}) ---\")\n",
    "    logger.info(f\"Session ID: {session_id}\")\n",
    "    logger.info(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "\n",
    "    try:\n",
    "        response = agent_runtime_client.invoke_agent(\n",
    "            agentId=agent_id,\n",
    "            agentAliasId=agent_alias_id,\n",
    "            sessionId=session_id,\n",
    "            inputText=prompt,\n",
    "            enableTrace=True # Enable trace for debugging\n",
    "        )\n",
    "\n",
    "        logger.info(\"Agent invocation successful. Processing response...\")\n",
    "        # print(f\"Raw Response: {response}\") # Optional: Print raw response for deep debug\n",
    "        completion_text = \"\"\n",
    "        trace_events = []\n",
    "\n",
    "        # The response is a stream. Iterate through the chunks.\n",
    "        for event in response.get('completion', []):\n",
    "            # print(f\"Event: {event}\") # Optional: Print each event\n",
    "            if 'chunk' in event:\n",
    "                data = event['chunk'].get('bytes', b'')\n",
    "                decoded_chunk = data.decode('utf-8')\n",
    "                completion_text += decoded_chunk\n",
    "                # Log chunk as it arrives (can be verbose)\n",
    "                # print(decoded_chunk, end='') \n",
    "            elif 'trace' in event:\n",
    "                trace_part = event['trace'].get('trace')\n",
    "                if trace_part:\n",
    "                     # Log the full trace part object for detailed debugging\n",
    "                    #  print(f\"Trace Event: {json.dumps(trace_part)}\")\n",
    "                     trace_events.append(trace_part)\n",
    "            else:\n",
    "                 logger.warning(f\"Unhandled event type in stream: {event}\")\n",
    "\n",
    "        # Log final combined response\n",
    "        logger.info(f\"--- Agent Final Response ---{completion_text}\")\n",
    "        \n",
    "        # Keep trace summary log (optional, can be removed if too verbose)\n",
    "        if trace_events:\n",
    "             logger.info(\"--- Invocation Trace Summary ---\")\n",
    "             for i, trace in enumerate(trace_events):\n",
    "                  trace_type = trace.get('type')\n",
    "                  step_type = trace.get('orchestration', {}).get('stepType')\n",
    "                  model_invocation_input = trace.get('modelInvocationInput')\n",
    "                  if model_invocation_input:\n",
    "                      fm_input = model_invocation_input.get('text',\n",
    "                          json.dumps(model_invocation_input.get('invocationInput',{}).get('toolConfiguration',{})) # Handle tool input\n",
    "                      )\n",
    "                      # logger.info(f\"  Model Input Snippet: {fm_input[:150]}...\") # Can be verbose\n",
    "                  observation = trace.get('observation')\n",
    "                  # if observation:\n",
    "                      # logger.info(f\"  Observation: {observation}\") # Can be verbose\n",
    "                  log_line = f\"Trace {i+1}: Type={trace_type}, Step={step_type}\"\n",
    "                  rationale = trace.get('rationale', {}).get('text')\n",
    "                  if rationale: log_line += f\", Rationale=\\\"{rationale[:100]}...\\\"\"\n",
    "                  logger.info(log_line) # Log summary line\n",
    "\n",
    "        return completion_text\n",
    "\n",
    "    except ClientError as e:\n",
    "        logger.error(f\"Error invoking agent: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None\n",
    "    except Exception as e:\n",
    "         logger.error(f\"Unexpected error during agent invocation: {e}\")\n",
    "         logger.error(traceback.format_exc())\n",
    "         return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Execution Flow\n",
    "\n",
    "This is the primary section of the notebook where all the previously defined helper functions are called in sequence to set up the complete Bedrock Agent environment with a Lambda-backed tool, and then test its invocation. The flow is designed to be largely idempotent where possible, meaning it can often be re-run, and it will attempt to clean up or reuse existing resources before creating new ones (e.g., IAM roles, Lambda functions, agents). The major steps are outlined below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initial Setup\n",
    "\n",
    "This first step in the main execution flow performs essential preliminary tasks:\n",
    "1.  Logs a starting message for the script execution.\n",
    "2.  Calls `check_environment_variables()` to ensure all required environment variables (AWS credentials, Couchbase password, etc.) are set. If not, it raises an `EnvironmentError` to halt execution, as the subsequent steps depend on these variables.\n",
    "3.  Calls `initialize_aws_clients()` to get the necessary `boto3` client objects for Bedrock, IAM, Lambda, etc.\n",
    "4.  Calls `connect_couchbase()` to establish a connection to the Couchbase cluster.\n",
    "If any of these critical initialization steps fail, an exception is raised to stop the notebook's execution, preventing errors in later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 16:33:50,043 - INFO - --- Starting Bedrock Agent Experiment Script ---\n",
      "2025-05-08 16:33:50,044 - INFO - All required environment variables are set.\n",
      "2025-05-08 16:33:50,044 - INFO - Initializing AWS clients in region: us-east-1\n",
      "2025-05-08 16:33:50,389 - INFO - AWS clients initialized successfully.\n",
      "2025-05-08 16:33:50,389 - INFO - Connecting to Couchbase cluster at couchbases://cb.hlcup4o4jmjr55yf.cloud.couchbase.com...\n",
      "2025-05-08 16:33:52,610 - INFO - Successfully connected to Couchbase.\n",
      "2025-05-08 16:33:52,611 - INFO - AWS clients and Couchbase connection initialized.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"--- Starting Bedrock Agent Experiment Script ---\")\n",
    "\n",
    "if not check_environment_variables():\n",
    "    # In a notebook, raising an exception might be better than exit(1)\n",
    "    raise EnvironmentError(\"Missing required environment variables. Check logs.\")\n",
    "\n",
    "# Initialize all clients, including the agent client\n",
    "try:\n",
    "    bedrock_runtime_client, iam_client, lambda_client, bedrock_agent_client, bedrock_agent_runtime_client = initialize_aws_clients()\n",
    "    cb_cluster = connect_couchbase()\n",
    "    logger.info(\"AWS clients and Couchbase connection initialized.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Initialization failed: {e}\")\n",
    "    raise # Re-raise the exception to stop execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Couchbase Setup\n",
    "\n",
    "This block focuses on preparing the Couchbase environment to serve as the vector store for the agent. It involves:\n",
    "1.  Calling `setup_collection()`: This helper function ensures that the target Couchbase bucket, scope, and collection (defined by `CB_BUCKET_NAME`, `SCOPE_NAME`, `COLLECTION_NAME`) are created if they don't already exist. It also ensures a primary index is present on the collection.\n",
    "2.  Calling `setup_search_index()`: This creates or updates the Couchbase Full-Text Search (FTS) index (named by `INDEX_NAME`) using the definition from `INDEX_JSON_PATH`. This search index is crucial for performing vector similarity searches.\n",
    "3.  Calling `clear_collection()`: This function deletes all existing documents from the target collection. This step ensures that each run of the notebook starts with a clean slate, preventing data from previous experiments from interfering with the current one.\n",
    "If any part of this Couchbase setup fails, an exception is logged and re-raised to stop further execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 16:33:52,617 - INFO - Setting up collection: vector-search-testing/shared/bedrock\n",
      "2025-05-08 16:33:53,708 - INFO - Bucket 'vector-search-testing' exists.\n",
      "2025-05-08 16:33:54,622 - INFO - Scope 'shared' already exists.\n",
      "2025-05-08 16:33:55,526 - INFO - Collection 'bedrock' already exists.\n",
      "2025-05-08 16:33:55,527 - INFO - Ensuring primary index exists on `vector-search-testing`.`shared`.`bedrock`...\n",
      "2025-05-08 16:33:56,470 - INFO - Primary index present or created successfully.\n",
      "2025-05-08 16:33:56,470 - INFO - Collection setup complete.\n",
      "2025-05-08 16:33:56,471 - INFO - Couchbase collection 'vector-search-testing.shared.bedrock' setup complete.\n",
      "2025-05-08 16:33:56,471 - INFO - Looking for index definition at: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/aws_index.json\n",
      "2025-05-08 16:33:56,472 - INFO - Loaded index definition from /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/aws_index.json, ensuring name is 'vector_search_bedrock' and source is 'vector-search-testing'.\n",
      "2025-05-08 16:33:56,472 - INFO - Upserting search index 'vector_search_bedrock'...\n",
      "2025-05-08 16:33:57,145 - WARNING - Search index 'vector_search_bedrock' likely already existed (caught QueryIndexAlreadyExistsException, check if applicable). Upsert attempted.\n",
      "2025-05-08 16:33:57,145 - INFO - Couchbase search index 'vector_search_bedrock' setup complete.\n",
      "2025-05-08 16:33:57,146 - WARNING - Attempting to clear all documents from `vector-search-testing`.`shared`.`bedrock`...\n",
      "2025-05-08 16:33:57,375 - WARNING - Could not retrieve mutation count after delete: 'list' object has no attribute 'meta_data'\n",
      "2025-05-08 16:33:57,376 - INFO - Successfully cleared documents from the collection (approx. 0 mutations).\n",
      "2025-05-08 16:33:57,377 - INFO - Cleared any existing documents from the collection.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Use the setup functions with the script's config variables\n",
    "    cb_collection = setup_collection(cb_cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)\n",
    "    logger.info(f\"Couchbase collection '{CB_BUCKET_NAME}.{SCOPE_NAME}.{COLLECTION_NAME}' setup complete.\")\n",
    "\n",
    "    # Pass required args to setup_search_index\n",
    "    setup_search_index(cb_cluster, INDEX_NAME, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME, INDEX_JSON_PATH)\n",
    "    logger.info(f\"Couchbase search index '{INDEX_NAME}' setup complete.\")\n",
    "\n",
    "    # Clear any existing documents from previous runs\n",
    "    clear_collection(cb_cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)\n",
    "    logger.info(\"Cleared any existing documents from the collection.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Couchbase setup failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Vector Store Initialization and Data Loading\n",
    "\n",
    "With the Couchbase infrastructure in place, this section prepares the LangChain vector store and populates it with data:\n",
    "1.  **Initialize `BedrockEmbeddings`:** Creates an instance of the `BedrockEmbeddings` client, specifying the `EMBEDDING_MODEL_ID` (e.g., Amazon Titan Text Embeddings V2). This client will be used by the vector store to convert text documents into numerical embeddings for similarity searching.\n",
    "2.  **Initialize `CouchbaseSearchVectorStore`:** Creates an instance of `CouchbaseSearchVectorStore`. This LangChain component acts as an abstraction layer over the Couchbase collection and search index, providing methods for adding documents and performing similarity searches. It's configured with the Couchbase cluster connection, bucket/scope/collection names, the embeddings client, and the search index name.\n",
    "3.  **Load Documents from JSON:** Reads document data from the `DOCS_JSON_PATH` file. This file is expected to contain a list of documents, each with `text` and `metadata` fields.\n",
    "4.  **Add Documents to Vector Store:** If documents are loaded, their texts and metadatas are extracted. The `vector_store.add_texts()` method is then called to process these documents: each document's text is converted into an embedding (using the `BedrockEmbeddings` client), and both the text and its embedding (along with metadata) are stored in the Couchbase collection. The search index (`INDEX_NAME`) is then updated to include these new vectors, making them searchable.\n",
    "Error handling is included to catch issues like file not found or problems during embedding generation or data insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 16:33:57,384 - INFO - Initializing Bedrock Embeddings client with model: amazon.titan-embed-text-v2:0\n",
      "2025-05-08 16:33:57,385 - INFO - Successfully created Bedrock embeddings client.\n",
      "2025-05-08 16:33:57,385 - INFO - Initializing CouchbaseSearchVectorStore with index: vector_search_bedrock\n",
      "2025-05-08 16:34:00,839 - INFO - Successfully created Couchbase vector store.\n",
      "2025-05-08 16:34:00,840 - INFO - Looking for documents at: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/documents.json\n",
      "2025-05-08 16:34:00,846 - INFO - Loaded 7 documents from /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/documents.json\n",
      "2025-05-08 16:34:00,848 - INFO - Adding 7 documents to vector store...\n",
      "2025-05-08 16:34:04,633 - INFO - Successfully added 7 documents to the vector store.\n",
      "2025-05-08 16:34:04,633 - INFO - --- Couchbase Setup and Data Loading Complete ---\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logger.info(f\"Initializing Bedrock Embeddings client with model: {EMBEDDING_MODEL_ID}\")\n",
    "    embeddings = BedrockEmbeddings(\n",
    "        client=bedrock_runtime_client,\n",
    "        model_id=EMBEDDING_MODEL_ID\n",
    "    )\n",
    "    logger.info(\"Successfully created Bedrock embeddings client.\")\n",
    "\n",
    "    logger.info(f\"Initializing CouchbaseSearchVectorStore with index: {INDEX_NAME}\")\n",
    "    vector_store = CouchbaseSearchVectorStore(\n",
    "        cluster=cb_cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=embeddings,\n",
    "        index_name=INDEX_NAME\n",
    "    )\n",
    "    logger.info(\"Successfully created Couchbase vector store.\")\n",
    "\n",
    "    # Load documents from JSON file\n",
    "    logger.info(f\"Looking for documents at: {DOCS_JSON_PATH}\")\n",
    "    if not os.path.exists(DOCS_JSON_PATH):\n",
    "         logger.error(f\"Documents file not found: {DOCS_JSON_PATH}\")\n",
    "         raise FileNotFoundError(f\"Documents file not found: {DOCS_JSON_PATH}\")\n",
    "\n",
    "    with open(DOCS_JSON_PATH, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        documents_to_load = data.get('documents', [])\n",
    "    logger.info(f\"Loaded {len(documents_to_load)} documents from {DOCS_JSON_PATH}\")\n",
    "\n",
    "    # Add documents to vector store\n",
    "    if documents_to_load:\n",
    "        logger.info(f\"Adding {len(documents_to_load)} documents to vector store...\")\n",
    "        texts = [doc.get('text', '') for doc in documents_to_load]\n",
    "        metadatas = []\n",
    "        for i, doc in enumerate(documents_to_load):\n",
    "            metadata_raw = doc.get('metadata', {})\n",
    "            if isinstance(metadata_raw, str):\n",
    "                try:\n",
    "                    metadata = json.loads(metadata_raw)\n",
    "                    if not isinstance(metadata, dict):\n",
    "                         logger.warning(f\"Metadata for doc {i} parsed from string is not a dict: {metadata}. Using empty dict.\")\n",
    "                         metadata = {}\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.warning(f\"Could not parse metadata string for doc {i}: {metadata_raw}. Using empty dict.\")\n",
    "                    metadata = {}\n",
    "            elif isinstance(metadata_raw, dict):\n",
    "                metadata = metadata_raw\n",
    "            else:\n",
    "                logger.warning(f\"Metadata for doc {i} is not a string or dict: {metadata_raw}. Using empty dict.\")\n",
    "                metadata = {}\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "        inserted_ids = vector_store.add_texts(texts=texts, metadatas=metadatas)\n",
    "        logger.info(f\"Successfully added {len(inserted_ids)} documents to the vector store.\")\n",
    "    else:\n",
    "         logger.warning(\"No documents found in the JSON file to add.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "     logger.error(f\"Setup failed: {e}\")\n",
    "     raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during vector store setup or data loading: {e}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise \n",
    "\n",
    "logger.info(\"--- Couchbase Setup and Data Loading Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Create IAM Role\n",
    "\n",
    "This step ensures that the necessary IAM (Identity and Access Management) role for the Bedrock Agent and its Lambda function is in place. \n",
    "- It defines a `agent_role_name` (e.g., `bedrock_agent_lambda_exp_role`).\n",
    "- It calls the `create_agent_role()` helper function. This function (described in section 3.7) either creates a new IAM role with this name or updates an existing one. \n",
    "- The role is configured with a trust policy allowing both the Bedrock service and the Lambda service to assume it. \n",
    "- It attaches necessary permissions policies, including `AWSLambdaBasicExecutionRole` for Lambda logging and custom inline policies for Bedrock access and any other required permissions.\n",
    "- The AWS Account ID, needed for defining precise resource ARNs in policies, is fetched dynamically using the STS client if not already available as an environment variable.\n",
    "The ARN of this role (`agent_role_arn`) is stored, as it's a required parameter for creating both the Bedrock Agent and the AWS Lambda function that the agent will invoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 16:34:04,639 - INFO - Checking/Creating IAM role: bedrock_agent_lambda_exp_role\n",
      "2025-05-08 16:34:05,586 - INFO - IAM role 'bedrock_agent_lambda_exp_role' already exists with ARN: arn:aws:iam::598307997273:role/bedrock_agent_lambda_exp_role\n",
      "2025-05-08 16:34:05,586 - INFO - Updating trust policy for existing role 'bedrock_agent_lambda_exp_role'...\n",
      "2025-05-08 16:34:05,851 - INFO - Trust policy updated for role 'bedrock_agent_lambda_exp_role'.\n",
      "2025-05-08 16:34:05,851 - INFO - Attaching basic Lambda execution policy to role 'bedrock_agent_lambda_exp_role'...\n",
      "2025-05-08 16:34:06,120 - INFO - Attached basic Lambda execution policy.\n",
      "2025-05-08 16:34:06,121 - INFO - Putting basic inline policy 'LambdaBasicLoggingPermissions' for role 'bedrock_agent_lambda_exp_role'...\n",
      "2025-05-08 16:34:06,376 - INFO - Successfully put inline policy 'LambdaBasicLoggingPermissions'.\n",
      "2025-05-08 16:34:06,377 - INFO - Putting Bedrock permissions policy 'BedrockAgentPermissions' for role 'bedrock_agent_lambda_exp_role'...\n",
      "2025-05-08 16:34:06,645 - INFO - Successfully put inline policy 'BedrockAgentPermissions'.\n",
      "2025-05-08 16:34:06,646 - INFO - Waiting 10s for policy changes to propagate...\n",
      "2025-05-08 16:34:16,651 - INFO - Agent IAM Role ARN: arn:aws:iam::598307997273:role/bedrock_agent_lambda_exp_role\n"
     ]
    }
   ],
   "source": [
    "agent_role_name = \"bedrock_agent_lambda_exp_role\"\n",
    "try:\n",
    "    # Ensure AWS_ACCOUNT_ID is loaded correctly\n",
    "    if not AWS_ACCOUNT_ID:\n",
    "        logger.info(\"Attempting to fetch AWS Account ID...\")\n",
    "        sts_client = boto3.client('sts', region_name=AWS_REGION)\n",
    "        AWS_ACCOUNT_ID = sts_client.get_caller_identity().get('Account')\n",
    "        if not AWS_ACCOUNT_ID:\n",
    "            raise ValueError(\"AWS Account ID could not be determined. Please set the AWS_ACCOUNT_ID environment variable.\")\n",
    "        logger.info(f\"Fetched AWS Account ID: {AWS_ACCOUNT_ID}\")\n",
    "        \n",
    "    agent_role_arn = create_agent_role(iam_client, agent_role_name, AWS_ACCOUNT_ID)\n",
    "    logger.info(f\"Agent IAM Role ARN: {agent_role_arn}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create/verify IAM role: {e}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Deploy Lambda Function\n",
    "\n",
    "This section orchestrates the deployment of the AWS Lambda function that will execute the agent's `searchAndFormatDocuments` tool. The process involves several steps managed by the helper functions:\n",
    "1.  **Define Lambda Details:** Specifies the `search_format_lambda_name` (e.g., `bedrock_agent_search_format_exp`), the `lambda_source_dir` (where the Lambda's Python script and `Makefile` are located), and `lambda_build_dir` (where the final .zip package will be placed).\n",
    "2.  **Cleanup Old Lambdas (Optional but Recommended):** Calls `delete_lambda_function` for potentially conflicting older Lambda functions (e.g., separate researcher/writer Lambdas from previous experiments or an old version of the current combined Lambda). This ensures a cleaner environment, especially during iterative development.\n",
    "3.  **Package Lambda:** Calls `package_function()`. This helper (described in 3.8.3) uses the `Makefile` in `lambda_source_dir` to install dependencies, prepare the handler script (`bedrock_agent_search_and_format.py`), and create a .zip deployment package (`search_format_zip_path`).\n",
    "4.  **Create/Update Lambda in AWS:** Calls `create_lambda_function()`. This helper (described in 3.8.4) takes the .zip package and either creates a new Lambda function in AWS or updates an existing one. It handles S3 upload for large packages, sets environment variables (like Couchbase connection info and Bedrock model IDs), configures the IAM role, runtime, handler, timeout, and memory. It also adds permissions for Bedrock to invoke the Lambda and waits for the Lambda to become active.\n",
    "5.  **Cleanup Deployment Package:** After successful deployment, the local .zip file is removed to save space.\n",
    "The ARN of the deployed Lambda (`search_format_lambda_arn`) is stored, as it's needed to link this Lambda to the Bedrock Agent's action group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 16:34:16,658 - INFO - --- Starting Lambda Deployment (Single Function) --- \n",
      "2025-05-08 16:34:16,659 - INFO - Deleting potentially conflicting old Lambda functions...\n",
      "2025-05-08 16:34:16,659 - INFO - Attempting to delete Lambda function: bedrock_agent_researcher_exp...\n",
      "2025-05-08 16:34:16,660 - INFO - Attempting to remove permission AllowBedrockInvokeBasic-bedrock_agent_researcher_exp from bedrock_agent_researcher_exp...\n",
      "2025-05-08 16:34:17,492 - INFO - Permission AllowBedrockInvokeBasic-bedrock_agent_researcher_exp not found on bedrock_agent_researcher_exp. Skipping removal.\n",
      "2025-05-08 16:34:17,793 - INFO - Lambda function 'bedrock_agent_researcher_exp' does not exist. No need to delete.\n",
      "2025-05-08 16:34:17,793 - INFO - Attempting to delete Lambda function: bedrock_agent_writer_exp...\n",
      "2025-05-08 16:34:17,794 - INFO - Attempting to remove permission AllowBedrockInvokeBasic-bedrock_agent_writer_exp from bedrock_agent_writer_exp...\n",
      "2025-05-08 16:34:18,115 - INFO - Permission AllowBedrockInvokeBasic-bedrock_agent_writer_exp not found on bedrock_agent_writer_exp. Skipping removal.\n",
      "2025-05-08 16:34:18,437 - INFO - Lambda function 'bedrock_agent_writer_exp' does not exist. No need to delete.\n",
      "2025-05-08 16:34:18,445 - INFO - Attempting to delete Lambda function: bedrock_agent_search_format_exp...\n",
      "2025-05-08 16:34:18,447 - INFO - Attempting to remove permission AllowBedrockInvokeBasic-bedrock_agent_search_format_exp from bedrock_agent_search_format_exp...\n",
      "2025-05-08 16:34:18,732 - INFO - Successfully removed permission AllowBedrockInvokeBasic-bedrock_agent_search_format_exp from bedrock_agent_search_format_exp.\n",
      "2025-05-08 16:34:21,096 - INFO - Function bedrock_agent_search_format_exp exists. Deleting...\n",
      "2025-05-08 16:34:21,638 - INFO - Waiting for bedrock_agent_search_format_exp to be deleted...\n",
      "2025-05-08 16:34:31,645 - INFO - Function bedrock_agent_search_format_exp deletion initiated.\n",
      "2025-05-08 16:34:31,648 - INFO - Old Lambda deletion checks complete.\n",
      "2025-05-08 16:34:31,649 - INFO - Packaging Lambda function 'bedrock_agent_search_format_exp'...\n",
      "2025-05-08 16:34:31,653 - INFO - --- Packaging function bedrock_agent_search_and_format --- \n",
      "2025-05-08 16:34:31,654 - INFO - Source Dir (Makefile location & make cwd): /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions\n",
      "2025-05-08 16:34:31,654 - INFO - Build Dir (Final zip location): /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments\n",
      "2025-05-08 16:34:31,655 - INFO - Copying /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/bedrock_agent_search_and_format.py to /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/lambda_function.py\n",
      "2025-05-08 16:34:31,660 - INFO - Running make command: make -f /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/Makefile clean package (in /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions)\n",
      "2025-05-08 16:34:56,840 - INFO - Make command completed successfully.\n",
      "2025-05-08 16:34:56,841 - INFO - Moving and renaming /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/lambda_package.zip to /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_search_and_format.zip\n",
      "2025-05-08 16:34:56,842 - INFO - Zip file ready: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_search_and_format.zip\n",
      "2025-05-08 16:34:56,842 - INFO - Cleaning up temporary script: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/lambda_functions/lambda_function.py\n",
      "2025-05-08 16:34:56,843 - INFO - Lambda function packaged at: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_search_and_format.zip\n",
      "2025-05-08 16:34:56,843 - INFO - Creating/Updating Lambda function 'bedrock_agent_search_format_exp'...\n",
      "2025-05-08 16:34:56,843 - INFO - Deploying Lambda function bedrock_agent_search_format_exp from /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_search_and_format.zip...\n",
      "2025-05-08 16:34:56,857 - INFO - Found credentials in environment variables.\n",
      "2025-05-08 16:34:57,008 - INFO - Zip file size: 50.58 MB\n",
      "2025-05-08 16:34:57,008 - INFO - Package size (50.58 MB) requires S3 deployment.\n",
      "2025-05-08 16:34:57,008 - INFO - Preparing to upload /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_search_and_format.zip to S3 in region us-east-1...\n",
      "2025-05-08 16:34:58,163 - INFO - Generated unique S3 bucket name: lambda-deployment-598307997273-1746702298\n",
      "2025-05-08 16:34:58,952 - INFO - Creating S3 bucket: lambda-deployment-598307997273-1746702298...\n",
      "2025-05-08 16:34:59,399 - INFO - Created S3 bucket: lambda-deployment-598307997273-1746702298. Waiting for availability...\n",
      "2025-05-08 16:34:59,695 - INFO - Bucket lambda-deployment-598307997273-1746702298 is available.\n",
      "2025-05-08 16:34:59,695 - INFO - Uploading /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_search_and_format.zip to s3://lambda-deployment-598307997273-1746702298/lambda/bedrock_agent_search_and_format.zip-39ffda44...\n",
      "2025-05-08 16:35:30,171 - INFO - Successfully uploaded to s3://lambda-deployment-598307997273-1746702298/lambda/bedrock_agent_search_and_format.zip-39ffda44\n",
      "2025-05-08 16:35:30,174 - INFO - Creating function 'bedrock_agent_search_format_exp' (attempt 1/3)...\n",
      "2025-05-08 16:35:33,184 - INFO - Successfully created function 'bedrock_agent_search_format_exp' with ARN: arn:aws:lambda:us-east-1:598307997273:function:bedrock_agent_search_format_exp\n",
      "2025-05-08 16:35:38,190 - INFO - Adding basic invoke permission (AllowBedrockInvokeBasic-bedrock_agent_search_format_exp) to bedrock_agent_search_format_exp...\n",
      "2025-05-08 16:35:38,497 - INFO - Successfully added basic invoke permission AllowBedrockInvokeBasic-bedrock_agent_search_format_exp.\n",
      "2025-05-08 16:35:38,498 - INFO - Waiting for function 'bedrock_agent_search_format_exp' to become active...\n",
      "2025-05-08 16:35:38,807 - INFO - Function 'bedrock_agent_search_format_exp' is active.\n",
      "2025-05-08 16:35:38,810 - INFO - Search/Format Lambda Deployed: arn:aws:lambda:us-east-1:598307997273:function:bedrock_agent_search_format_exp\n",
      "2025-05-08 16:35:38,811 - INFO - Cleaning up deployment zip file...\n",
      "2025-05-08 16:35:38,815 - INFO - Removed zip file: /Users/kaustavghosh/Desktop/vector-search-cookbook/awsbedrock-agents/lambda-experiments/bedrock_agent_search_and_format.zip\n",
      "2025-05-08 16:35:38,815 - INFO - --- Lambda Deployment Complete --- \n"
     ]
    }
   ],
   "source": [
    "search_format_lambda_name = \"bedrock_agent_search_format_exp\"\n",
    "# Adjust source/build dirs for notebook context if necessary\n",
    "lambda_source_dir = os.path.join(SCRIPT_DIR, 'lambda_functions') \n",
    "lambda_build_dir = SCRIPT_DIR # Final zip ends up in the notebook's directory\n",
    "\n",
    "logger.info(\"--- Starting Lambda Deployment (Single Function) --- \")\n",
    "search_format_lambda_arn = None\n",
    "search_format_zip_path = None\n",
    "\n",
    "try:\n",
    "    # Delete old lambdas if they exist (optional, but good cleanup)\n",
    "    logger.info(\"Deleting potentially conflicting old Lambda functions...\")\n",
    "    delete_lambda_function(lambda_client, \"bedrock_agent_researcher_exp\")\n",
    "    delete_lambda_function(lambda_client, \"bedrock_agent_writer_exp\")\n",
    "    # Delete the new lambda if it exists from a previous run\n",
    "    delete_lambda_function(lambda_client, search_format_lambda_name)\n",
    "    logger.info(\"Old Lambda deletion checks complete.\")\n",
    "\n",
    "    logger.info(f\"Packaging Lambda function '{search_format_lambda_name}'...\")\n",
    "    search_format_zip_path = package_function(\"bedrock_agent_search_and_format\", lambda_source_dir, lambda_build_dir)\n",
    "    logger.info(f\"Lambda function packaged at: {search_format_zip_path}\")\n",
    "\n",
    "    logger.info(f\"Creating/Updating Lambda function '{search_format_lambda_name}'...\")\n",
    "    search_format_lambda_arn = create_lambda_function(\n",
    "        lambda_client=lambda_client, function_name=search_format_lambda_name,\n",
    "        handler='lambda_function.lambda_handler', role_arn=agent_role_arn,\n",
    "        zip_file=search_format_zip_path, region=AWS_REGION\n",
    "    )\n",
    "    logger.info(f\"Search/Format Lambda Deployed: {search_format_lambda_arn}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "     logger.error(f\"Lambda packaging failed: Required file not found. {e}\")\n",
    "     raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Lambda deployment failed: {e}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise\n",
    "finally:\n",
    "    logger.info(\"Cleaning up deployment zip file...\")\n",
    "    if search_format_zip_path and os.path.exists(search_format_zip_path):\n",
    "        try:\n",
    "            os.remove(search_format_zip_path)\n",
    "            logger.info(f\"Removed zip file: {search_format_zip_path}\")\n",
    "        except OSError as e:\n",
    "            logger.warning(f\"Could not remove zip file {search_format_zip_path}: {e}\")\n",
    "\n",
    "logger.info(\"--- Lambda Deployment Complete --- \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Agent Setup\n",
    "\n",
    "This part of the script focuses on creating the Bedrock Agent itself.\n",
    "1.  **Define Agent Name:** An `agent_name` is defined (e.g., `couchbase_search_format_agent_exp`).\n",
    "2.  **Cleanup Existing Agent (Idempotency):** It calls `delete_agent_and_resources()` first. This helper function (described in 3.9.3) attempts to find an agent with the same name and, if found, deletes it along with its action groups and aliases. This ensures that each run starts with a clean slate for the agent, preventing conflicts or issues from previous configurations.\n",
    "3.  **Create New Agent:** After the cleanup attempt, it calls `create_agent()`. This helper function (described in 3.10.1) creates a new Bedrock Agent with the specified name, the IAM role ARN (`agent_role_arn`), the foundation model ID (`AGENT_MODEL_ID`), and a set of instructions guiding the agent on how to behave and use its tools.\n",
    "The `agent_id` and `agent_arn` returned by `create_agent()` are stored for subsequent steps like creating action groups and preparing the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 16:35:38,825 - INFO - Checking for and deleting existing agent: couchbase_search_format_agent_exp\n",
      "2025-05-08 16:35:38,826 - INFO - Attempting to find agent by name: couchbase_search_format_agent_exp\n",
      "2025-05-08 16:35:39,714 - INFO - Found agent 'couchbase_search_format_agent_exp' with ID: VFK6ZEOFGW\n",
      "2025-05-08 16:35:39,714 - WARNING - --- Deleting Agent Resources for 'couchbase_search_format_agent_exp' (ID: VFK6ZEOFGW) ---\n",
      "2025-05-08 16:35:39,715 - INFO - Listing action groups for agent VFK6ZEOFGW...\n",
      "2025-05-08 16:35:40,042 - INFO - Found 1 action groups to delete.\n",
      "2025-05-08 16:35:40,043 - INFO - Attempting to delete action group EY8TDM3LFI for agent VFK6ZEOFGW...\n",
      "2025-05-08 16:35:40,384 - INFO - Successfully deleted action group EY8TDM3LFI for agent VFK6ZEOFGW.\n",
      "2025-05-08 16:35:45,390 - INFO - Attempting to delete agent VFK6ZEOFGW ('couchbase_search_format_agent_exp')...\n",
      "2025-05-08 16:35:45,735 - INFO - Waiting up to 2 minutes for agent VFK6ZEOFGW deletion...\n",
      "2025-05-08 16:35:51,737 - INFO - Agent VFK6ZEOFGW successfully deleted.\n",
      "2025-05-08 16:35:51,738 - INFO - --- Agent Resource Deletion Complete for 'couchbase_search_format_agent_exp' ---\n",
      "2025-05-08 16:35:51,738 - INFO - Deletion process completed for any existing agent named couchbase_search_format_agent_exp.\n",
      "2025-05-08 16:35:51,739 - INFO - --- Creating Agent: couchbase_search_format_agent_exp ---\n",
      "2025-05-08 16:35:51,739 - INFO - --- Creating Agent: couchbase_search_format_agent_exp ---\n",
      "2025-05-08 16:35:52,334 - INFO - Agent creation initiated. Name: couchbase_search_format_agent_exp, ID: YU7YWIVKVP, ARN: arn:aws:bedrock:us-east-1:598307997273:agent/YU7YWIVKVP, Status: CREATING\n",
      "2025-05-08 16:35:52,335 - INFO - Waiting for agent YU7YWIVKVP to reach initial state...\n",
      "2025-05-08 16:35:52,806 - INFO - Agent YU7YWIVKVP status: CREATING\n",
      "2025-05-08 16:35:58,248 - INFO - Agent YU7YWIVKVP status: NOT_PREPARED\n",
      "2025-05-08 16:35:58,695 - INFO - Agent YU7YWIVKVP successfully created (Status: NOT_PREPARED).\n",
      "2025-05-08 16:35:58,696 - INFO - Agent created successfully. ID: YU7YWIVKVP, ARN: arn:aws:bedrock:us-east-1:598307997273:agent/YU7YWIVKVP\n"
     ]
    }
   ],
   "source": [
    "agent_name = f\"couchbase_search_format_agent_exp\"\n",
    "agent_id = None\n",
    "agent_arn = None\n",
    "alias_name = \"prod\" # Define alias name here\n",
    "# agent_alias_id_to_use will be set later after preparation\n",
    "\n",
    "# 1. Attempt to find and delete existing agent to ensure a clean state\n",
    "logger.info(f\"Checking for and deleting existing agent: {agent_name}\")\n",
    "try:\n",
    "    delete_agent_and_resources(bedrock_agent_client, agent_name) # Handles finding and deleting\n",
    "    logger.info(f\"Deletion process completed for any existing agent named {agent_name}.\")\n",
    "except Exception as e:\n",
    "    # Log error during find/delete but proceed to creation attempt\n",
    "    logger.error(f\"Error during agent finding/deletion phase: {e}. Proceeding to creation attempt.\")\n",
    "\n",
    "# 2. Always attempt to create the agent after the delete phase\n",
    "logger.info(f\"--- Creating Agent: {agent_name} ---\")\n",
    "try:\n",
    "    agent_id, agent_arn = create_agent(\n",
    "        agent_client=bedrock_agent_client,\n",
    "        agent_name=agent_name,\n",
    "        agent_role_arn=agent_role_arn,\n",
    "        foundation_model_id=AGENT_MODEL_ID\n",
    "    )\n",
    "    if not agent_id:\n",
    "        raise Exception(\"create_agent function did not return a valid agent ID.\")\n",
    "    logger.info(f\"Agent created successfully. ID: {agent_id}, ARN: {agent_arn}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to create agent '{agent_name}': {e}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Action Group Setup\n",
    "\n",
    "Once the agent is created and the Lambda function is deployed, this step links them together by creating an Action Group. \n",
    "- It defines an `action_group_name` (e.g., `SearchAndFormatActionGroup`).\n",
    "- It calls the `create_action_group()` helper function (described in 3.10.2). This function is responsible for:\n",
    "    - Taking the `agent_id` and the `search_format_lambda_arn` (the ARN of the deployed Lambda function) as input.\n",
    "    - Defining the `functionSchema` which tells the agent how to use the Lambda function (i.e., the tool name `searchAndFormatDocuments` and its parameters like `query`, `k`, `style`).\n",
    "    - Setting the `actionGroupExecutor` to point to the Lambda ARN, so Bedrock knows which Lambda to invoke.\n",
    "    - Creating a new action group or updating an existing one with the same name for the `DRAFT` version of the agent.\n",
    "- A 30-second pause (`time.sleep(30)`) is added after the action group setup. This is a crucial step to give AWS services enough time to propagate the changes and ensure that the agent is aware of the newly configured or updated action group before proceeding to the preparation phase. Without such a delay, the preparation step might fail or not correctly incorporate the action group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 16:35:58,703 - INFO - Creating/Updating Action Group 'SearchAndFormatActionGroup' for agent YU7YWIVKVP...\n",
      "2025-05-08 16:35:58,703 - INFO - --- Creating/Updating Action Group (Function Details): SearchAndFormatActionGroup for Agent: YU7YWIVKVP ---\n",
      "2025-05-08 16:35:58,704 - INFO - Lambda ARN: arn:aws:lambda:us-east-1:598307997273:function:bedrock_agent_search_format_exp\n",
      "2025-05-08 16:35:58,704 - INFO - Checking if action group 'SearchAndFormatActionGroup' already exists for agent YU7YWIVKVP DRAFT version...\n",
      "2025-05-08 16:35:59,013 - INFO - Action group 'SearchAndFormatActionGroup' does not exist. Creating new with Function Details.\n",
      "2025-05-08 16:35:59,417 - INFO - Successfully created Action Group 'SearchAndFormatActionGroup' with ID: BDXKOMHCKI using Function Details.\n",
      "2025-05-08 16:36:04,423 - INFO - Action Group 'SearchAndFormatActionGroup' created/updated with ID: BDXKOMHCKI\n",
      "2025-05-08 16:36:04,423 - INFO - Waiting 30s after action group setup before preparing agent...\n"
     ]
    }
   ],
   "source": [
    "# --- Action Group Creation/Update (Now assumes agent_id is valid) ---\n",
    "action_group_name = \"SearchAndFormatActionGroup\"\n",
    "action_group_id = None\n",
    "try:\n",
    "    if not agent_id:\n",
    "        raise ValueError(\"Agent ID is not set. Cannot create action group.\")\n",
    "    if not search_format_lambda_arn:\n",
    "        raise ValueError(\"Lambda ARN is not set. Cannot create action group.\")\n",
    "        \n",
    "    logger.info(f\"Creating/Updating Action Group '{action_group_name}' for agent {agent_id}...\")\n",
    "    action_group_id = create_action_group(\n",
    "        agent_client=bedrock_agent_client,\n",
    "        agent_id=agent_id,\n",
    "        action_group_name=action_group_name,\n",
    "        function_arn=search_format_lambda_arn,\n",
    "        # schema_path=None # No longer needed explicitly if default is None\n",
    "    )\n",
    "    if not action_group_id:\n",
    "        raise Exception(\"create_action_group did not return a valid ID.\")\n",
    "    logger.info(f\"Action Group '{action_group_name}' created/updated with ID: {action_group_id}\")\n",
    "\n",
    "    # Add a slightly longer wait after action group modification/creation\n",
    "    logger.info(\"Waiting 30s after action group setup before preparing agent...\")\n",
    "    time.sleep(30)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to set up action group: {e}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Prepare Agent and Handle Alias\n",
    "\n",
    "After the agent and its action group (linking to the Lambda tool) are defined, this section makes the agent ready for use and assigns an alias to it:\n",
    "1.  **Prepare Agent:** It calls the `prepare_agent()` helper function (described in 3.10.3). This function initiates the preparation process for the `DRAFT` version of the agent and uses a custom waiter to wait until the agent's status becomes `PREPARED`. This step is vital as it compiles all agent configurations.\n",
    "2.  **Alias Handling (Create or Update):** Once the agent is successfully prepared:\n",
    "    - An `alias_name` (e.g., `prod`) is defined.\n",
    "    - The code checks if an alias with this name already exists for the agent using `list_agent_aliases`.\n",
    "    - If the alias exists, its ID (`agent_alias_id_to_use`) is retrieved. The notebook assumes the existing alias will correctly point to the latest prepared version (DRAFT) or could be updated if necessary (though direct update logic for the alias to point to a specific version isn't explicitly shown here beyond creation).\n",
    "    - If the alias does not exist, `create_agent_alias()` is called. This creates a new alias that, by default, points to the latest prepared version of the agent (which is the `DRAFT` version that was just prepared).\n",
    "    - A brief pause (`time.sleep(10)`) is added to allow the alias changes to propagate.\n",
    "The `agent_alias_id_to_use` is now ready for invoking the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 16:36:34,472 - INFO - --- Preparing Agent: YU7YWIVKVP ---\n",
      "2025-05-08 16:36:34,508 - INFO - --- Preparing Agent: YU7YWIVKVP ---\n",
      "2025-05-08 16:36:35,064 - INFO - Agent preparation initiated for version 'DRAFT'. Status: PREPARING. Prepared At: 2025-05-08 11:06:35.026059+00:00\n",
      "2025-05-08 16:36:35,065 - INFO - Waiting for agent YU7YWIVKVP preparation to complete (up to 10 minutes)...\n",
      "2025-05-08 16:37:06,059 - INFO - Agent YU7YWIVKVP successfully prepared.\n",
      "2025-05-08 16:37:06,060 - INFO - Agent YU7YWIVKVP preparation seems complete (waiter succeeded).\n",
      "2025-05-08 16:37:06,060 - INFO - --- Setting up Alias 'prod' for Agent YU7YWIVKVP ---\n",
      "2025-05-08 16:37:06,061 - INFO - Checking for alias 'prod' for agent YU7YWIVKVP...\n",
      "2025-05-08 16:37:06,366 - INFO - Alias 'prod' not found. Creating new alias...\n",
      "2025-05-08 16:37:06,771 - INFO - Successfully created alias 'prod' with ID: QWIYXVWI9T. (Defaults to latest prepared version - DRAFT)\n",
      "2025-05-08 16:37:06,771 - INFO - Waiting 10s for alias 'prod' changes to propagate...\n",
      "2025-05-08 16:37:16,774 - INFO - Agent YU7YWIVKVP preparation and alias 'prod' (QWIYXVWI9T) setup complete.\n"
     ]
    }
   ],
   "source": [
    "agent_alias_id_to_use = None # Initialize alias ID\n",
    "alias_name = \"prod\" # Make sure alias_name is defined\n",
    "if agent_id:\n",
    "    logger.info(f\"--- Preparing Agent: {agent_id} ---\")\n",
    "    preparation_successful = False\n",
    "    try:\n",
    "        # prepare_agent now ONLY prepares, doesn't handle alias or return its ID\n",
    "        prepare_agent(bedrock_agent_client, agent_id) \n",
    "        logger.info(f\"Agent {agent_id} preparation seems complete (waiter succeeded).\")\n",
    "        preparation_successful = True # Flag success\n",
    "\n",
    "    except Exception as e: # Catch errors from preparation\n",
    "        logger.error(f\"Error during agent preparation for {agent_id}: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise # Stop execution if preparation fails\n",
    "\n",
    "    # --- Alias Handling (runs only if preparation succeeded) ---\n",
    "    if preparation_successful:\n",
    "        logger.info(f\"--- Setting up Alias '{alias_name}' for Agent {agent_id} ---\") # Add log\n",
    "        try:\n",
    "            # --- Alias Creation/Update Logic (Copied/adapted from main.py's __main__) ---\n",
    "            logger.info(f\"Checking for alias '{alias_name}' for agent {agent_id}...\")\n",
    "            existing_alias = None\n",
    "            paginator = bedrock_agent_client.get_paginator('list_agent_aliases')\n",
    "            for page in paginator.paginate(agentId=agent_id):\n",
    "                for alias_summary in page.get('agentAliasSummaries', []):\n",
    "                    if alias_summary.get('agentAliasName') == alias_name:\n",
    "                        existing_alias = alias_summary\n",
    "                        break\n",
    "                if existing_alias:\n",
    "                    break\n",
    "            \n",
    "            if existing_alias:\n",
    "                agent_alias_id_to_use = existing_alias['agentAliasId']\n",
    "                logger.info(f\"Using existing alias '{alias_name}' with ID: {agent_alias_id_to_use}.\")\n",
    "                # Optional: Update alias to point to DRAFT if needed, \n",
    "                # but create_agent_alias defaults to latest prepared (DRAFT) so just checking existence is often enough.\n",
    "            else:\n",
    "                logger.info(f\"Alias '{alias_name}' not found. Creating new alias...\")\n",
    "                create_alias_response = bedrock_agent_client.create_agent_alias(\n",
    "                    agentId=agent_id,\n",
    "                    agentAliasName=alias_name\n",
    "                    # routingConfiguration removed - defaults to latest prepared (DRAFT)\n",
    "                )\n",
    "                agent_alias_id_to_use = create_alias_response.get('agentAlias', {}).get('agentAliasId')\n",
    "                logger.info(f\"Successfully created alias '{alias_name}' with ID: {agent_alias_id_to_use}. (Defaults to latest prepared version - DRAFT)\")\n",
    "\n",
    "            if not agent_alias_id_to_use:\n",
    "                    raise ValueError(f\"Failed to get a valid alias ID for '{alias_name}'\")\n",
    "\n",
    "            logger.info(f\"Waiting 10s for alias '{alias_name}' changes to propagate...\")\n",
    "            time.sleep(10)\n",
    "            logger.info(f\"Agent {agent_id} preparation and alias '{alias_name}' ({agent_alias_id_to_use}) setup complete.\")\n",
    "\n",
    "\n",
    "        except Exception as alias_e: # Catch errors from alias logic\n",
    "            logger.error(f\"Failed to create/update alias '{alias_name}' for agent {agent_id}: {alias_e}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            raise # Use raise for notebook context\n",
    "        # End of Alias specific try/except\n",
    "    # End of if preparation_successful\n",
    "else:\n",
    "    logger.error(\"Agent ID not available, skipping preparation and alias setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Test Agent Invocation\n",
    "\n",
    "This is the final operational step where the fully configured Bedrock Agent is tested.\n",
    "- It first checks if both `agent_id` and `agent_alias_id_to_use` are available (i.e., the previous setup steps were successful).\n",
    "- A unique `session_id` is generated for this specific interaction.\n",
    "- A `test_prompt` is defined (e.g., \"Search for information about Project Chimera and format the results using bullet points.\"). This prompt is designed to trigger the agent's tool (`searchAndFormatDocuments`).\n",
    "- It then calls the `test_agent_invocation()` helper function (described in 3.11.1). This function sends the prompt to the Bedrock Agent Runtime using the specified agent ID and alias ID.\n",
    "- The `test_agent_invocation` function handles the streaming response from the agent, concatenates the text chunks, logs trace information for debugging, and prints the agent's final completion. \n",
    "This step demonstrates an end-to-end test of the agent: receiving a prompt, deciding to use its Lambda-backed tool, Bedrock invoking the Lambda, the Lambda executing (performing search and formatting), returning results to the agent, and the agent formulating a final response to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 16:37:16,782 - INFO - --- Invoking Agent YU7YWIVKVP using Alias 'prod' (QWIYXVWI9T) ---\n",
      "2025-05-08 16:37:16,783 - INFO - --- Testing Agent Invocation (Agent ID: YU7YWIVKVP, Alias: QWIYXVWI9T) ---\n",
      "2025-05-08 16:37:16,783 - INFO - Session ID: d8f5ebc7-1066-4a02-96b5-9170c506d41c\n",
      "2025-05-08 16:37:16,783 - INFO - Prompt: \"Search for information about Project Chimera and format the results using bullet points.\"\n",
      "2025-05-08 16:37:17,720 - INFO - Agent invocation successful. Processing response...\n",
      "2025-05-08 16:37:28,666 - INFO - --- Agent Final Response ---• Project Chimera combines quantum entanglement communication with neural networks for secure, real-time data analysis across distributed nodes. Lead developer: Dr. Aris Thorne.\n",
      "\n",
      "• Chimera operates in two modes:\n",
      "    - 'Quantum Sync' for high-fidelity data transfer\n",
      "    - 'Neural Inference' for localized edge processing based on the synced data.\n",
      "\n",
      "• A key aspect of Chimera is its \"Ephemeral Key Protocol\" (EKP), which generates one-time quantum keys for each transmission, ensuring absolute forward secrecy.\n",
      "2025-05-08 16:37:28,666 - INFO - --- Invocation Trace Summary ---\n",
      "2025-05-08 16:37:28,666 - INFO - Trace 1: Type=None, Step=None\n",
      "2025-05-08 16:37:28,667 - INFO - Trace 2: Type=None, Step=None\n",
      "2025-05-08 16:37:28,667 - INFO - Trace 3: Type=None, Step=None\n",
      "2025-05-08 16:37:28,667 - INFO - Trace 4: Type=None, Step=None\n",
      "2025-05-08 16:37:28,668 - INFO - Trace 5: Type=None, Step=None\n",
      "2025-05-08 16:37:28,668 - INFO - Trace 6: Type=None, Step=None\n",
      "2025-05-08 16:37:28,669 - INFO - Trace 7: Type=None, Step=None\n",
      "2025-05-08 16:37:28,669 - INFO - Trace 8: Type=None, Step=None\n"
     ]
    }
   ],
   "source": [
    "# --- Test Invocation ---\n",
    "# Agent ID and custom alias ID should be valid here\n",
    "if agent_id and agent_alias_id_to_use: # Check both are set\n",
    "     session_id = str(uuid.uuid4()) \n",
    "     test_prompt = \"Search for information about Project Chimera and format the results using bullet points.\"\n",
    "     logger.info(f\"--- Invoking Agent {agent_id} using Alias '{alias_name}' ({agent_alias_id_to_use}) ---\") # Updated log\n",
    "     try:\n",
    "         completion = test_agent_invocation(\n",
    "             agent_runtime_client=bedrock_agent_runtime_client,\n",
    "             agent_id=agent_id,\n",
    "             agent_alias_id=agent_alias_id_to_use, \n",
    "             session_id=session_id,\n",
    "             prompt=test_prompt\n",
    "         )\n",
    "         if completion is None:\n",
    "              logger.error(\"Agent invocation failed.\")\n",
    "         # The function already logs the final response\n",
    "     except Exception as e:\n",
    "          logger.error(f\"Error during test invocation: {e}\")\n",
    "          logger.error(traceback.format_exc())\n",
    "else:\n",
    "     logger.error(\"Agent ID or Alias ID not available, skipping invocation test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the Lambda approach for implementing AWS Bedrock agents with Couchbase Vector Search. This approach allows the agent to invoke AWS Lambda functions to execute operations, providing better scalability and separation of concerns.\n",
    "\n",
    "Key components of this implementation include:\n",
    "\n",
    "1. **Vector Store Setup**: We set up a Couchbase vector store to store and search documents using semantic similarity.\n",
    "2. **Lambda Function Deployment**: We deployed Lambda functions that handle the agent's function calls.\n",
    "3. **Agent Creation**: We created two specialized agents - a researcher agent for searching documents and a writer agent for formatting results.\n",
    "4. **Lambda Integration**: We integrated the agents with Lambda functions, allowing them to execute operations in a serverless environment.\n",
    "\n",
    "This approach is particularly useful for production environments where scalability and separation of concerns are important. The Lambda functions can be deployed independently and can access other AWS services, providing more flexibility and power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
