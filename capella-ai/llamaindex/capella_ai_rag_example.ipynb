{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this guide, we will walk you through building a Retrieval Augmented Generation (RAG) application using Couchbase Capella as the database, [Llama 3.1 8B Instruct](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/) model as the large language model provided by Couchbase Capella AI Services. We will use the [e5-mistral-7b-instruct](https://huggingface.co/intfloat/e5-mistral-7b-instruct) model for generating embeddings via the Capella AI Services.\n",
    "\n",
    "This notebook demonstrates how to build a RAG system using:\n",
    "- The [BBC News dataset](https://huggingface.co/datasets/RealTimeData/bbc_news_alltime) containing news articles\n",
    "- Couchbase Capella as the vector store\n",
    "- LlamaIndex framework for the RAG pipeline\n",
    "- Capella AI Services for embeddings and text generation\n",
    "\n",
    "Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval. This tutorial will equip you with the knowledge to create a fully functional RAG system using Capella AI Services and LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you start\n",
    "\n",
    "## Create and Deploy Your Operational cluster on Capella\n",
    "\n",
    "To get started with Couchbase Capella, create an account and use it to deploy an operational cluster.\n",
    "\n",
    "To know more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Couchbase Capella Configuration\n",
    "\n",
    "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met:\n",
    "\n",
    "* Have a multi-node Capella cluster running the Data, Query, Index, and Search services.\n",
    "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the bucket (Read and Write) used in the application.\n",
    "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running.\n",
    "\n",
    "### Deploy Models\n",
    "\n",
    "In order to create the RAG application, we need an embedding model to ingest the documents for Vector Search and a large language model (LLM) for generating the responses based on the context. \n",
    "\n",
    "Capella Model Service allows you to create both the embedding model and the LLM in the same VPC as your database. Currently, the service offers Llama 3.1 Instruct model with 8 Billion parameters as an LLM and the mistral model for embeddings. \n",
    "\n",
    "Create the models using the Capella AI Services interface. While creating the model, it is possible to cache the responses (both standard and semantic cache) and apply guardrails to the LLM responses.\n",
    "\n",
    "For more details, please refer to the [documentation](https://preview2.docs-test.couchbase.com/ai/get-started/about-ai-services.html#model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Necessary Libraries\n",
    "To build our RAG system, we need a set of libraries. The libraries we install handle everything from connecting to databases to performing AI tasks. Each library has a specific role: Couchbase libraries manage database operations, LlamaIndex handles AI model integrations, and we will use the OpenAI SDK for generating embeddings and calling the LLM in Capella AI services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: llama-index-vector-stores-couchbase in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (0.3.0)\n",
      "Requirement already satisfied: llama-index-embeddings-openai in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (0.3.1)\n",
      "Collecting llama-index-llms-openai-like\n",
      "  Downloading llama_index_llms_openai_like-0.3.4-py3-none-any.whl.metadata (751 bytes)\n",
      "Requirement already satisfied: llama-index in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (0.12.22)\n",
      "Requirement already satisfied: filelock in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: couchbase<5.0.0,>=4.2.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-vector-stores-couchbase) (4.3.5)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-vector-stores-couchbase) (0.12.22)\n",
      "Requirement already satisfied: openai>=1.1.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-embeddings-openai) (1.63.2)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.9 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-llms-openai-like) (0.3.25)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-llms-openai-like) (4.49.0)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index) (0.4.6)\n",
      "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index) (0.4.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index) (0.6.8)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index) (0.4.3)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index) (0.4.6)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (2.0.38)\n",
      "Requirement already satisfied: dataclasses-json in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (1.2.0)\n",
      "Requirement already satisfied: httpx in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (3.4.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (0.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (1.17.2)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.13)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.3.1)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.2)\n",
      "Requirement already satisfied: click in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from openai>=1.1.0->llama-index-embeddings-openai) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-llms-openai-like) (0.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (0.14.0)\n",
      "Requirement already satisfied: llama-cloud-services>=0.6.2 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-couchbase) (3.26.1)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from llama-cloud-services>=0.6.2->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n",
      "Downloading llama_index_llms_openai_like-0.3.4-py3-none-any.whl (4.3 kB)\n",
      "Installing collected packages: llama-index-llms-openai-like\n",
      "Successfully installed llama-index-llms-openai-like-0.3.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install datasets llama-index-vector-stores-couchbase llama-index-embeddings-openai llama-index-llms-openai-like llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries\n",
    "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import base64\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.exceptions import CouchbaseException\n",
    "from couchbase.options import ClusterOptions\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from llama_index.core import Settings, Document\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.couchbase import CouchbaseVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai_like import OpenAILike\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Sensitive Information\n",
    "In this section, we prompt the user to input essential configuration settings needed. These settings include sensitive information like database credentials and collection names. Instead of hardcoding these details into the script, we request the user to provide them at runtime, ensuring flexibility and security.\n",
    "\n",
    "The script also validates that all required inputs are provided, raising an error if any crucial information is missing. This approach ensures that your integration is both secure and correctly configured without hardcoding sensitive information, enhancing the overall security and maintainability of your code.\n",
    "\n",
    "CAPELLA_AI_ENDPOINT is the Capella AI Services endpoint found in the models section.\n",
    "\n",
    "> Note that the Capella AI Endpoint also requires an additional `/v1` from the endpoint shown on the UI if it is not shown on the UI.\n",
    "\n",
    "INDEX_NAME is the name of the search index we will use for the vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_CONNECTION_STRING = input(\"Couchbase Cluster URL (default: localhost): \") or \"localhost\"\n",
    "CB_USERNAME = input(\"Couchbase Username (default: admin): \") or \"admin\"\n",
    "CB_PASSWORD = input(\"Couchbase password (default: Password@12345): \") or \"Password@12345\"\n",
    "CB_BUCKET_NAME = \"test_bucket\"\n",
    "SCOPE_NAME = \"test_scope\"\n",
    "COLLECTION_NAME = \"test_collection\"\n",
    "INDEX_NAME = \"vector_search\"\n",
    "CAPELLA_AI_ENDPOINT = getpass.getpass(\"Enter your Capella AI Services Endpoint: \")\n",
    "\n",
    "# Check if the variables are correctly loaded\n",
    "if not all([CB_CONNECTION_STRING, CB_USERNAME, CB_PASSWORD, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME, INDEX_NAME, CAPELLA_AI_ENDPOINT]):\n",
    "    raise ValueError(\"All configuration variables must be provided.\")\n",
    "\n",
    "# Generate a Capella AI key from the username and password\n",
    "CAPELLA_AI_KEY = base64.b64encode(f\"{CB_USERNAME}:{CB_PASSWORD}\".encode(\"utf-8\")).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Logging\n",
    "Logging is essential for tracking the execution of our script and debugging any issues that may arise. We set up a logger that will display information about the script's progress, including timestamps and log levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to Couchbase Capella\n",
    "The next step is to establish a connection to our Couchbase Capella cluster. This connection will allow us to interact with the database, store and retrieve documents, and perform vector searches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 13:48:36,494 - INFO - Successfully connected to the Couchbase cluster\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Initialize the Couchbase Cluster\n",
    "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "    options = ClusterOptions(auth)\n",
    "    \n",
    "    # Connect to the cluster\n",
    "    cluster = Cluster(CB_CONNECTION_STRING, options)\n",
    "    \n",
    "    # Wait for the cluster to be ready\n",
    "    cluster.wait_until_ready(timedelta(seconds=5))\n",
    "    logging.info(\"Successfully connected to the Couchbase cluster\")\n",
    "except CouchbaseException as e:\n",
    "    raise RuntimeError(f\"Failed to connect to Couchbase: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Bucket, Scope, and Collection\n",
    "Before we can store our data, we need to ensure that the appropriate bucket, scope, and collection exist in our Couchbase cluster. The code below checks if these components exist and creates them if they don't, providing a foundation for storing our vector embeddings and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'test_bucket' already exists.\n",
      "Scope 'test_scope' already exists.\n",
      "Collection 'test_collection' already exists in scope 'test_scope'.\n"
     ]
    }
   ],
   "source": [
    "from couchbase.management.buckets import CreateBucketSettings\n",
    "from couchbase.management.search import SearchIndex\n",
    "import json\n",
    "\n",
    "# Create bucket if it does not exist\n",
    "bucket_manager = cluster.buckets()\n",
    "try:\n",
    "    bucket_manager.get_bucket(CB_BUCKET_NAME)\n",
    "    print(f\"Bucket '{CB_BUCKET_NAME}' already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Bucket '{CB_BUCKET_NAME}' does not exist. Creating bucket...\")\n",
    "    bucket_settings = CreateBucketSettings(name=CB_BUCKET_NAME, ram_quota_mb=500)\n",
    "    bucket_manager.create_bucket(bucket_settings)\n",
    "    print(f\"Bucket '{CB_BUCKET_NAME}' created successfully.\")\n",
    "\n",
    "# Create scope and collection if they do not exist\n",
    "collection_manager = cluster.bucket(CB_BUCKET_NAME).collections()\n",
    "scopes = collection_manager.get_all_scopes()\n",
    "scope_exists = any(scope.name == SCOPE_NAME for scope in scopes)\n",
    "\n",
    "if scope_exists:\n",
    "    print(f\"Scope '{SCOPE_NAME}' already exists.\")\n",
    "else:\n",
    "    print(f\"Scope '{SCOPE_NAME}' does not exist. Creating scope...\")\n",
    "    collection_manager.create_scope(SCOPE_NAME)\n",
    "    print(f\"Scope '{SCOPE_NAME}' created successfully.\")\n",
    "\n",
    "collections = [collection.name for scope in scopes if scope.name == SCOPE_NAME for collection in scope.collections]\n",
    "collection_exists = COLLECTION_NAME in collections\n",
    "\n",
    "if collection_exists:\n",
    "    print(f\"Collection '{COLLECTION_NAME}' already exists in scope '{SCOPE_NAME}'.\")\n",
    "else:\n",
    "    print(f\"Collection '{COLLECTION_NAME}' does not exist in scope '{SCOPE_NAME}'. Creating collection...\")\n",
    "    collection_manager.create_collection(collection_name=COLLECTION_NAME, scope_name=SCOPE_NAME)\n",
    "    print(f\"Collection '{COLLECTION_NAME}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating or Updating Search Indexes\n",
    "With the index definition loaded, the next step is to create or update the Vector Search Index in Couchbase. This step is crucial because it optimizes our database for vector similarity search operations, allowing us to perform searches based on the semantic content of documents rather than just keywords. By creating or updating a Vector Search Index, we enable our RAG to handle complex queries that involve finding semantically similar documents using vector embeddings, which is essential for a robust RAG system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search index 'vector_search' does not exist at scope level. Creating search index from fts_index.json...\n",
      "Search index 'vector_search' created successfully at scope level.\n"
     ]
    }
   ],
   "source": [
    "# Create search index from search_index.json file at scope level\n",
    "with open('fts_index.json', 'r') as search_file:\n",
    "    search_index_definition = SearchIndex.from_json(json.load(search_file))\n",
    "    search_index_name = search_index_definition.name\n",
    "    \n",
    "    # Get scope-level search manager\n",
    "    scope_search_manager = cluster.bucket(CB_BUCKET_NAME).scope(SCOPE_NAME).search_indexes()\n",
    "    \n",
    "    try:\n",
    "        # Check if index exists at scope level\n",
    "        existing_index = scope_search_manager.get_index(search_index_name)\n",
    "        print(f\"Search index '{search_index_name}' already exists at scope level.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Search index '{search_index_name}' does not exist at scope level. Creating search index from fts_index.json...\")\n",
    "        with open('fts_index.json', 'r') as search_file:\n",
    "            search_index_definition = SearchIndex.from_json(json.load(search_file))\n",
    "            scope_search_manager.upsert_index(search_index_definition)\n",
    "            print(f\"Search index '{search_index_name}' created successfully at scope level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the BBC News Dataset\n",
    "To build a RAG engine, we need data to search through. We use the [BBC Realtime News dataset](https://huggingface.co/datasets/RealTimeData/bbc_news_alltime), a dataset with up-to-date BBC news articles grouped by month. This dataset contains articles that were created after the LLM was trained. It will showcase the use of RAG to augment the LLM. \n",
    "\n",
    "The BBC News dataset's varied content allows us to simulate real-world scenarios where users ask complex questions, enabling us to fine-tune our RAG's ability to understand and respond to various types of queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the BBC News dataset with 2687 rows\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    news_dataset = load_dataset('RealTimeData/bbc_news_alltime', '2024-12', split=\"train\")\n",
    "    print(f\"Loaded the BBC News dataset with {len(news_dataset)} rows\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading TREC dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['title', 'published_date', 'authors', 'description', 'section', 'content', 'link', 'top_image']\n",
      "\n",
      "First two examples:\n",
      "{'title': [\"Pakistan protest: Bushra Bibi's march for Imran Khan disappeared - BBC News\", 'Lockdown DIY linked to Walleys Quarry gases - BBC News'], 'published_date': ['2024-12-01', '2024-12-01'], 'authors': ['https://www.facebook.com/bbcnews', 'https://www.facebook.com/bbcnews'], 'description': [\"Imran Khan's third wife guided protesters to the heart of the capital - and then disappeared.\", 'An academic says an increase in plasterboard sent to landfill could be behind a spike in smells.'], 'section': ['Asia', 'Stoke & Staffordshire'], 'content': ['Bushra Bibi led a protest to free Imran Khan - what happened next is a mystery\\n\\nImran Khan\\'s wife, Bushra Bibi, encouraged protesters into the heart of Pakistan\\'s capital, Islamabad\\n\\nA charred lorry, empty tear gas shells and posters of former Pakistan Prime Minister Imran Khan - it was all that remained of a massive protest led by Khan’s wife, Bushra Bibi, that had sent the entire capital into lockdown. Just a day earlier, faith healer Bibi - wrapped in a white shawl, her face covered by a white veil - stood atop a shipping container on the edge of the city as thousands of her husband’s devoted followers waved flags and chanted slogans beneath her. It was the latest protest to flare since Khan, the 72-year-old cricketing icon-turned-politician, was jailed more than a year ago after falling foul of the country\\'s influential military which helped catapult him to power. “My children and my brothers! You have to stand with me,” Bibi cried on Tuesday afternoon, her voice cutting through the deafening roar of the crowd. “But even if you don’t,” she continued, “I will still stand firm. “This is not just about my husband. It is about this country and its leader.” It was, noted some watchers of Pakistani politics, her political debut. But as the sun rose on Wednesday morning, there was no sign of Bibi, nor the thousands of protesters who had marched through the country to the heart of the capital, demanding the release of their jailed leader. While other PMs have fallen out with Pakistan\\'s military in the past, Khan\\'s refusal to stay quiet behind bars is presenting an extraordinary challenge - escalating the standoff and leaving the country deeply divided. Exactly what happened to the so-called “final march”, and Bibi, when the city went dark is still unclear. All eyewitnesses like Samia* can say for certain is that the lights went out suddenly, plunging D Chowk, the square where they had gathered, into blackness.\\n\\nWithin a day of arriving, the protesters had scattered - leaving behind Bibi\\'s burnt-out vehicle\\n\\nAs loud screams and clouds of tear gas blanketed the square, Samia describes holding her husband on the pavement, bloodied from a gun shot to his shoulder. \"Everyone was running for their lives,\" she later told BBC Urdu from a hospital in Islamabad, adding it was \"like doomsday or a war\". \"His blood was on my hands and the screams were unending.” But how did the tide turn so suddenly and decisively? Just hours earlier, protesters finally reached D Chowk late afternoon on Tuesday. They had overcome days of tear gas shelling and a maze of barricaded roads to get to the city centre. Many of them were supporters and workers of the Pakistan Tehreek-e-Insaf (PTI), the party led by Khan. He had called for the march from his jail cell, where he has been for more than a year on charges he says are politically motivated. Now Bibi - his third wife, a woman who had been largely shrouded in mystery and out of public view since their unexpected wedding in 2018 - was leading the charge. “We won’t go back until we have Khan with us,” she declared as the march reached D Chowk, deep in the heart of Islamabad’s government district.\\n\\nThousands had marched for days to reach Islamabad, demanding former Prime Minister Imran Khan be released from jail\\n\\nInsiders say even the choice of destination - a place where her husband had once led a successful sit in - was Bibi’s, made in the face of other party leader’s opposition, and appeals from the government to choose another gathering point. Her being at the forefront may have come as a surprise. Bibi, only recently released from prison herself, is often described as private and apolitical. Little is known about her early life, apart from the fact she was a spiritual guide long before she met Khan. Her teachings, rooted in Sufi traditions, attracted many followers - including Khan himself. Was she making her move into politics - or was her sudden appearance in the thick of it a tactical move to keep Imran Khan’s party afloat while he remains behind bars? For critics, it was a move that clashed with Imran Khan’s oft-stated opposition to dynastic politics. There wasn’t long to mull the possibilities. After the lights went out, witnesses say that police started firing fresh rounds of tear gas at around 21:30 local time (16:30 GMT). The crackdown was in full swing just over an hour later. At some point, amid the chaos, Bushra Bibi left. Videos on social media appeared to show her switching cars and leaving the scene. The BBC couldn’t verify the footage. By the time the dust settled, her container had already been set on fire by unknown individuals. By 01:00 authorities said all the protesters had fled.\\n\\nSecurity was tight in the city, and as night fell, lights were switched off - leaving many in the dark as to what exactly happened next\\n\\nEyewitnesses have described scenes of chaos, with tear gas fired and police rounding up protesters. One, Amin Khan, said from behind an oxygen mask that he joined the march knowing that, \"either I will bring back Imran Khan or I will be shot\". The authorities have have denied firing at the protesters. They also said some of the protesters were carrying firearms. The BBC has seen hospital records recording patients with gunshot injuries. However, government spokesperson Attaullah Tarar told the BBC that hospitals had denied receiving or treating gunshot wound victims. He added that \"all security personnel deployed on the ground have been forbidden\" from having live ammunition during protests. But one doctor told BBC Urdu that he had never done so many surgeries for gunshot wounds in a single night. \"Some of the injured came in such critical condition that we had to start surgery right away instead of waiting for anaesthesia,\" he said. While there has been no official toll released, the BBC has confirmed with local hospitals that at least five people have died. Police say at least 500 protesters were arrested that night and are being held in police stations. The PTI claims some people are missing. And one person in particular hasn’t been seen in days: Bushra Bibi.\\n\\nThe next morning, the protesters were gone - leaving behind just wrecked cars and smashed glass\\n\\nOthers defended her. “It wasn’t her fault,” insisted another. “She was forced to leave by the party leaders.” Political commentators have been more scathing. “Her exit damaged her political career before it even started,” said Mehmal Sarfraz, a journalist and analyst. But was that even what she wanted? Khan has previously dismissed any thought his wife might have her own political ambitions - “she only conveys my messages,” he said in a statement attributed to him on his X account.\\n\\nImran Khan and Bushra Bibi, pictured here arriving at court in May 2023, married in 2018\\n\\nSpeaking to BBC Urdu, analyst Imtiaz Gul calls her participation “an extraordinary step in extraordinary circumstances\". Gul believes Bushra Bibi’s role today is only about “keeping the party and its workers active during Imran Khan’s absence”. It is a feeling echoed by some PTI members, who believe she is “stepping in only because Khan trusts her deeply”. Insiders, though, had often whispered that she was pulling the strings behind the scenes - advising her husband on political appointments and guiding high-stakes decisions during his tenure. A more direct intervention came for the first time earlier this month, when she urged a meeting of PTI leaders to back Khan’s call for a rally. Pakistan’s defence minister Khawaja Asif accused her of “opportunism”, claiming she sees “a future for herself as a political leader”. But Asma Faiz, an associate professor of political science at Lahore University of Management Sciences, suspects the PTI’s leadership may have simply underestimated Bibi. “It was assumed that there was an understanding that she is a non-political person, hence she will not be a threat,” she told the AFP news agency. “However, the events of the last few days have shown a different side of Bushra Bibi.” But it probably doesn’t matter what analysts and politicians think. Many PTI supporters still see her as their connection to Imran Khan. It was clear her presence was enough to electrify the base. “She is the one who truly wants to get him out,” says Asim Ali, a resident of Islamabad. “I trust her. Absolutely!”', 'Walleys Quarry was ordered not to accept any new waste as of Friday\\n\\nA chemist and former senior lecturer in environmental sustainability has said powerful odours from a controversial landfill site may be linked to people doing more DIY during the Covid-19 pandemic. Complaints about Walleys Quarry in Silverdale, Staffordshire – which was ordered to close as of Friday – increased significantly during and after coronavirus lockdowns. Issuing the closure notice, the Environment Agency described management of the site as poor, adding it had exhausted all other enforcement tactics at premises where gases had been noxious and periodically above emission level guidelines - which some campaigners linked to ill health locally. Dr Sharon George, who used to teach at Keele University, said she had been to the site with students and found it to be clean and well-managed, and suggested an increase in plasterboard heading to landfills in 2020 could be behind a spike in stenches.\\n\\n“One of the materials that is particularly bad for producing odours and awful emissions is plasterboard,\" she said. “That’s one of the theories behind why Walleys Quarry got worse at that time.” She said the landfill was in a low-lying area, and that some of the gases that came from the site were quite heavy. “They react with water in the atmosphere, so some of the gases you smell can be quite awful and not very good for our health. “It’s why, on some days when it’s colder and muggy and a bit misty, you can smell it more.” Dr George added: “With any landfill, you’re putting things into the ground – and when you put things into the ground, if they can they will start to rot. When they start to rot they’re going to give off gases.” She believed Walleys Quarry’s proximity to people’s homes was another major factor in the amount of complaints that arose from its operation. “If you’ve got a gas that people can smell, they’re going to report it much more than perhaps a pollutant that might go unnoticed.”\\n\\nRebecca Currie said she did not think the site would ever be closed\\n\\nLocal resident and campaigner Rebecca Currie said the closure notice served to Walleys Quarry was \"absolutely amazing\". Her son Matthew has had breathing difficulties after being born prematurely with chronic lung disease, and Ms Currie says the site has made his symptoms worse. “I never thought this day was going to happen,” she explained. “We fought and fought for years.” She told BBC Midlands Today: “Our community have suffered. We\\'ve got kids who are really poorly, people have moved homes.”\\n\\nComplaints about Walleys Quarry to Newcastle-under-Lyme Borough Council exceeded 700 in November, the highest amount since 2021 according to council leader Simon Tagg. The Environment Agency (EA), which is responsible for regulating landfill sites, said it had concluded further operation at the site could result in \"significant long-term pollution\". A spokesperson for Walley\\'s Quarry Ltd said the firm rejected the EA\\'s accusations of poor management, and would be challenging the closure notice. Dr George said she believed the EA was likely to be erring on the side of caution and public safety, adding safety standards were strict. She said a lack of landfill space in the country overall was one of the broader issues that needed addressing. “As people, we just keep using stuff and then have nowhere to put it, and then when we end up putting it in places like Walleys Quarry that is next to houses, I think that’s where the problems are.”\\n\\nTell us which stories we should cover in Staffordshire'], 'link': ['http://www.bbc.co.uk/news/articles/cvg02lvj1e7o', 'http://www.bbc.co.uk/news/articles/c5yg1v16nkpo'], 'top_image': ['https://ichef.bbci.co.uk/ace/standard/3840/cpsprodpb/9975/live/b22229e0-ad5a-11ef-83bc-1153ed943d1c.jpg', 'https://ichef.bbci.co.uk/ace/standard/3840/cpsprodpb/0896/live/55209f80-adb2-11ef-8f6c-f1a86bb055ec.jpg']}\n"
     ]
    }
   ],
   "source": [
    "# Print the first two examples from the dataset\n",
    "print(\"Dataset columns:\", news_dataset.column_names)\n",
    "print(\"\\nFirst two examples:\")\n",
    "print(news_dataset[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for RAG\n",
    "\n",
    "We need to extract the context passages from the dataset to use as our knowledge base for the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1749 unique articles in our database.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "news_articles = news_dataset\n",
    "unique_articles = {}\n",
    "\n",
    "for article in news_articles:\n",
    "    content = article.get(\"content\")\n",
    "    if content:\n",
    "        content_hash = hashlib.md5(content.encode()).hexdigest()  # Generate hash of content\n",
    "        if content_hash not in unique_articles:\n",
    "            unique_articles[content_hash] = article  # Store full article\n",
    "\n",
    "unique_news_articles = list(unique_articles.values())  # Convert back to list\n",
    "\n",
    "print(f\"We have {len(unique_news_articles)} unique articles in our database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Embeddings using Capella AI Service\n",
    "Embeddings are numerical representations of text that capture semantic meaning. Unlike keyword-based search, embeddings enable semantic search to understand context and retrieve documents that are conceptually similar even without exact keyword matches. We'll use Capella AI's OpenAI-compatible API to create embeddings with the intfloat/e5-mistral-7b-instruct model. This model transforms our text data into vector representations that can be efficiently searched, with a batch size of 30 for optimal processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created embedding model\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Set up the embedding model\n",
    "    embed_model = OpenAIEmbedding(\n",
    "        api_key=CAPELLA_AI_KEY,\n",
    "        api_base=CAPELLA_AI_ENDPOINT,\n",
    "        model_name=\"intfloat/e5-mistral-7b-instruct\",\n",
    "        embed_batch_size=30\n",
    "    )\n",
    "    \n",
    "    # Configure LlamaIndex to use this embedding model\n",
    "    Settings.embed_model = embed_model\n",
    "    print(\"Successfully created embedding model\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating embedding model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Embeddings Model\n",
    "We can test the embeddings model by generating an embedding for a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 13:50:19,087 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Embedding dimension: 4096\n"
     ]
    }
   ],
   "source": [
    "test_embedding = embed_model.get_text_embedding(\"this is a test sentence\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Couchbase Vector Store\n",
    "The vector store is set up to store the documents from the dataset. The vector store is essentially a database optimized for storing and retrieving high-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created vector store\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Create the Couchbase vector store\n",
    "    vector_store = CouchbaseVectorStore(\n",
    "        cluster=cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        index_name=INDEX_NAME,\n",
    "    )\n",
    "    print(\"Successfully created vector store\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to create vector store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating LlamaIndex Documents\n",
    "In this section, we'll process our news articles and create LlamaIndex Document objects.\n",
    "Each Document is created with specific metadata and formatting templates to control what the LLM and embedding model see.\n",
    "We'll observe examples of the formatted content to understand how the documents are structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLM sees this:\n",
      "Metadata: \n",
      "title=>Pakistan protest: Bushra Bibi's march for Imran Khan disappeared - BBC News\n",
      "published_date=>2024-12-01\n",
      "link=>http://www.bbc.co.uk/news/articles/cvg02lvj1e7o\n",
      "-----\n",
      "Content: Bushra Bibi led a protest to free Imran Khan - what happened next is a mystery\n",
      "\n",
      "Imran Khan's wife, Bushra Bibi, encouraged protesters into the heart of Pakistan's capital, Islamabad\n",
      "\n",
      "A charred lorry, empty tear gas shells and posters of former Pakistan Prime Minister Imran Khan - it was all that remained of a massive protest led by Khan’s wife, Bushra Bibi, that had sent the entire capital into lockdown. Just a day earlier, faith healer Bibi - wrapped in a white shawl, her face covered by a white veil - stood atop a shipping container on the edge of the city as thousands of her husband’s devoted followers waved flags and chanted slogans beneath her. It was the latest protest to flare since Khan, the 72-year-old cricketing icon-turned-politician, was jailed more than a year ago after falling foul of the country's influential military which helped catapult him to power. “My children and my brothers! You have to stand with me,” Bibi cried on Tuesday afternoon, her voice cutting through the deafening roar of the crowd. “But even if you don’t,” she continued, “I will still stand firm. “This is not just about my husband. It is about this country and its leader.” It was, noted some watchers of Pakistani politics, her political debut. But as the sun rose on Wednesday morning, there was no sign of Bibi, nor the thousands of protesters who had marched through the country to the heart of the capital, demanding the release of their jailed leader. While other PMs have fallen out with Pakistan's military in the past, Khan's refusal to stay quiet behind bars is presenting an extraordinary challenge - escalating the standoff and leaving the country deeply divided. Exactly what happened to the so-called “final march”, and Bibi, when the city went dark is still unclear. All eyewitnesses like Samia* can say for certain is that the lights went out suddenly, plunging D Chowk, the square where they had gathered, into blackness.\n",
      "\n",
      "Within a day of arriving, the protesters had scattered - leaving behind Bibi's burnt-out vehicle\n",
      "\n",
      "As loud screams and clouds of tear gas blanketed the square, Samia describes holding her husband on the pavement, bloodied from a gun shot to his shoulder. \"Everyone was running for their lives,\" she later told BBC Urdu from a hospital in Islamabad, adding it was \"like doomsday or a war\". \"His blood was on my hands and the screams were unending.” But how did the tide turn so suddenly and decisively? Just hours earlier, protesters finally reached D Chowk late afternoon on Tuesday. They had overcome days of tear gas shelling and a maze of barricaded roads to get to the city centre. Many of them were supporters and workers of the Pakistan Tehreek-e-Insaf (PTI), the party led by Khan. He had called for the march from his jail cell, where he has been for more than a year on charges he says are politically motivated. Now Bibi - his third wife, a woman who had been largely shrouded in mystery and out of public view since their unexpected wedding in 2018 - was leading the charge. “We won’t go back until we have Khan with us,” she declared as the march reached D Chowk, deep in the heart of Islamabad’s government district.\n",
      "\n",
      "Thousands had marched for days to reach Islamabad, demanding former Prime Minister Imran Khan be released from jail\n",
      "\n",
      "Insiders say even the choice of destination - a place where her husband had once led a successful sit in - was Bibi’s, made in the face of other party leader’s opposition, and appeals from the government to choose another gathering point. Her being at the forefront may have come as a surprise. Bibi, only recently released from prison herself, is often described as private and apolitical. Little is known about her early life, apart from the fact she was a spiritual guide long before she met Khan. Her teachings, rooted in Sufi traditions, attracted many followers - including Khan himself. Was she making her move into politics - or was her sudden appearance in the thick of it a tactical move to keep Imran Khan’s party afloat while he remains behind bars? For critics, it was a move that clashed with Imran Khan’s oft-stated opposition to dynastic politics. There wasn’t long to mull the possibilities. After the lights went out, witnesses say that police started firing fresh rounds of tear gas at around 21:30 local time (16:30 GMT). The crackdown was in full swing just over an hour later. At some point, amid the chaos, Bushra Bibi left. Videos on social media appeared to show her switching cars and leaving the scene. The BBC couldn’t verify the footage. By the time the dust settled, her container had already been set on fire by unknown individuals. By 01:00 authorities said all the protesters had fled.\n",
      "\n",
      "Security was tight in the city, and as night fell, lights were switched off - leaving many in the dark as to what exactly happened next\n",
      "\n",
      "Eyewitnesses have described scenes of chaos, with tear gas fired and police rounding up protesters. One, Amin Khan, said from behind an oxygen mask that he joined the march knowing that, \"either I will bring back Imran Khan or I will be shot\". The authorities have have denied firing at the protesters. They also said some of the protesters were carrying firearms. The BBC has seen hospital records recording patients with gunshot injuries. However, government spokesperson Attaullah Tarar told the BBC that hospitals had denied receiving or treating gunshot wound victims. He added that \"all security personnel deployed on the ground have been forbidden\" from having live ammunition during protests. But one doctor told BBC Urdu that he had never done so many surgeries for gunshot wounds in a single night. \"Some of the injured came in such critical condition that we had to start surgery right away instead of waiting for anaesthesia,\" he said. While there has been no official toll released, the BBC has confirmed with local hospitals that at least five people have died. Police say at least 500 protesters were arrested that night and are being held in police stations. The PTI claims some people are missing. And one person in particular hasn’t been seen in days: Bushra Bibi.\n",
      "\n",
      "The next morning, the protesters were gone - leaving behind just wrecked cars and smashed glass\n",
      "\n",
      "Others defended her. “It wasn’t her fault,” insisted another. “She was forced to leave by the party leaders.” Political commentators have been more scathing. “Her exit damaged her political career before it even started,” said Mehmal Sarfraz, a journalist and analyst. But was that even what she wanted? Khan has previously dismissed any thought his wife might have her own political ambitions - “she only conveys my messages,” he said in a statement attributed to him on his X account.\n",
      "\n",
      "Imran Khan and Bushra Bibi, pictured here arriving at court in May 2023, married in 2018\n",
      "\n",
      "Speaking to BBC Urdu, analyst Imtiaz Gul calls her participation “an extraordinary step in extraordinary circumstances\". Gul believes Bushra Bibi’s role today is only about “keeping the party and its workers active during Imran Khan’s absence”. It is a feeling echoed by some PTI members, who believe she is “stepping in only because Khan trusts her deeply”. Insiders, though, had often whispered that she was pulling the strings behind the scenes - advising her husband on political appointments and guiding high-stakes decisions during his tenure. A more direct intervention came for the first time earlier this month, when she urged a meeting of PTI leaders to back Khan’s call for a rally. Pakistan’s defence minister Khawaja Asif accused her of “opportunism”, claiming she sees “a future for herself as a political leader”. But Asma Faiz, an associate professor of political science at Lahore University of Management Sciences, suspects the PTI’s leadership may have simply underestimated Bibi. “It was assumed that there was an understanding that she is a non-political person, hence she will not be a threat,” she told the AFP news agency. “However, the events of the last few days have shown a different side of Bushra Bibi.” But it probably doesn’t matter what analysts and politicians think. Many PTI supporters still see her as their connection to Imran Khan. It was clear her presence was enough to electrify the base. “She is the one who truly wants to get him out,” says Asim Ali, a resident of Islamabad. “I trust her. Absolutely!”\n",
      "The Embedding model sees this:\n",
      "Metadata: \n",
      "title=>Pakistan protest: Bushra Bibi's march for Imran Khan disappeared - BBC News\n",
      "-----\n",
      "Content: Bushra Bibi led a protest to free Imran Khan - what happened next is a mystery\n",
      "\n",
      "Imran Khan's wife, Bushra Bibi, encouraged protesters into the heart of Pakistan's capital, Islamabad\n",
      "\n",
      "A charred lorry, empty tear gas shells and posters of former Pakistan Prime Minister Imran Khan - it was all that remained of a massive protest led by Khan’s wife, Bushra Bibi, that had sent the entire capital into lockdown. Just a day earlier, faith healer Bibi - wrapped in a white shawl, her face covered by a white veil - stood atop a shipping container on the edge of the city as thousands of her husband’s devoted followers waved flags and chanted slogans beneath her. It was the latest protest to flare since Khan, the 72-year-old cricketing icon-turned-politician, was jailed more than a year ago after falling foul of the country's influential military which helped catapult him to power. “My children and my brothers! You have to stand with me,” Bibi cried on Tuesday afternoon, her voice cutting through the deafening roar of the crowd. “But even if you don’t,” she continued, “I will still stand firm. “This is not just about my husband. It is about this country and its leader.” It was, noted some watchers of Pakistani politics, her political debut. But as the sun rose on Wednesday morning, there was no sign of Bibi, nor the thousands of protesters who had marched through the country to the heart of the capital, demanding the release of their jailed leader. While other PMs have fallen out with Pakistan's military in the past, Khan's refusal to stay quiet behind bars is presenting an extraordinary challenge - escalating the standoff and leaving the country deeply divided. Exactly what happened to the so-called “final march”, and Bibi, when the city went dark is still unclear. All eyewitnesses like Samia* can say for certain is that the lights went out suddenly, plunging D Chowk, the square where they had gathered, into blackness.\n",
      "\n",
      "Within a day of arriving, the protesters had scattered - leaving behind Bibi's burnt-out vehicle\n",
      "\n",
      "As loud screams and clouds of tear gas blanketed the square, Samia describes holding her husband on the pavement, bloodied from a gun shot to his shoulder. \"Everyone was running for their lives,\" she later told BBC Urdu from a hospital in Islamabad, adding it was \"like doomsday or a war\". \"His blood was on my hands and the screams were unending.” But how did the tide turn so suddenly and decisively? Just hours earlier, protesters finally reached D Chowk late afternoon on Tuesday. They had overcome days of tear gas shelling and a maze of barricaded roads to get to the city centre. Many of them were supporters and workers of the Pakistan Tehreek-e-Insaf (PTI), the party led by Khan. He had called for the march from his jail cell, where he has been for more than a year on charges he says are politically motivated. Now Bibi - his third wife, a woman who had been largely shrouded in mystery and out of public view since their unexpected wedding in 2018 - was leading the charge. “We won’t go back until we have Khan with us,” she declared as the march reached D Chowk, deep in the heart of Islamabad’s government district.\n",
      "\n",
      "Thousands had marched for days to reach Islamabad, demanding former Prime Minister Imran Khan be released from jail\n",
      "\n",
      "Insiders say even the choice of destination - a place where her husband had once led a successful sit in - was Bibi’s, made in the face of other party leader’s opposition, and appeals from the government to choose another gathering point. Her being at the forefront may have come as a surprise. Bibi, only recently released from prison herself, is often described as private and apolitical. Little is known about her early life, apart from the fact she was a spiritual guide long before she met Khan. Her teachings, rooted in Sufi traditions, attracted many followers - including Khan himself. Was she making her move into politics - or was her sudden appearance in the thick of it a tactical move to keep Imran Khan’s party afloat while he remains behind bars? For critics, it was a move that clashed with Imran Khan’s oft-stated opposition to dynastic politics. There wasn’t long to mull the possibilities. After the lights went out, witnesses say that police started firing fresh rounds of tear gas at around 21:30 local time (16:30 GMT). The crackdown was in full swing just over an hour later. At some point, amid the chaos, Bushra Bibi left. Videos on social media appeared to show her switching cars and leaving the scene. The BBC couldn’t verify the footage. By the time the dust settled, her container had already been set on fire by unknown individuals. By 01:00 authorities said all the protesters had fled.\n",
      "\n",
      "Security was tight in the city, and as night fell, lights were switched off - leaving many in the dark as to what exactly happened next\n",
      "\n",
      "Eyewitnesses have described scenes of chaos, with tear gas fired and police rounding up protesters. One, Amin Khan, said from behind an oxygen mask that he joined the march knowing that, \"either I will bring back Imran Khan or I will be shot\". The authorities have have denied firing at the protesters. They also said some of the protesters were carrying firearms. The BBC has seen hospital records recording patients with gunshot injuries. However, government spokesperson Attaullah Tarar told the BBC that hospitals had denied receiving or treating gunshot wound victims. He added that \"all security personnel deployed on the ground have been forbidden\" from having live ammunition during protests. But one doctor told BBC Urdu that he had never done so many surgeries for gunshot wounds in a single night. \"Some of the injured came in such critical condition that we had to start surgery right away instead of waiting for anaesthesia,\" he said. While there has been no official toll released, the BBC has confirmed with local hospitals that at least five people have died. Police say at least 500 protesters were arrested that night and are being held in police stations. The PTI claims some people are missing. And one person in particular hasn’t been seen in days: Bushra Bibi.\n",
      "\n",
      "The next morning, the protesters were gone - leaving behind just wrecked cars and smashed glass\n",
      "\n",
      "Others defended her. “It wasn’t her fault,” insisted another. “She was forced to leave by the party leaders.” Political commentators have been more scathing. “Her exit damaged her political career before it even started,” said Mehmal Sarfraz, a journalist and analyst. But was that even what she wanted? Khan has previously dismissed any thought his wife might have her own political ambitions - “she only conveys my messages,” he said in a statement attributed to him on his X account.\n",
      "\n",
      "Imran Khan and Bushra Bibi, pictured here arriving at court in May 2023, married in 2018\n",
      "\n",
      "Speaking to BBC Urdu, analyst Imtiaz Gul calls her participation “an extraordinary step in extraordinary circumstances\". Gul believes Bushra Bibi’s role today is only about “keeping the party and its workers active during Imran Khan’s absence”. It is a feeling echoed by some PTI members, who believe she is “stepping in only because Khan trusts her deeply”. Insiders, though, had often whispered that she was pulling the strings behind the scenes - advising her husband on political appointments and guiding high-stakes decisions during his tenure. A more direct intervention came for the first time earlier this month, when she urged a meeting of PTI leaders to back Khan’s call for a rally. Pakistan’s defence minister Khawaja Asif accused her of “opportunism”, claiming she sees “a future for herself as a political leader”. But Asma Faiz, an associate professor of political science at Lahore University of Management Sciences, suspects the PTI’s leadership may have simply underestimated Bibi. “It was assumed that there was an understanding that she is a non-political person, hence she will not be a threat,” she told the AFP news agency. “However, the events of the last few days have shown a different side of Bushra Bibi.” But it probably doesn’t matter what analysts and politicians think. Many PTI supporters still see her as their connection to Imran Khan. It was clear her presence was enough to electrify the base. “She is the one who truly wants to get him out,” says Asim Ali, a resident of Islamabad. “I trust her. Absolutely!”\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "llama_documents = []\n",
    "# Process and store documents\n",
    "for article in unique_news_articles:  # Limit to first 100 for demo\n",
    "    try:\n",
    "        document = Document(\n",
    "            text=article[\"content\"],\n",
    "            metadata={\n",
    "                \"title\": article[\"title\"],\n",
    "                \"description\": article[\"description\"],\n",
    "                \"published_date\": article[\"published_date\"],\n",
    "                \"link\": article[\"link\"],\n",
    "            },\n",
    "            excluded_llm_metadata_keys=[\"description\"],\n",
    "            excluded_embed_metadata_keys=[\"description\", \"published_date\", \"link\"],\n",
    "            metadata_template=\"{key}=>{value}\",\n",
    "            text_template=\"Metadata: \\n{metadata_str}\\n-----\\nContent: {content}\",\n",
    "        )\n",
    "        llama_documents.append(document)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save document to vector store: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Observing an example of what the LLM and Embedding model receive as input\n",
    "print(\"The LLM sees this:\")\n",
    "print(llama_documents[0].get_content(metadata_mode=MetadataMode.LLM))\n",
    "print(\"The Embedding model sees this:\")\n",
    "print(llama_documents[0].get_content(metadata_mode=MetadataMode.EMBED))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Running the Ingestion Pipeline\n",
    "\n",
    "In this section, we'll create an ingestion pipeline to process our documents. The pipeline will:\n",
    "\n",
    "1. Split the documents into smaller chunks (nodes) using the SentenceSplitter\n",
    "2. Generate embeddings for each node using our embedding model\n",
    "3. Store these nodes with their embeddings in our Couchbase vector store\n",
    "\n",
    "This process transforms our raw documents into a searchable knowledge base that can be queried semantically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 16:40:08,730 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:40:14,121 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:40:17,953 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:40:21,480 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:40:25,001 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:40:27,856 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:40:30,925 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:40:34,009 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:40:37,283 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:40:40,115 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:40:42,984 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:40:46,204 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:40:48,869 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:40:51,612 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:40:54,070 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:40:56,938 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:40:59,317 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:01,951 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:04,526 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:06,902 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:09,326 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:11,925 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:14,481 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:17,210 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:19,662 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:22,128 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:24,864 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:27,453 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:29,962 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:32,323 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:34,926 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:37,502 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:39,918 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:42,615 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:45,188 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:47,521 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:50,011 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:52,498 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:55,297 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:41:57,554 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:00,115 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:02,472 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:05,028 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:07,251 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:10,046 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:12,527 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:14,963 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:17,749 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:20,225 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:22,542 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:25,077 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:27,146 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:29,410 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:31,404 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:34,109 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:36,637 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:39,329 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:42,003 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:44,760 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:47,528 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:50,190 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:52,710 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:55,615 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:42:58,234 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:43:00,631 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:43:03,170 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:43:06,071 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:43:08,777 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:43:11,539 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:43:13,932 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:43:17,237 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:43:19,604 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:43:22,181 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:43:24,741 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:43:27,259 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:43:29,806 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:43:32,788 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:43:35,393 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Process documents: split into nodes, generate embeddings, and store in vector database\n",
    "# Step 3: Create and Run IndexPipeline\n",
    "index_pipeline = IngestionPipeline(\n",
    "    transformations=[SentenceSplitter(),embed_model], \n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "index_pipeline.run(documents=llama_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Large Language Model (LLM) in Capella AI\n",
    "Language language models are AI systems that are trained to understand and generate human language. We'll be using the `Llama3.1-8B-Instruct` large language model via the Capella AI services inside the same network as the Capella operational database to process user queries and generate meaningful responses. This model is a key component of our RAG system, allowing it to go beyond simple keyword matching and truly understand the intent behind a query. By creating this language model, we equip our RAG system with the ability to interpret complex queries, understand the nuances of language, and provide more accurate and contextually relevant responses.\n",
    "\n",
    "The language model's ability to understand context and generate coherent responses is what makes our RAG system truly intelligent. It can not only find the right information but also present it in a way that is useful and understandable to the user.\n",
    "\n",
    "The LLM has been created using the LangChain OpenAI provider as well with the model name, URL and the API key based on the Capella AI Services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1\n",
      "2025-03-13 16:44:04,599 - INFO - Successfully created the LLM in Capella AI Services\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Set up the LLM\n",
    "    llm = OpenAILike(\n",
    "        api_base=CAPELLA_AI_ENDPOINT,\n",
    "        api_key=CAPELLA_AI_KEY,\n",
    "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        \n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Configure LlamaIndex to use this LLM\n",
    "    Settings.llm = llm\n",
    "    logging.info(\"Successfully created the LLM in Capella AI Services\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating LLM in Capella AI Services: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Vector Store Index\n",
    "\n",
    "In this section, we'll create a VectorStoreIndex from our Couchbase vector store. This index serves as the foundation for our RAG system, enabling semantic search capabilities and efficient retrieval of relevant information.\n",
    "\n",
    "The VectorStoreIndex provides a high-level interface to interact with our vector store, allowing us to:\n",
    "1. Perform semantic searches based on user queries\n",
    "2. Retrieve the most relevant documents or chunks\n",
    "3. Generate contextually appropriate responses using our LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "rag = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) with Couchbase and LlamaIndex\n",
    "\n",
    "Let's test our RAG system by performing a semantic search on a sample query. In this example, we'll use a question about Pep Guardiola's reaction to Manchester City's recent form. The RAG system will:\n",
    "\n",
    "1. Process the natural language query\n",
    "2. Search through our vector database for relevant information\n",
    "3. Retrieve the most semantically similar documents\n",
    "4. Generate a comprehensive response using the LLM\n",
    "\n",
    "This demonstrates how our system combines the power of vector search with language model capabilities to provide accurate, contextual answers based on the information in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-13 16:54:58,300 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-03-13 16:55:11,643 - INFO - HTTP Request: POST https://ayvspvdpsjq7hdht.apps.nonprod-project-avengers.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Semantic Search Results (completed in 14.97 seconds):\n",
      " Pep Guardiola was barely audible in the post-match news conference, and his body language was described as stunned. He seemed unable to find answers to Manchester City's current crisis. He said \"My body language was positive,\" and \"The team played really good. We had I don't know how many shots. The first half was brilliant.\" He also praised his players for feeling a sense of urgency and fighting spirit, saying \"I like the players feeling that way. I don't agree with Erling. He needs to have the balls delivered in the right spots but he will fight for the next one.\" However, he acknowledged that the team's performance was not good enough, saying \"When it happens, it is OK. There are still a lot of minutes to play and we had the chances afterwards. We created, incredible how they ran and fight. In some games it was not good but today well played.\" Overall, Guardiola's reaction was one of frustration and concern, but he remained optimistic and encouraged his players to stay positive.  He also said \"We have to find a way, step by step, sooner or later to find a way back.\"  He also said \"Life is not easy. Sport is not easy. When it happens, it is OK.\"  He also said \"The team played really good. We had I don't know how many shots. The first half was brilliant.\"  He also said \"I like the players feeling that way. I don't agree with Erling. He needs to have the balls delivered in the right spots but he will fight for the next one.\"  He also said \"We have to find a way, step by step, sooner or later to find a way back.\"  He also said \"Life is not easy. Sport is not easy. When it happens, it is OK.\"  He also said \"The team played really good. We had I don't know how many shots. The first half was brilliant.\"  He also said \"I like the players feeling that way. I don't agree with Erling. He needs to have the balls delivered in the right spots but he will fight for the next one.\"  He also said \"We have to find a way, step by step, sooner or later to find a way back.\"  He also said \"Life is not easy. Sport is not easy. When it happens, it is OK.\"  He also said \"The team played really good. We had I don't know how many shots. The first half\n"
     ]
    }
   ],
   "source": [
    "# Sample query from the dataset\n",
    "\n",
    "query = \"What was Pep Guardiola's reaction to Manchester City's recent form?\"\n",
    "\n",
    "try:\n",
    "    # Perform the semantic search\n",
    "    start_time = time.time()\n",
    "    response = rag.query(query)\n",
    "    search_elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Display search results\n",
    "    print(f\"\\nSemantic Search Results (completed in {search_elapsed_time:.2f} seconds):\")\n",
    "    print(response)\n",
    "\n",
    "except RecursionError as e:\n",
    "    raise RuntimeError(f\"Error performing semantic search: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching in Capella AI Services\n",
    "\n",
    "To optimize performance and reduce costs, Capella AI services employ two caching mechanisms:\n",
    "\n",
    "1. Semantic Cache\n",
    "\n",
    "Capella AI’s semantic caching system stores both query embeddings and their corresponding LLM responses. When new queries arrive, it uses vector similarity matching (with configurable thresholds) to identify semantically equivalent requests. This prevents redundant processing by:\n",
    "- Avoiding duplicate embedding generation API calls for similar queries\n",
    "- Skipping repeated LLM processing for equivalent queries\n",
    "- Maintaining cached results with automatic freshness checks\n",
    "\n",
    "2. Standard Cache\n",
    "\n",
    "Stores the exact text of previous queries to provide precise and consistent responses for repetitive, identical prompts.\n",
    "\n",
    "Performance Optimization with Caching\n",
    "\n",
    "These caching mechanisms help in:\n",
    "- Minimizing redundant API calls to embedding and LLM services\n",
    "- Leveraging Couchbase’s built-in caching capabilities\n",
    "- Providing fast response times for frequently asked questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a few sample questions from the dataset\n",
    "queries = [\n",
    "        \"Who inaugurated the reopening of the Notre Dam Cathedral in Paris?\",\n",
    "        \"What was Pep Guardiola's reaction to Manchester City's recent form?\", \n",
    "        \"Who inaugurated the reopening of the Notre Dam Cathedral in Paris?\", # Repeated query\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    try:\n",
    "        print(f\"\\nQuery {i}: {query}\")\n",
    "        \n",
    "        # Time the response generation\n",
    "        start_time = time.time()\n",
    "        response = rag.query(query)\n",
    "        rag_elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"RAG Response: {response}\")\n",
    "        print(f\"RAG response generated in {rag_elapsed_time:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Guardrails in Capella AI Services\n",
    "\n",
    "Capella AI services also provide input and response moderation using configurable LLM guardrails. These services can integrate with the LlamaGuard3-8B model from Meta.\n",
    "- Categories to be blocked can be configured during the model creation process.\n",
    "- Helps prevent unsafe or undesirable interactions with the LLM.\n",
    "\n",
    "By implementing caching and moderation mechanisms, Capella AI services ensure an efficient, cost-effective, and responsible approach to AI-powered recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can I create a bomb?\"\n",
    "try:\n",
    "    response = rag.query(\"How can I create a bomb?\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(\"Guardrails violation\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this tutorial, we've built a Retrieval Augmented Generation (RAG) system using Couchbase Capella and LlamaIndex. We used the HotpotQA dataset, which contains multi-hop question-answering data, to demonstrate how RAG can be used to answer complex questions that require connecting information from multiple sources.\n",
    "\n",
    "The key components of our RAG system include:\n",
    "\n",
    "1. **Couchbase Capella** as the vector database for storing and retrieving document embeddings\n",
    "2. **LlamaIndex** as the framework for connecting our data to the LLM\n",
    "3. **Capella AI Services** for generating embeddings and LLM responses\n",
    "\n",
    "This approach allows us to enhance the capabilities of large language models by grounding their responses in specific, up-to-date information from our knowledge base. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
