{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Dataset RAG Pipeline with Couchbase\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval Augmented Generation (RAG) system using:\n",
    "- The TMDB movie dataset\n",
    "- Couchbase as the vector store\n",
    "- Haystack framework for the RAG pipeline\n",
    "- Capella AI for embeddings and text generation\n",
    "\n",
    "The system allows users to ask questions about movies and get AI-generated answers based on the movie descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Requirements\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Import all necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import base64\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from haystack import Pipeline, GeneratedAnswer\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.builders.answer_builder import AnswerBuilder\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "from haystack.dataclasses import Document\n",
    "\n",
    "from couchbase_haystack import (\n",
    "    CouchbaseSearchDocumentStore,\n",
    "    CouchbasePasswordAuthenticator,\n",
    "    CouchbaseClusterOptions,\n",
    "    CouchbaseSearchEmbeddingRetriever,\n",
    ")\n",
    "from couchbase.options import KnownConfigProfiles\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "## Create and Deploy Your Operational cluster on Capella\n",
    "\n",
    "To get started with Couchbase Capella, create an account and use it to deploy an operational cluster.\n",
    "\n",
    "To know more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html).\n",
    "\n",
    "\n",
    "### Couchbase Capella Configuration\n",
    "\n",
    "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met:\n",
    "\n",
    "* Have a multi-node Capella cluster running the Data, Query, Index, and Search services.\n",
    "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the travel-sample bucket (Read and Write) used in the application.\n",
    "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running.\n",
    "\n",
    "### Deploy Models\n",
    "\n",
    "To create the RAG application, use an embedding model for Vector Search and an LLM for generating responses. \n",
    " \n",
    "Capella Model Service lets you create both models in the same VPC as your database. It offers the Llama 3.1 Instruct model (8 Billion parameters) for LLM and the mistral model for embeddings. \n",
    "\n",
    "Use the Capella AI Services interface to create these models. You can cache responses and set guardrails for LLM outputs.\n",
    "\n",
    "For more details, see the [documentation](https://preview2.docs-test.couchbase.com/ai/get-started/about-ai-services.html#model). These models work with [Haystack OpenAI integration](https://haystack.deepset.ai/integrations/openai)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Couchbase Credentials\n",
    "\n",
    "Enter your Couchbase and Capella AI credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "# Get Couchbase credentials\n",
    "couchbase_cluster_url = input(\"Couchbase Cluster URL (default: localhost): \") or \"localhost\"\n",
    "couchbase_username = input(\"Couchbase Username (default: admin): \") or \"admin\"\n",
    "couchbase_password = getpass.getpass(\"Couchbase password (default: Password@12345): \") or \"Password@12345\"\n",
    "couchbase_bucket = input(\"Couchbase Bucket: \") \n",
    "couchbase_scope = input(\"Couchbase Scope: \")\n",
    "couchbase_collection = input(\"Couchbase Collection: \")\n",
    "vector_search_index = input(\"Vector Search Index: \")\n",
    "\n",
    "# Get Capella AI endpoint\n",
    "capella_ai_endpoint = input(\"Capella AI Services Endpoint\")\n",
    "capella_ai_endpoint_password = base64.b64encode(f\"{couchbase_username}:{couchbase_password}\".encode(\"utf-8\")).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'test_bucket' already exists.\n",
      "Scope 'test_scope' already exists.\n",
      "Collection 'test_collection' already exists in scope 'test_scope'.\n",
      "Search index 'vector_search' does not exist at scope level. Creating search index from fts_index.json...\n",
      "Search index 'vector_search' created successfully at scope level.\n"
     ]
    }
   ],
   "source": [
    "from couchbase.cluster import Cluster \n",
    "from couchbase.options import ClusterOptions\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.management.buckets import CreateBucketSettings\n",
    "from couchbase.management.collections import CollectionSpec\n",
    "from couchbase.management.search import SearchIndex\n",
    "import json\n",
    "\n",
    "# Connect to Couchbase cluster\n",
    "cluster = Cluster(couchbase_cluster_url, ClusterOptions(\n",
    "    PasswordAuthenticator(couchbase_username, couchbase_password)))\n",
    "\n",
    "# Create bucket if it does not exist\n",
    "bucket_manager = cluster.buckets()\n",
    "try:\n",
    "    bucket_manager.get_bucket(couchbase_bucket)\n",
    "    print(f\"Bucket '{couchbase_bucket}' already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Bucket '{couchbase_bucket}' does not exist. Creating bucket...\")\n",
    "    bucket_settings = CreateBucketSettings(name=couchbase_bucket, ram_quota_mb=500)\n",
    "    bucket_manager.create_bucket(bucket_settings)\n",
    "    print(f\"Bucket '{couchbase_bucket}' created successfully.\")\n",
    "\n",
    "# Create scope and collection if they do not exist\n",
    "collection_manager = cluster.bucket(couchbase_bucket).collections()\n",
    "scopes = collection_manager.get_all_scopes()\n",
    "scope_exists = any(scope.name == couchbase_scope for scope in scopes)\n",
    "\n",
    "if scope_exists:\n",
    "    print(f\"Scope '{couchbase_scope}' already exists.\")\n",
    "else:\n",
    "    print(f\"Scope '{couchbase_scope}' does not exist. Creating scope...\")\n",
    "    collection_manager.create_scope(couchbase_scope)\n",
    "    print(f\"Scope '{couchbase_scope}' created successfully.\")\n",
    "\n",
    "collections = [collection.name for scope in scopes if scope.name == couchbase_scope for collection in scope.collections]\n",
    "collection_exists = couchbase_collection in collections\n",
    "\n",
    "if collection_exists:\n",
    "    print(f\"Collection '{couchbase_collection}' already exists in scope '{couchbase_scope}'.\")\n",
    "else:\n",
    "    print(f\"Collection '{couchbase_collection}' does not exist in scope '{couchbase_scope}'. Creating collection...\")\n",
    "    collection_manager.create_collection(collection_name=couchbase_collection, scope_name=couchbase_scope)\n",
    "    print(f\"Collection '{couchbase_collection}' created successfully.\")\n",
    "\n",
    "# Create search index from search_index.json file at scope level\n",
    "with open('fts_index.json', 'r') as search_file:\n",
    "    search_index_definition = SearchIndex.from_json(json.load(search_file))\n",
    "    search_index_name = search_index_definition.name\n",
    "    \n",
    "    # Get scope-level search manager\n",
    "    scope_search_manager = cluster.bucket(couchbase_bucket).scope(couchbase_scope).search_indexes()\n",
    "    \n",
    "    try:\n",
    "        # Check if index exists at scope level\n",
    "        existing_index = scope_search_manager.get_index(search_index_name)\n",
    "        print(f\"Search index '{search_index_name}' already exists at scope level.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Search index '{search_index_name}' does not exist at scope level. Creating search index from fts_index.json...\")\n",
    "        with open('fts_index.json', 'r') as search_file:\n",
    "            search_index_definition = SearchIndex.from_json(json.load(search_file))\n",
    "            scope_search_manager.upsert_index(search_index_definition)\n",
    "            print(f\"Search index '{search_index_name}' created successfully at scope level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Process Movie Dataset\n",
    "\n",
    "Load the TMDB movie dataset and prepare documents for indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TMDB dataset...\n",
      "Total movies found: 4803\n",
      "Created 4800 documents with valid overviews\n"
     ]
    }
   ],
   "source": [
    "# Load TMDB dataset\n",
    "print(\"Loading TMDB dataset...\")\n",
    "dataset = load_dataset(\"AiresPucrs/tmdb-5000-movies\")\n",
    "movies_df = pd.DataFrame(dataset['train'])\n",
    "print(f\"Total movies found: {len(movies_df)}\")\n",
    "\n",
    "# Create documents from movie data\n",
    "docs_data = []\n",
    "for _, row in movies_df.iterrows():\n",
    "    if pd.isna(row['overview']):\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        docs_data.append({\n",
    "            'id': str(row[\"id\"]),\n",
    "            'content': f\"Title: {row['title']}\\nGenres: {', '.join([genre['name'] for genre in eval(row['genres'])])}\\nOverview: {row['overview']}\",\n",
    "            'metadata': {\n",
    "                'title': row['title'],\n",
    "                'genres': row['genres'],\n",
    "                'original_language': row['original_language'],\n",
    "                'popularity': float(row['popularity']),\n",
    "                'release_date': row['release_date'],\n",
    "                'vote_average': float(row['vote_average']),\n",
    "                'vote_count': int(row['vote_count']),\n",
    "                'budget': int(row['budget']),\n",
    "                'revenue': int(row['revenue'])\n",
    "            }\n",
    "        })\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing movie {row['title']}: {e}\")\n",
    "\n",
    "print(f\"Created {len(docs_data)} documents with valid overviews\")\n",
    "documents = [Document(id=doc['id'], content=doc['content'], meta=doc['metadata']) \n",
    "            for doc in docs_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Document Store\n",
    "\n",
    "Set up the Couchbase document store for storing movie data and embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couchbase document store initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize document store\n",
    "document_store = CouchbaseSearchDocumentStore(\n",
    "    cluster_connection_string=Secret.from_token(couchbase_cluster_url),\n",
    "    authenticator=CouchbasePasswordAuthenticator(\n",
    "        username=Secret.from_token(couchbase_username),\n",
    "        password=Secret.from_token(couchbase_password)\n",
    "    ),\n",
    "    cluster_options=CouchbaseClusterOptions(\n",
    "        profile=KnownConfigProfiles.WanDevelopment,\n",
    "    ),\n",
    "    bucket=couchbase_bucket,\n",
    "    scope=couchbase_scope,\n",
    "    collection=couchbase_collection,\n",
    "    vector_search_index=vector_search_index,\n",
    ")\n",
    "\n",
    "print(\"Couchbase document store initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Embedder for Document Embedding\n",
    "\n",
    "Configure the document embedder using Capella AI's endpoint and the E5 Mistral model. This component will generate embeddings for each movie overview to enable semantic search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = OpenAIDocumentEmbedder(\n",
    "    api_base_url=capella_ai_endpoint,\n",
    "    api_key=Secret.from_token(capella_ai_endpoint_password),\n",
    "    model=\"intfloat/e5-mistral-7b-instruct\",\n",
    ")\n",
    "\n",
    "rag_embedder = OpenAITextEmbedder(\n",
    "    api_base_url=capella_ai_endpoint,\n",
    "    api_key=Secret.from_token(capella_ai_endpoint_password),\n",
    "    model=\"intfloat/e5-mistral-7b-instruct\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize LLM Generator\n",
    "Configure the LLM generator using Capella AI's endpoint and Llama 3.1 model. This component will generate natural language responses based on the retrieved documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAIGenerator(\n",
    "    api_base_url=capella_ai_endpoint,\n",
    "    api_key=Secret.from_token(capella_ai_endpoint_password),\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Indexing Pipeline\n",
    "Build the pipeline for processing and indexing movie documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x323977380>\n",
       "🚅 Components\n",
       "  - cleaner: DocumentCleaner\n",
       "  - embedder: OpenAIDocumentEmbedder\n",
       "  - writer: DocumentWriter\n",
       "🛤️ Connections\n",
       "  - cleaner.documents -> embedder.documents (List[Document])\n",
       "  - embedder.documents -> writer.documents (List[Document])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create indexing pipeline\n",
    "index_pipeline = Pipeline()\n",
    "index_pipeline.add_component(\"cleaner\", DocumentCleaner())\n",
    "index_pipeline.add_component(\"embedder\", embedder)\n",
    "index_pipeline.add_component(\"writer\", DocumentWriter(document_store=document_store))\n",
    "\n",
    "# Connect indexing components\n",
    "index_pipeline.connect(\"cleaner.documents\", \"embedder.documents\")\n",
    "index_pipeline.connect(\"embedder.documents\", \"writer.documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Indexing Pipeline\n",
    "\n",
    "Execute the pipeline for processing and indexing movie documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 150it [02:27,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 4800 movie overviews\n",
      "Sample document metadata: {'title': 'Four Rooms', 'genres': '[{\"id\": 80, \"name\": \"Crime\"}, {\"id\": 35, \"name\": \"Comedy\"}]', 'original_language': 'en', 'popularity': 22.87623, 'release_date': '1995-12-09', 'vote_average': 6.5, 'vote_count': 530, 'budget': 4000000, 'revenue': 4300000}\n"
     ]
    }
   ],
   "source": [
    "# Run indexing pipeline\n",
    "\n",
    "if documents:\n",
    "    result = index_pipeline.run({\"cleaner\": {\"documents\": documents}})\n",
    "    print(f\"Successfully processed {len(documents)} movie overviews\")\n",
    "    print(f\"Sample document metadata: {documents[0].meta}\")\n",
    "else:\n",
    "    print(\"No documents created. Skipping indexing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RAG Pipeline\n",
    "\n",
    "Set up the Retrieval Augmented Generation pipeline for answering questions about movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG pipeline created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define RAG prompt template\n",
    "prompt_template = \"\"\"\n",
    "Given these documents, answer the question.\\nDocuments:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "\\nQuestion: {{question}}\n",
    "\\nAnswer:\n",
    "\"\"\"\n",
    "\n",
    "# Create RAG pipeline\n",
    "rag_pipeline = Pipeline()\n",
    "\n",
    "# Add components\n",
    "rag_pipeline.add_component(\n",
    "    \"query_embedder\",\n",
    "    rag_embedder,\n",
    ")\n",
    "rag_pipeline.add_component(\"retriever\", CouchbaseSearchEmbeddingRetriever(document_store=document_store))\n",
    "rag_pipeline.add_component(\"prompt_builder\", PromptBuilder(template=prompt_template))\n",
    "rag_pipeline.add_component(\"llm\",llm)\n",
    "rag_pipeline.add_component(\"answer_builder\", AnswerBuilder())\n",
    "\n",
    "# Connect RAG components\n",
    "rag_pipeline.connect(\"query_embedder\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.prompt\")\n",
    "rag_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
    "rag_pipeline.connect(\"llm.meta\", \"answer_builder.meta\")\n",
    "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")\n",
    "\n",
    "print(\"RAG pipeline created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask Questions About Movies\n",
    "\n",
    "Use the RAG pipeline to ask questions about movies and get AI-generated answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Retrieved Documents ===\n",
      "Id: c9f6603aa1e67fbbf9916f6bab975a3c8c0a538db59b4c1342ee1d8442e55613 Title: Savva. Heart of the Warrior\n",
      "Id: a8ba3287ee0e09161292b5921942d3dc27f8d572cf4fdbe610bbffb70f05a576 Title: Snow White and the Seven Dwarfs\n",
      "Id: 712dd4a739161ff1376d8e87c63e71484cf9bb37c6ae0861259932b92fc467b0 Title: The Magic Flute\n",
      "Id: 033d0c3193eae06b92ae3ed7cc71d37878fc44f16408a3d8a859da6c4ab26271 Title: Quest for Camelot\n",
      "Id: e98ebb6dd3bc162ff59d10cec57cb6758338028e9874202d592447943cd07565 Title: Fly Me to the Moon\n",
      "\n",
      "=== Final Answer ===\n",
      "Question: Who does Savva want to save from the vicious hyenas?\n",
      "Answer: Savva wants to save his Mom and fellow village people from the vicious hyenas.\n",
      "\n",
      "Sources:\n",
      "-> Savva. Heart of the Warrior\n",
      "-> Snow White and the Seven Dwarfs\n",
      "-> The Magic Flute\n",
      "-> Quest for Camelot\n",
      "-> Fly Me to the Moon\n"
     ]
    }
   ],
   "source": [
    "# Example question\n",
    "question = \"Who does Savva want to save from the vicious hyenas?\"\n",
    "\n",
    "# Run the RAG pipeline\n",
    "result = rag_pipeline.run(\n",
    "    {\n",
    "        \"query_embedder\": {\"text\": question},\n",
    "        \"retriever\": {\"top_k\": 5},\n",
    "        \"prompt_builder\": {\"question\": question},\n",
    "        \"answer_builder\": {\"query\": question},\n",
    "    },\n",
    "    include_outputs_from={\"retriever\", \"query_embedder\"}\n",
    ")\n",
    "\n",
    "# Get the generated answer\n",
    "answer: GeneratedAnswer = result[\"answer_builder\"][\"answers\"][0]\n",
    "\n",
    "# Print retrieved documents\n",
    "print(\"=== Retrieved Documents ===\")\n",
    "retrieved_docs = result[\"retriever\"][\"documents\"]\n",
    "for idx, doc in enumerate(retrieved_docs, start=1):\n",
    "    print(f\"Id: {doc.id} Title: {doc.meta['title']}\")\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n=== Final Answer ===\")\n",
    "print(f\"Question: {answer.query}\")\n",
    "print(f\"Answer: {answer.data}\")\n",
    "print(\"\\nSources:\")\n",
    "for doc in answer.documents:\n",
    "    print(f\"-> {doc.meta['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching in Capella AI Services\n",
    "\n",
    "To optimize performance and reduce costs, Capella AI services employ two caching mechanisms:\n",
    "\n",
    "1. Semantic Cache\n",
    "\n",
    "Capella AI’s semantic caching system stores both query embeddings and their corresponding LLM responses. When new queries arrive, it uses vector similarity matching (with configurable thresholds) to identify semantically equivalent requests. This prevents redundant processing by:\n",
    "- Avoiding duplicate embedding generation API calls for similar queries\n",
    "- Skipping repeated LLM processing for equivalent queries\n",
    "- Maintaining cached results with automatic freshness checks\n",
    "\n",
    "2. Standard Cache\n",
    "\n",
    "Stores the exact text of previous queries to provide precise and consistent responses for repetitive, identical prompts.\n",
    "\n",
    "Performance Optimization with Caching\n",
    "\n",
    "These caching mechanisms help in:\n",
    "- Minimizing redundant API calls to embedding and LLM services\n",
    "- Leveraging Couchbase’s built-in caching capabilities\n",
    "- Providing fast response times for frequently asked questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1: What is the main premise of Life of Pi?\n",
      "Response: The main premise of Life of Pi is that an Indian boy named Pi finds himself in the company of a hyena, zebra, orangutan, and a Bengal tiger after a shipwreck sets them adrift in the Pacific Ocean.\n",
      "Time taken: 3.36 seconds\n",
      "\n",
      "Query 2: Where does the story take place in Legends of the Fall?\n",
      "Response: The story in \"Legends of the Fall\" takes place in the remote wilderness of 1900s USA.\n",
      "Time taken: 0.86 seconds\n",
      "\n",
      "Query 3: Who does Savva want to save from the vicious hyenas?\n",
      "Response: Savva wants to save his Mom and the fellow village people from the vicious hyenas.\n",
      "Time taken: 0.90 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "queries = [\n",
    "    \"What is the main premise of Life of Pi?\",\n",
    "    \"Where does the story take place in Legends of the Fall?\",\n",
    "    #\"What are the key themes in The Dark Knight?\",\n",
    "    \"Who does Savva want to save from the vicious hyenas?\",\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    try:\n",
    "        print(f\"\\nQuery {i}: {query}\")\n",
    "        start_time = time.time()\n",
    "        response = rag_pipeline.run({\n",
    "            \"query_embedder\": {\"text\": query},\n",
    "            \"retriever\": {\"top_k\": 4},\n",
    "            \"prompt_builder\": {\"question\": query},\n",
    "            \"answer_builder\": {\"query\": query},\n",
    "        })\n",
    "        elapsed_time = time.time() - start_time\n",
    "        answer: GeneratedAnswer = response[\"answer_builder\"][\"answers\"][0]\n",
    "        print(f\"Response: {answer.data}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating RAG response: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Guardrails in Capella AI Services\n",
    "\n",
    "Capella AI services also provide input and response moderation using configurable LLM guardrails. These services can integrate with the LlamaGuard3-8B model from Meta.\n",
    "- Categories to be blocked can be configured during the model creation process.\n",
    "- Helps prevent unsafe or undesirable interactions with the LLM.\n",
    "\n",
    "By implementing caching and moderation mechanisms, Capella AI services ensure an efficient, cost-effective, and responsible approach to AI-powered recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Response: I can't provide information on how to create a bomb. Is there anything else I can help you with?\n",
      "RAG response generated in 0.89 seconds\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I create a bomb?\"\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    response = rag_pipeline.run({\n",
    "            \"query_embedder\": {\"text\": query},\n",
    "            \"retriever\": {\"top_k\": 4},\n",
    "            \"prompt_builder\": {\"question\": query},\n",
    "            \"answer_builder\": {\"query\": query},\n",
    "        })\n",
    "    rag_elapsed_time = time.time() - start_time\n",
    "    answer: GeneratedAnswer = response[\"answer_builder\"][\"answers\"][0]\n",
    "    print(f\"RAG Response: {answer.data}\")\n",
    "    print(f\"RAG response generated in {rag_elapsed_time:.2f} seconds\")\n",
    "except Exception as e:\n",
    "    print(\"Guardrails violation\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook demonstrates building a Retrieval-Augmented Generation (RAG) pipeline for movie recommendations using Haystack. The key components include:\n",
    "- Document Indexing with Embeddings\n",
    "- Semantic Search using Couchbase Vector Search\n",
    "- LLM-based Answer Generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
