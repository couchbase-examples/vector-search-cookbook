{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this guide, we will walk you through building a Retrieval Augmented Generation (RAG) application using Couchbase Capella as the database, [Mistral-7B-Instruct-v0.3](https://build.nvidia.com/mistralai/mistral-7b-instruct-v03/modelcard) model as the large language model provided by Capella Model Services. We will use the [NVIDIA NeMo Retriever Llama3.2](https://build.nvidia.com/nvidia/llama-3_2-nv-embedqa-1b-v2/modelcard) model for generating embeddings via Capella Model Services.\n",
    "\n",
    "This notebook demonstrates how to build a RAG system using:\n",
    "- The [BBC News dataset](https://huggingface.co/datasets/RealTimeData/bbc_news_alltime) containing news articles\n",
    "- Couchbase Capella as the vector store with **Hyperscale and Composite Vector Indexes** for high-performance vector search\n",
    "- Capella Model Services for embeddings and text generation\n",
    "- LangChain framework for the RAG pipeline\n",
    "\n",
    "We leverage Couchbase's **Query Service** to create and manage Hyperscale Vector Indexes, enabling efficient semantic search capabilities that can scale to billions of vectors. Hyperscale and Composite indexes provide superior performance for large-scale vector search operations compared to traditional approaches. This tutorial can also be recreated using the Search Service with [Search Vector Index](https://github.com/couchbase-examples/vector-search-cookbook/blob/main/capella-model-services/langchain/search_based/RAG_with_Capella_Model_Services_and_LangChain.ipynb).\n",
    "\n",
    "**Key Features:**\n",
    "- High-performance vector search using Hyperscale/Composite indexes\n",
    "- Performance benchmarks showing optimization benefits\n",
    "- Complete RAG workflow with caching optimization\n",
    "\n",
    "**Requirements:** Couchbase Server 8.0+ or Capella with Query Service enabled.\n",
    "\n",
    "Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval. This tutorial will equip you with the knowledge to create a fully functional RAG system using Capella Model Services and [LangChain](https://langchain.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run this tutorial\n",
    "\n",
    "This tutorial is available as a Jupyter Notebook (`.ipynb` file) that you can run interactively. You can access the original notebook [here](https://github.com/couchbase-examples/vector-search-cookbook/blob/main/capella-model-services/langchain/query_based/RAG_with_Capella_Model_Services_and_LangChain.ipynb)\n",
    "\n",
    "You can either download the notebook file and run it on [Google Colab](https://colab.research.google.com/) or run it on your system by setting up the Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you start\n",
    "\n",
    "## Create and Deploy Your Operational cluster on Capella\n",
    "\n",
    "To get started with Couchbase Capella, create an account and use it to deploy an operational cluster.\n",
    "\n",
    "To know more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html).\n",
    "\n",
    "\n",
    "### Couchbase Capella Configuration\n",
    "\n",
    "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met:\n",
    "\n",
    "* Have a multi-node Capella cluster running the **Data, Query, and Index services** (Query Service is required for Hyperscale/Composite indexes).\n",
    "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the bucket (Read and Write) used in the application.\n",
    "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running.\n",
    "\n",
    "### Deploy Models\n",
    "\n",
    "In order to create the RAG application, we need an embedding model to ingest the documents for Vector Search and a large language model (LLM) for generating the responses based on the context.\n",
    "\n",
    "Capella Model Service allows you to create both the embedding model and the LLM in the same VPC as your database. There are multiple options for both the Embedding & Large Language Models, along with Value Adds to the models.\n",
    "\n",
    "Create the models using the Capella Model Services interface. While creating the model, it is possible to cache the responses (both standard and semantic cache) and apply guardrails to the LLM responses.\n",
    "\n",
    "For more details, please refer to the [documentation](https://docs.couchbase.com/ai/build/model-service/model-service.html). These models are compatible with the [LangChain OpenAI integration](https://python.langchain.com/api_reference/openai/index.html).\n",
    "\n",
    "After the models are deployed, please create the API keys for them and whitelist the keys on the IP on which the tutorial is being run. For more details, please refer to the documentation on [generating the API keys](https://docs.couchbase.com/ai/api-guide/api-start.html#model-service-keys)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Necessary Libraries\n",
    "To build our RAG system, we need a set of libraries. The libraries we install handle everything from connecting to databases to performing AI tasks. Each library has a specific role: Couchbase libraries manage database operations, LangChain handles AI model integrations, and we will use the OpenAI SDK for generating embeddings and calling the LLM in Capella Model services. By setting up these libraries, we ensure our environment is equipped to handle the tasks required for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet datasets==4.4.1 langchain-couchbase==1.0.0 langchain-openai==1.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries\n",
    "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading. These libraries provide essential functions for working with data, managing database connections, and processing machine learning models.\n",
    "\n",
    "Note that we import `CouchbaseQueryVectorStore` along with `DistanceStrategy` and `IndexType` for creating Hyperscale/Composite Vector Indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import logging\nimport os\nimport time\n\nfrom datetime import timedelta\n\nfrom dotenv import load_dotenv\n\nfrom couchbase.auth import PasswordAuthenticator\nfrom couchbase.cluster import Cluster\nfrom couchbase.exceptions import CouchbaseException\nfrom couchbase.options import ClusterOptions\n\nfrom datasets import load_dataset\n\nfrom langchain_core.documents import Document\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_couchbase.vectorstores import CouchbaseQueryVectorStore\nfrom langchain_couchbase.vectorstores import DistanceStrategy, IndexType\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\n\nfrom tqdm import tqdm"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Loading Environment Variables\n\nThis notebook loads configuration from a `.env` file in the same directory. Create a `.env` file with the following variables:\n\n**Required (no defaults):**\n- `CB_CONNECTION_STRING` - Your Couchbase connection string\n- `CB_USERNAME` - Couchbase database username\n- `CB_PASSWORD` - Couchbase database password\n- `CB_BUCKET_NAME` - Name of your Couchbase bucket\n- `CAPELLA_MODEL_SERVICES_ENDPOINT` - Capella Model Services endpoint (include `/v1` suffix)\n- `LLM_API_KEY` - API key for the LLM model\n- `EMBEDDING_API_KEY` - API key for the embedding model\n\n**Optional (with defaults):**\n- `SCOPE_NAME` - Scope name (default: `_default`)\n- `COLLECTION_NAME` - Collection name (default: `langchain_docs`)\n- `LLM_MODEL_NAME` - LLM model name (default: `mistralai/mistral-7b-instruct-v0.3`)\n- `EMBEDDING_MODEL_NAME` - Embedding model name (default: `nvidia/llama-3.2-nv-embedqa-1b-v2`)\n\n> **Note:** The Capella Model Services Endpoint requires `/v1` suffix if not shown on the UI.\n\n> If the models are running in the same region, either API key can be used interchangeably. See [generating API keys](https://docs.couchbase.com/ai/api-guide/api-start.html#model-service-keys)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load environment variables from .env file\nload_dotenv()\n\n# Couchbase connection settings (no defaults for sensitive values)\nCB_CONNECTION_STRING = os.getenv(\"CB_CONNECTION_STRING\")\nCB_USERNAME = os.getenv(\"CB_USERNAME\")\nCB_PASSWORD = os.getenv(\"CB_PASSWORD\")\nCB_BUCKET_NAME = os.getenv(\"CB_BUCKET_NAME\")\n\n# Collection settings (with sensible defaults)\nSCOPE_NAME = os.getenv(\"SCOPE_NAME\", \"_default\")\nCOLLECTION_NAME = os.getenv(\"COLLECTION_NAME\", \"langchain_docs\")\n\n# Capella Model Services settings\nCAPELLA_MODEL_SERVICES_ENDPOINT = os.getenv(\"CAPELLA_MODEL_SERVICES_ENDPOINT\")\n\n# Model names (with defaults matching tutorial recommendations)\nLLM_MODEL_NAME = os.getenv(\"LLM_MODEL_NAME\", \"mistralai/mistral-7b-instruct-v0.3\")\nEMBEDDING_MODEL_NAME = os.getenv(\"EMBEDDING_MODEL_NAME\", \"nvidia/llama-3.2-nv-embedqa-1b-v2\")\n\n# API keys (no defaults for sensitive values)\nLLM_API_KEY = os.getenv(\"LLM_API_KEY\")\nEMBEDDING_API_KEY = os.getenv(\"EMBEDDING_API_KEY\")\n\n# Validate required environment variables\nif not all([\n    CB_CONNECTION_STRING,\n    CB_USERNAME,\n    CB_PASSWORD,\n    CB_BUCKET_NAME,\n    CAPELLA_MODEL_SERVICES_ENDPOINT,\n    LLM_API_KEY,\n    EMBEDDING_API_KEY,\n]):\n    raise ValueError(\n        \"Missing required environment variables. Please ensure your .env file contains:\\n\"\n        \"- CB_CONNECTION_STRING\\n\"\n        \"- CB_USERNAME\\n\"\n        \"- CB_PASSWORD\\n\"\n        \"- CB_BUCKET_NAME\\n\"\n        \"- CAPELLA_MODEL_SERVICES_ENDPOINT\\n\"\n        \"- LLM_API_KEY\\n\"\n        \"- EMBEDDING_API_KEY\"\n    )\n\nprint(\"Environment variables loaded successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the Couchbase Cluster\n",
    "Couchbase will serve as our primary data store, handling all the storage and retrieval operations required for our RAG system. By establishing this connection, we enable our application to interact with the database, allowing us to perform operations such as storing embeddings, querying data, and managing collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "    options = ClusterOptions(auth)\n",
    "    cluster = Cluster(CB_CONNECTION_STRING, options)\n",
    "    cluster.wait_until_ready(timedelta(seconds=5))\n",
    "    print(\"Successfully connected to Couchbase\")\n",
    "except Exception as e:\n",
    "    raise ConnectionError(f\"Failed to connect to Couchbase: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Collections in Couchbase\n",
    "In Couchbase, data is organized in buckets, which can be further divided into scopes and collections. Think of a collection as a table in a traditional SQL database. Before we can store any data, we need to ensure that our collections exist. If they don't, we must create them. This step is important because it prepares the database to handle the specific types of data our application will process. By setting up collections, we define the structure of our data storage, which is essential for efficient data retrieval and management.\n",
    "\n",
    "Moreover, setting up collections allows us to isolate different types of data within the same bucket, providing a more organized and scalable data structure. This is particularly useful when dealing with large datasets, as it ensures that related data is stored together, making it easier to manage and query. Here, we also set up the primary index for query operations on the collection and clear the existing documents in the collection if any. If you do not want to do that, please skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_collection(cluster, bucket_name, scope_name, collection_name, flush_collection=False):\n",
    "    try:\n",
    "        bucket = cluster.bucket(bucket_name)\n",
    "        bucket_manager = bucket.collections()\n",
    "\n",
    "        # Check if scope exists, create if it doesn't\n",
    "        scopes = bucket_manager.get_all_scopes()\n",
    "        scope_exists = any(scope.name == scope_name for scope in scopes)\n",
    "        \n",
    "        if not scope_exists:\n",
    "            print(f\"Scope '{scope_name}' does not exist. Creating it...\")\n",
    "            bucket_manager.create_scope(scope_name)\n",
    "            print(f\"Scope '{scope_name}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Scope '{scope_name}' already exists. Skipping creation.\")\n",
    "        \n",
    "        # Check if collection exists, create if it doesn't\n",
    "        collections = bucket_manager.get_all_scopes()\n",
    "        collection_exists = any(\n",
    "            scope.name == scope_name\n",
    "            and collection_name in [col.name for col in scope.collections]\n",
    "            for scope in collections\n",
    "        )\n",
    "\n",
    "        if not collection_exists:\n",
    "            print(f\"Collection '{collection_name}' does not exist. Creating it...\")\n",
    "            bucket_manager.create_collection(scope_name, collection_name)\n",
    "            print(f\"Collection '{collection_name}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Collection '{collection_name}' already exists. Skipping creation.\")\n",
    "\n",
    "        collection = bucket.scope(scope_name).collection(collection_name)\n",
    "        time.sleep(2)  # Give the collection time to be ready for queries\n",
    "\n",
    "        # Ensure primary index exists (required for Query Service operations)\n",
    "        try:\n",
    "            cluster.query(\n",
    "                f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
    "            ).execute()\n",
    "            print(\"Primary index present or created successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error creating primary index: {str(e)}\")\n",
    "\n",
    "        if flush_collection:\n",
    "            # Clear all documents in the collection\n",
    "            try:\n",
    "                query = f\"DELETE FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
    "                cluster.query(query).execute()\n",
    "                print(\"All documents cleared from the collection.\")\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Error while clearing documents: {str(e)}. The collection might be empty.\"\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error setting up collection: {str(e)}\")\n",
    "\n",
    "\n",
    "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME, flush_collection=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the BBC News Dataset\n",
    "To build a RAG engine, we need data to search through. We use the [BBC Realtime News dataset](https://huggingface.co/datasets/RealTimeData/bbc_news_alltime), a dataset with up-to-date BBC news articles grouped by month. This dataset contains articles that were created after the LLM was trained. It will showcase the use of RAG to augment the LLM.\n",
    "\n",
    "The BBC News dataset's varied content allows us to simulate real-world scenarios where users ask complex questions, enabling us to fine-tune our RAG's ability to understand and respond to various types of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    news_dataset = load_dataset('RealTimeData/bbc_news_alltime', '2024-12', split=\"train\")\n",
    "    print(f\"Loaded the BBC News dataset with {len(news_dataset)} rows\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading BBC dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the Data\n",
    "\n",
    "We will use the content of the news articles for our RAG system.\n",
    "\n",
    "The dataset contains a few duplicate records. We are removing them to avoid duplicate results in the retrieval stage of our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles = news_dataset[\"content\"]\n",
    "unique_articles = set()\n",
    "for article in news_articles:\n",
    "    if article:\n",
    "        unique_articles.add(article)\n",
    "unique_news_articles = list(unique_articles)\n",
    "print(f\"We have {len(unique_news_articles)} unique articles in our database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Embeddings using Capella Model Service\n",
    "Embeddings are at the heart of semantic search. They are numerical representations of text that capture the semantic meaning of the words and phrases. Unlike traditional keyword-based search, which looks for exact matches, embeddings allow our search engine to understand the context and nuances of language, enabling it to retrieve documents that are semantically similar to the query, even if they don't contain the exact keywords. By creating embeddings using Capella Model service, we equip our RAG system with the ability to understand and process natural language in a way that is much closer to how humans understand language. This step transforms our raw text data into a format that the Capella vector store can use to find and rank relevant documents.\n",
    "\n",
    "We are using the OpenAI Embeddings via the [LangChain OpenAI provider](https://python.langchain.com/docs/integrations/providers/openai/) with a few extra parameters specific to the Capella Model Services such as disabling the tokenization and handling of longer inputs using the LangChain handler. We provide the model and api_key and the URL for the SDK to those for Capella Model Services. For this tutorial, we are using the [nvidia/llama-3.2-nv-embedqa-1b-v2](https://build.nvidia.com/nvidia/llama-3_2-nv-embedqa-1b-v2) embedding model. If you are using a different model, you would need to change the model name accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        openai_api_key=EMBEDDING_API_KEY,\n",
    "        openai_api_base=CAPELLA_MODEL_SERVICES_ENDPOINT,\n",
    "        check_embedding_ctx_length=False,\n",
    "        tiktoken_enabled=False,\n",
    "        model=EMBEDDING_MODEL_NAME,\n",
    "    )\n",
    "    print(\"Successfully created CapellaAIEmbeddings\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating CapellaAIEmbeddings: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Embeddings Model\n",
    "We can test the embeddings model by generating an embedding for a string using the LangChain OpenAI package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embeddings.embed_query(\"this is a test sentence\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Couchbase Query Vector Store\n",
    "The vector store is set up to store the documents from the dataset. We use `CouchbaseQueryVectorStore` which enables Hyperscale and Composite Vector Indexes for high-performance vector storage and retrieval using Couchbase's Query Service.\n",
    "\n",
    "**Key differences from Search Vector Store:**\n",
    "- Uses Query Service instead of Search Service\n",
    "- Supports Hyperscale indexes that can scale to billions of vectors\n",
    "- Index is created programmatically after data ingestion\n",
    "- No need for a separate JSON index definition file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vector_store = CouchbaseQueryVectorStore(\n",
    "        cluster=cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=embeddings,\n",
    "        distance_metric=DistanceStrategy.COSINE\n",
    "    )\n",
    "    print(\"Successfully created Couchbase Query Vector Store\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to create vector store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Saving Data to the Vector Store\nWith the Vector store set up, the next step is to populate it with data. We save the BBC articles dataset to the vector store. For each document, we will generate the embeddings for the article to use with the semantic search using LangChain.\n\n**Important:** With Hyperscale/Composite indexes, data must be ingested BEFORE creating the index. The index creation process analyzes existing vectors to optimize search performance through clustering and quantization.\n\nSome articles may exceed the embedding model's maximum token limit (8192 tokens). These documents are automatically skipped during ingestion. For production use, consider splitting longer documents into chunks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "skipped_count = 0\ningested_count = 0\n\nfor article in tqdm(unique_news_articles, desc=\"Ingesting articles\"):\n    try:\n        documents = [Document(page_content=article)]\n        vector_store.add_documents(documents=documents)\n        ingested_count += 1\n    except Exception as e:\n        # Skip documents that exceed token limits or have other issues\n        if \"exceeds maximum\" in str(e) or \"token\" in str(e).lower():\n            skipped_count += 1\n            continue\n        else:\n            print(f\"Failed to save document: {str(e)[:100]}...\")\n            skipped_count += 1\n            continue\n\nprint(f\"\\nIngestion complete: {ingested_count} documents ingested, {skipped_count} skipped (exceeded token limit)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Search Performance Testing\n",
    "\n",
    "Now let's demonstrate the performance benefits of Hyperscale Vector Index by testing pure vector search performance. We'll compare:\n",
    "\n",
    "1. **Baseline Performance**: Vector search without Hyperscale index optimization\n",
    "2. **Hyperscale-Optimized Performance**: Same search with Hyperscale index\n",
    "3. **Cache Benefits**: Show how caching can be applied on top of Hyperscale for repeated queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Index Types Overview\n",
    "\n",
    "Before we start testing, let's understand the index types available:\n",
    "\n",
    "**BHIVE (Hyperscale) Vector Indexes:**\n",
    "- **Best for**: Pure vector searches - content discovery, recommendations, semantic search\n",
    "- **Performance**: High performance with low memory footprint, designed to scale to billions of vectors\n",
    "- **Optimization**: Optimized for concurrent operations, supports simultaneous searches and inserts\n",
    "- **Use when**: You primarily perform vector-only queries without complex scalar filtering\n",
    "\n",
    "**Composite Vector Indexes:**\n",
    "- **Best for**: Filtered vector searches that combine vector search with scalar value filtering\n",
    "- **Performance**: Efficient pre-filtering where scalar attributes reduce the vector comparison scope\n",
    "- **Use when**: Your queries combine vector similarity with scalar filters that eliminate large portions of data\n",
    "\n",
    "For this tutorial, we'll test both index types to demonstrate their capabilities.\n",
    "\n",
    "For more information, see [Couchbase Hyperscale and Composite Vector Index Documentation](https://docs.couchbase.com/cloud/vector-index/use-vector-indexes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Search Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vector_search_performance(vector_store, query, label=\"Vector Search\"):\n",
    "    \"\"\"Test pure vector search performance and return timing metrics\"\"\"\n",
    "    print(f\"\\n[{label}] Testing vector search performance\")\n",
    "    print(f\"[{label}] Query: '{query}'\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        results = vector_store.similarity_search_with_score(query, k=3)\n",
    "        end_time = time.time()\n",
    "        search_time = end_time - start_time\n",
    "        \n",
    "        print(f\"[{label}] Vector search completed in {search_time:.4f} seconds\")\n",
    "        print(f\"[{label}] Found {len(results)} documents\")\n",
    "        \n",
    "        if results:\n",
    "            doc, distance = results[0]\n",
    "            print(f\"[{label}] Top result distance: {distance:.6f} (lower = more similar)\")\n",
    "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "            print(f\"[{label}] Top result preview: {preview}\")\n",
    "        \n",
    "        return search_time\n",
    "    except Exception as e:\n",
    "        print(f\"[{label}] Vector search failed: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Baseline Performance (No Hyperscale Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"What was Pep Guardiola's reaction to Manchester City's current form?\"\n",
    "print(\"Testing baseline vector search performance without Hyperscale index optimization...\")\n",
    "baseline_time = test_vector_search_performance(vector_store, test_query, \"Baseline Search\")\n",
    "print(f\"\\nBaseline vector search time (without Hyperscale index): {baseline_time:.4f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating BHIVE (Hyperscale) Vector Index\n",
    "\n",
    "Now let's create a BHIVE vector index to enable high-performance vector searches. The index creation is done programmatically through the vector store.\n",
    "\n",
    "**Index Configuration:**\n",
    "- `index_type`: `IndexType.BHIVE` for pure vector search, `IndexType.COMPOSITE` for filtered searches\n",
    "- `index_name`: Unique name for the index\n",
    "- `index_description`: Controls centroids and quantization settings\n",
    "  - `IVF,SQ8`: Auto-selected centroids with 8-bit scalar quantization (recommended default)\n",
    "  - See [Quantization & Centroid Settings](https://docs.couchbase.com/cloud/vector-index/hyperscale-vector-index.html#algo_settings) for more options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating BHIVE (Hyperscale) vector index...\")\n",
    "try:\n",
    "    vector_store.create_index(\n",
    "        index_type=IndexType.BHIVE,\n",
    "        index_name=\"langchain_bhive_index\",\n",
    "        index_description=\"IVF,SQ8\"\n",
    "    )\n",
    "    print(\"BHIVE vector index created successfully\")\n",
    "    \n",
    "    # Wait for index to become available\n",
    "    print(\"Waiting for index to become available...\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(\"BHIVE vector index already exists, proceeding...\")\n",
    "    else:\n",
    "        print(f\"Error creating BHIVE vector index: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: BHIVE (Hyperscale) Optimized Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing vector search performance with BHIVE (Hyperscale) optimization...\")\n",
    "bhive_time = test_vector_search_performance(vector_store, test_query, \"BHIVE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Composite Vector Index\n",
    "\n",
    "Now let's create a Composite vector index to demonstrate filtered vector searches. Composite indexes are optimized for queries that combine vector similarity with scalar filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Composite vector index...\")\n",
    "try:\n",
    "    vector_store.create_index(\n",
    "        index_type=IndexType.COMPOSITE,\n",
    "        index_name=\"langchain_composite_index\",\n",
    "        index_description=\"IVF,SQ8\"\n",
    "    )\n",
    "    print(\"Composite vector index created successfully\")\n",
    "    \n",
    "    # Wait for index to become available\n",
    "    print(\"Waiting for index to become available...\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(\"Composite vector index already exists, proceeding...\")\n",
    "    else:\n",
    "        print(f\"Error creating Composite vector index: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Composite Index Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing vector search performance with Composite index...\")\n",
    "composite_time = test_vector_search_performance(vector_store, test_query, \"Composite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Baseline Search Time:     {baseline_time:.4f} seconds\")\n",
    "\n",
    "if baseline_time and bhive_time:\n",
    "    speedup = baseline_time / bhive_time if bhive_time > 0 else float('inf')\n",
    "    percent_improvement = ((baseline_time - bhive_time) / baseline_time) * 100 if baseline_time > 0 else 0\n",
    "    print(f\"BHIVE Search Time:        {bhive_time:.4f} seconds ({speedup:.2f}x faster, {percent_improvement:.1f}% improvement)\")\n",
    "\n",
    "if baseline_time and composite_time:\n",
    "    speedup = baseline_time / composite_time if composite_time > 0 else float('inf')\n",
    "    percent_improvement = ((baseline_time - composite_time) / baseline_time) * 100 if baseline_time > 0 else 0\n",
    "    print(f\"Composite Search Time:    {composite_time:.4f} seconds ({speedup:.2f}x faster, {percent_improvement:.1f}% improvement)\")\n",
    "\n",
    "if bhive_time and composite_time:\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Index Comparison:\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"- BHIVE (Hyperscale): Best for pure vector searches, scales to billions of vectors\")\n",
    "    print(\"- Composite: Best for filtered searches combining vector + scalar filters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Semantic Search\n",
    "Semantic search in Couchbase involves converting queries and documents into vector representations using an embeddings model. These vectors capture the semantic meaning of the text and are stored directly in Couchbase. When a query is made, Couchbase performs a similarity search by comparing the query vector against the stored document vectors.\n",
    "\n",
    "With Hyperscale indexes, the similarity metric (COSINE) is configured at vector store initialization time via `distance_metric=DistanceStrategy.COSINE`. The search process uses the Query Service for efficient ANN (Approximate Nearest Neighbor) search.\n",
    "\n",
    "**Distance Interpretation**: In vector search using Hyperscale indexes, lower distance values indicate higher similarity, while higher distance values indicate lower similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What was Pep Guardiola's reaction to Manchester City's current form?\"\n",
    "\n",
    "try:\n",
    "    # Perform the semantic search\n",
    "    start_time = time.time()\n",
    "    search_results = vector_store.similarity_search_with_score(query, k=5)\n",
    "    search_elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Display search results\n",
    "    print(\n",
    "        f\"\\nSemantic Search Results (completed in {search_elapsed_time:.2f} seconds):\"\n",
    "    )\n",
    "    for doc, score in search_results:\n",
    "        print(f\"Score: {score:.4f}, ID: {doc.id}, Text: {doc.page_content[:200]}...\")\n",
    "        print(\"---\"*20)\n",
    "\n",
    "except CouchbaseException as e:\n",
    "    raise RuntimeError(f\"Error performing semantic search: {str(e)}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Unexpected error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) with Couchbase and LangChain\n",
    "Couchbase and LangChain can be seamlessly integrated to create RAG (Retrieval-Augmented Generation) chains, enhancing the process of generating contextually relevant responses. In this setup, Couchbase serves as the vector store, where embeddings of documents are stored. When a query is made, LangChain retrieves the most relevant documents from Couchbase by comparing the query's embedding with the stored document embeddings using our Hyperscale-optimized search. These documents, which provide contextual information, are then passed to a large language model using LangChain.\n",
    "\n",
    "The language model, equipped with the context from the retrieved documents, generates a response that is both informed and contextually accurate. This integration allows the RAG chain to leverage Couchbase's efficient storage and retrieval capabilities with Hyperscale performance, while the LLM handles the generation of responses based on the context provided by the retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Large Language Model (LLM) in Capella Model Services\n",
    "We'll be using the [mistralai/mistral-7b-instruct-v0.3](https://build.nvidia.com/mistralai/mistral-7b-instruct-v03) large language model via the Capella Model Services inside the same network as the Capella operational database to process user queries and generate meaningful responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    llm = ChatOpenAI(openai_api_base=CAPELLA_MODEL_SERVICES_ENDPOINT, openai_api_key=LLM_API_KEY, model=LLM_MODEL_NAME, temperature=0)\n",
    "    logging.info(\"Successfully created the Chat model in Capella Model Services\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating Chat model in Capella Model Services: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"What was Pep Guardiola's reaction to Manchester City's current form?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful bot. If you cannot answer based on the context provided, respond with a generic answer. Answer the question as truthfully as possible using the context below:\n",
    "    {context}\n",
    "    Question: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "rag_chain = (\n",
    "    {\"context\": vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "logging.info(\"Successfully created RAG chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get responses\n",
    "query = \"What was Pep Guardiola's reaction to Manchester City's recent form?\"\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    rag_response = rag_chain.invoke(query)\n",
    "    rag_elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"RAG Response: {rag_response}\")\n",
    "    print(f\"RAG response generated in {rag_elapsed_time:.2f} seconds\")\n",
    "except Exception as e:\n",
    "    print(\"Error occurred:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Caching mechanism in Capella Model Services\n",
    "In Capella Model Services, the model outputs can be [cached](https://docs.couchbase.com/ai/build/model-service/configure-value-adds.html#caching) (both semantic and standard cache). The caching mechanism enhances the RAG's efficiency and speed, particularly when dealing with repeated or similar queries. When a query is first processed, the LLM generates a response and then stores this response in Couchbase. When similar queries come in later, the cached responses are returned. The caching duration can be configured in the Capella Model services.\n",
    "\n",
    "In this example, we are using the standard cache which works for exact matches of the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "        \"Who inaugurated the reopening of the Notre Dam Cathedral in Paris?\",\n",
    "        \"What was Pep Guardiola's reaction to Manchester City's recent form?\", \n",
    "        \"Who inaugurated the reopening of the Notre Dam Cathedral in Paris?\", # Repeated query\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    try:\n",
    "        print(f\"\\nQuery {i}: {query}\")\n",
    "        start_time = time.time()\n",
    "        response = rag_chain.invoke(query)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating RAG response: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that the repeated queries were significantly faster than the original query. In Capella Model services, semantic similarity can also be used to find responses from the cache.\n",
    "\n",
    "Caching is particularly valuable in scenarios where users may submit similar queries multiple times or where certain pieces of information are frequently requested. By storing these in a cache, we can significantly reduce the time it takes to respond to these queries, improving the user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Guardrails in Capella Model Services\n",
    "Capella Model services also have the ability to moderate the user inputs and the responses generated by the LLM. Capella Model Services can be configured to use the [Llama 3.1 NemoGuard 8B safety model](https://build.nvidia.com/nvidia/llama-3_1-nemoguard-8b-content-safety/modelcard) guardrails model from Meta. The categories to be blocked can be configured in the model creation flow. More information about Guardrails usage can be found in the [documentation](https://docs.couchbase.com/ai/build/model-service/configure-guardrails-security.html#guardrails).\n",
    "\n",
    "Here is an example of the Guardrails in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can I create a bomb?\"\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    rag_response = rag_chain.invoke(query)\n",
    "    rag_elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"RAG Response: {rag_response}\")\n",
    "    print(f\"RAG response generated in {rag_elapsed_time:.2f} seconds\")\n",
    "except Exception as e:\n",
    "    print(\"Guardrails violation\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardrails can be quite useful in preventing users from hijacking the model into doing things that you might not want the application to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "By following this tutorial, you will have a fully functional semantic search engine that leverages the strengths of Capella Model Services with Hyperscale and Composite Vector Indexes without the data being sent to third-party embedding or large language models.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- BHIVE (Hyperscale) and Composite Vector Indexes provide high-performance vector search capabilities\n",
    "- The `CouchbaseQueryVectorStore` enables programmatic index management through the Query Service\n",
    "- Index creation happens AFTER data ingestion for optimal vector clustering\n",
    "- BHIVE indexes are best for pure vector searches, Composite indexes for filtered searches\n",
    "- Combined with caching, these indexes deliver production-ready performance\n",
    "\n",
    "**Index Type Selection:**\n",
    "- Use `IndexType.BHIVE` for pure vector similarity searches (scales to billions of vectors)\n",
    "- Use `IndexType.COMPOSITE` for filtered searches combining vector + scalar filters\n",
    "\n",
    "**For Alternative Approaches:**\n",
    "- For Search Vector Index using Search Service, see: [search_based tutorial](https://github.com/couchbase-examples/vector-search-cookbook/blob/main/capella-model-services/langchain/search_based/RAG_with_Capella_Model_Services_and_LangChain.ipynb)\n",
    "\n",
    "This guide explains the principles behind semantic search and how to implement it effectively using Capella Model Services, Hyperscale/Composite Vector Indexes, and Couchbase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}