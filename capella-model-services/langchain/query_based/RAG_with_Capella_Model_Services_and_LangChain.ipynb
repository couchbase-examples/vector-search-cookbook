{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this guide, we will walk you through building a Retrieval Augmented Generation (RAG) application using Couchbase Capella as the database, [Mistral-7B-Instruct-v0.3](https://build.nvidia.com/mistralai/mistral-7b-instruct-v03/modelcard) model as the large language model provided by Capella Model Services. We will use the [NVIDIA NeMo Retriever Llama3.2](https://build.nvidia.com/nvidia/llama-3_2-nv-embedqa-1b-v2/modelcard) model for generating embeddings via Capella Model Services.\n",
    "\n",
    "This notebook demonstrates how to build a RAG system using:\n",
    "- The [BBC News dataset](https://huggingface.co/datasets/RealTimeData/bbc_news_alltime) containing news articles\n",
    "- Couchbase Capella as the vector store with **Hyperscale and Composite Vector Indexes** for high-performance vector search\n",
    "- Capella Model Services for embeddings and text generation\n",
    "- LangChain framework for the RAG pipeline\n",
    "\n",
    "We leverage Couchbase's **Query Service** to create and manage Hyperscale Vector Indexes, enabling efficient semantic search capabilities that can scale to billions of vectors. Hyperscale and Composite indexes provide superior performance for large-scale vector search operations compared to traditional approaches. This tutorial can also be recreated using the Search Service with [Search Vector Index](https://github.com/couchbase-examples/vector-search-cookbook/blob/main/capella-model-services/langchain/search_based/RAG_with_Capella_Model_Services_and_LangChain.ipynb).\n",
    "\n",
    "**Key Features:**\n",
    "- High-performance vector search using Hyperscale/Composite indexes\n",
    "- Performance benchmarks showing optimization benefits\n",
    "- Complete RAG workflow with caching optimization\n",
    "\n",
    "**Requirements:** Couchbase Server 8.0+ or Capella with Query Service enabled.\n",
    "\n",
    "Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval. This tutorial will equip you with the knowledge to create a fully functional RAG system using Capella Model Services and [LangChain](https://langchain.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run this tutorial\n",
    "\n",
    "This tutorial is available as a Jupyter Notebook (`.ipynb` file) that you can run interactively. You can access the original notebook [here](https://github.com/couchbase-examples/vector-search-cookbook/blob/main/capella-model-services/langchain/query_based/RAG_with_Capella_Model_Services_and_LangChain.ipynb)\n",
    "\n",
    "You can either download the notebook file and run it on [Google Colab](https://colab.research.google.com/) or run it on your system by setting up the Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "### Create and Deploy Your Operational cluster on Capella\n",
    "\n",
    "To get started with Couchbase Capella, create an account and use it to deploy an operational cluster.\n",
    "\n",
    "To know more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html).\n",
    "\n",
    "\n",
    "#### Couchbase Capella Configuration\n",
    "\n",
    "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met:\n",
    "\n",
    "* Have a multi-node Capella cluster running the **Data, Query, and Index services** (Query Service is required for Hyperscale/Composite indexes).\n",
    "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the bucket (Read and Write) used in the application.\n",
    "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running.\n",
    "\n",
    "#### Deploy Models\n",
    "\n",
    "In order to create the RAG application, we need an embedding model to ingest the documents for Vector Search and a large language model (LLM) for generating the responses based on the context.\n",
    "\n",
    "Capella Model Service allows you to create both the embedding model and the LLM in the same VPC as your database. There are multiple options for both the Embedding & Large Language Models, along with Value Adds to the models.\n",
    "\n",
    "Create the models using the Capella Model Services interface. While creating the model, it is possible to cache the responses (both standard and semantic cache) and apply guardrails to the LLM responses.\n",
    "\n",
    "For more details, please refer to the [documentation](https://docs.couchbase.com/ai/build/model-service/model-service.html). These models are compatible with the [LangChain OpenAI integration](https://python.langchain.com/api_reference/openai/index.html).\n",
    "\n",
    "After the models are deployed, please create the API keys for them and whitelist the keys on the IP on which the tutorial is being run. For more details, please refer to the documentation on [generating the API keys](https://docs.couchbase.com/ai/api-guide/api-start.html#model-service-keys)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Necessary Libraries\n",
    "To build our RAG system, we need a set of libraries. The libraries we install handle everything from connecting to databases to performing AI tasks. Each library has a specific role: Couchbase libraries manage database operations, LangChain handles AI model integrations, and we will use the OpenAI SDK for generating embeddings and calling the LLM in Capella Model services. By setting up these libraries, we ensure our environment is equipped to handle the tasks required for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet datasets==4.4.1 langchain-couchbase==1.0.0 langchain-openai==1.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading. These libraries provide essential functions for working with data, managing database connections, and processing machine learning models.\n",
    "\n",
    "Note that we import `CouchbaseQueryVectorStore` along with `DistanceStrategy` and `IndexType` for creating Hyperscale/Composite Vector Indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaustavghosh/Library/Python/3.14/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/kaustavghosh/Library/Python/3.14/lib/python/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.exceptions import CouchbaseException\n",
    "from couchbase.options import ClusterOptions\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_couchbase.cache import CouchbaseCache\n",
    "from langchain_couchbase.vectorstores import CouchbaseQueryVectorStore\n",
    "from langchain_couchbase.vectorstores import DistanceStrategy, IndexType\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Environment Variables\n",
    "\n",
    "This notebook loads configuration from a `.env` file in the same directory. Create a `.env` file with the following variables:\n",
    "\n",
    "**Required (no defaults):**\n",
    "- `CB_CONNECTION_STRING` - Your Couchbase connection string\n",
    "- `CB_USERNAME` - Couchbase database username\n",
    "- `CB_PASSWORD` - Couchbase database password\n",
    "- `CB_BUCKET_NAME` - Name of your Couchbase bucket\n",
    "- `CAPELLA_MODEL_SERVICES_ENDPOINT` - Capella Model Services endpoint (include `/v1` suffix)\n",
    "- `LLM_API_KEY` - API key for the LLM model\n",
    "- `EMBEDDING_API_KEY` - API key for the embedding model\n",
    "\n",
    "**Optional (with defaults):**\n",
    "- `SCOPE_NAME` - Scope name (default: `_default`)\n",
    "- `COLLECTION_NAME` - Collection name (default: `langchain_docs`)\n",
    "- `CACHE_COLLECTION` - Cache collection name (default: `cache`)\n",
    "- `LLM_MODEL_NAME` - LLM model name (default: `mistralai/mistral-7b-instruct-v0.3`)\n",
    "- `EMBEDDING_MODEL_NAME` - Embedding model name (default: `nvidia/llama-3.2-nv-embedqa-1b-v2`)\n",
    "\n",
    "> **Note:** The Capella Model Services Endpoint requires `/v1` suffix if not shown on the UI.\n",
    "\n",
    "> If the models are running in the same region, either API key can be used interchangeably. See [generating API keys](https://docs.couchbase.com/ai/api-guide/api-start.html#model-service-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Couchbase connection settings (no defaults for sensitive values)\n",
    "CB_CONNECTION_STRING = os.getenv(\"CB_CONNECTION_STRING\")\n",
    "CB_USERNAME = os.getenv(\"CB_USERNAME\")\n",
    "CB_PASSWORD = os.getenv(\"CB_PASSWORD\")\n",
    "CB_BUCKET_NAME = os.getenv(\"CB_BUCKET_NAME\")\n",
    "\n",
    "# Collection settings (with sensible defaults)\n",
    "SCOPE_NAME = os.getenv(\"SCOPE_NAME\", \"_default\")\n",
    "COLLECTION_NAME = os.getenv(\"COLLECTION_NAME\", \"langchain_docs\")\n",
    "CACHE_COLLECTION = os.getenv(\"CACHE_COLLECTION\", \"cache\")\n",
    "\n",
    "# Capella Model Services settings\n",
    "CAPELLA_MODEL_SERVICES_ENDPOINT = os.getenv(\"CAPELLA_MODEL_SERVICES_ENDPOINT\")\n",
    "\n",
    "# Model names (with defaults matching tutorial recommendations)\n",
    "LLM_MODEL_NAME = os.getenv(\"LLM_MODEL_NAME\", \"mistralai/mistral-7b-instruct-v0.3\")\n",
    "EMBEDDING_MODEL_NAME = os.getenv(\"EMBEDDING_MODEL_NAME\", \"nvidia/llama-3.2-nv-embedqa-1b-v2\")\n",
    "\n",
    "# API keys (no defaults for sensitive values)\n",
    "LLM_API_KEY = os.getenv(\"LLM_API_KEY\")\n",
    "EMBEDDING_API_KEY = os.getenv(\"EMBEDDING_API_KEY\")\n",
    "\n",
    "# Validate required environment variables\n",
    "if not all([\n",
    "    CB_CONNECTION_STRING,\n",
    "    CB_USERNAME,\n",
    "    CB_PASSWORD,\n",
    "    CB_BUCKET_NAME,\n",
    "    CAPELLA_MODEL_SERVICES_ENDPOINT,\n",
    "    LLM_API_KEY,\n",
    "    EMBEDDING_API_KEY,\n",
    "]):\n",
    "    raise ValueError(\n",
    "        \"Missing required environment variables. Please ensure your .env file contains:\\n\"\n",
    "        \"- CB_CONNECTION_STRING\\n\"\n",
    "        \"- CB_USERNAME\\n\"\n",
    "        \"- CB_PASSWORD\\n\"\n",
    "        \"- CB_BUCKET_NAME\\n\"\n",
    "        \"- CAPELLA_MODEL_SERVICES_ENDPOINT\\n\"\n",
    "        \"- LLM_API_KEY\\n\"\n",
    "        \"- EMBEDDING_API_KEY\"\n",
    "    )\n",
    "\n",
    "print(\"Environment variables loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the Couchbase Cluster\n",
    "Couchbase will serve as our primary data store, handling all the storage and retrieval operations required for our RAG system. By establishing this connection, we enable our application to interact with the database, allowing us to perform operations such as storing embeddings, querying data, and managing collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Couchbase\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "    options = ClusterOptions(auth)\n",
    "    cluster = Cluster(CB_CONNECTION_STRING, options)\n",
    "    cluster.wait_until_ready(timedelta(seconds=5))\n",
    "    print(\"Successfully connected to Couchbase\")\n",
    "except Exception as e:\n",
    "    raise ConnectionError(f\"Failed to connect to Couchbase: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Collections in Couchbase\n",
    "In Couchbase, data is organized in buckets, which can be further divided into scopes and collections. Think of a collection as a table in a traditional SQL database. Before we can store any data, we need to ensure that our collections exist. If they don't, we must create them. This step is important because it prepares the database to handle the specific types of data our application will process. By setting up collections, we define the structure of our data storage, which is essential for efficient data retrieval and management.\n",
    "\n",
    "Moreover, setting up collections allows us to isolate different types of data within the same bucket, providing a more organized and scalable data structure. This is particularly useful when dealing with large datasets, as it ensures that related data is stored together, making it easier to manage and query. Here, we clear the existing documents in the collection if any. If you do not want to do that, please skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scope 'shared' already exists. Skipping creation.\n",
      "Collection 'langchain_query' already exists. Skipping creation.\n",
      "Primary index created/verified for langchain_query.\n",
      "All documents cleared from the collection.\n",
      "Scope 'shared' already exists. Skipping creation.\n",
      "Collection 'cache' already exists. Skipping creation.\n",
      "Primary index created/verified for cache.\n",
      "All documents cleared from the collection.\n"
     ]
    }
   ],
   "source": [
    "def setup_collection(cluster, bucket_name, scope_name, collection_name, flush_collection=False):\n",
    "    try:\n",
    "        bucket = cluster.bucket(bucket_name)\n",
    "        bucket_manager = bucket.collections()\n",
    "\n",
    "        # Check if scope exists, create if it doesn't\n",
    "        scopes = bucket_manager.get_all_scopes()\n",
    "        scope_exists = any(scope.name == scope_name for scope in scopes)\n",
    "        \n",
    "        if not scope_exists:\n",
    "            print(f\"Scope '{scope_name}' does not exist. Creating it...\")\n",
    "            bucket_manager.create_scope(scope_name)\n",
    "            print(f\"Scope '{scope_name}' created successfully.\")\n",
    "            # Refresh scopes list after creation\n",
    "            scopes = bucket_manager.get_all_scopes()\n",
    "        else:\n",
    "            print(f\"Scope '{scope_name}' already exists. Skipping creation.\")\n",
    "        \n",
    "        # Check if collection exists, create if it doesn't (reuse scopes variable)\n",
    "        collection_exists = any(\n",
    "            scope.name == scope_name\n",
    "            and collection_name in [col.name for col in scope.collections]\n",
    "            for scope in scopes\n",
    "        )\n",
    "\n",
    "        if not collection_exists:\n",
    "            print(f\"Collection '{collection_name}' does not exist. Creating it...\")\n",
    "            bucket_manager.create_collection(scope_name, collection_name)\n",
    "            print(f\"Collection '{collection_name}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Collection '{collection_name}' already exists. Skipping creation.\")\n",
    "\n",
    "        collection = bucket.scope(scope_name).collection(collection_name)\n",
    "        time.sleep(2)  # Give the collection time to be ready for queries\n",
    "\n",
    "        # Create primary index for the collection (required for DELETE operations)\n",
    "        try:\n",
    "            index_name = f\"`{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
    "            query = f\"CREATE PRIMARY INDEX IF NOT EXISTS ON {index_name}\"\n",
    "            cluster.query(query).execute()\n",
    "            print(f\"Primary index created/verified for {collection_name}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Note: Could not create primary index: {str(e)}\")\n",
    "\n",
    "        if flush_collection:\n",
    "            # Clear all documents in the collection\n",
    "            try:\n",
    "                query = f\"DELETE FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
    "                cluster.query(query).execute()\n",
    "                print(\"All documents cleared from the collection.\")\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Error while clearing documents: {str(e)}. The collection might be empty.\"\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error setting up collection: {str(e)}\")\n",
    "\n",
    "\n",
    "# Setup main collection for vector store\n",
    "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME, flush_collection=True)\n",
    "\n",
    "# Setup cache collection for LLM response caching\n",
    "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, CACHE_COLLECTION, flush_collection=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the BBC News Dataset\n",
    "To build a RAG engine, we need data to search through. We use the [BBC Realtime News dataset](https://huggingface.co/datasets/RealTimeData/bbc_news_alltime), a dataset with up-to-date BBC news articles grouped by month. This dataset contains articles that were created after the LLM was trained. It will showcase the use of RAG to augment the LLM.\n",
    "\n",
    "The BBC News dataset's varied content allows us to simulate real-world scenarios where users ask complex questions, enabling us to fine-tune our RAG's ability to understand and respond to various types of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the BBC News dataset with 2687 rows\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    news_dataset = load_dataset('RealTimeData/bbc_news_alltime', '2024-12', split=\"train\")\n",
    "    print(f\"Loaded the BBC News dataset with {len(news_dataset)} rows\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading BBC dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': [\"Pakistan protest: Bushra Bibi's march for Imran Khan disappeared - BBC News\", 'Lockdown DIY linked to Walleys Quarry gases - BBC News', 'Newscast - What next for the assisted dying bill? - BBC Sounds', \"F1: Bernie Ecclestone to sell car collection worth 'hundreds of millions' - BBC Sport\", 'British man Tyler Kerry from Basildon dies on holiday in Turkey - BBC News'], 'published_date': ['2024-12-01', '2024-12-01', '2024-12-01', '2024-12-01', '2024-12-01'], 'authors': ['https://www.facebook.com/bbcnews', 'https://www.facebook.com/bbcnews', None, 'https://www.facebook.com/BBCSport/', 'https://www.facebook.com/bbcnews'], 'description': [\"Imran Khan's third wife guided protesters to the heart of the capital - and then disappeared.\", 'An academic says an increase in plasterboard sent to landfill could be behind a spike in smells.', 'And rebel forces in Syria have taken control of Aleppo', 'Former Formula 1 boss Bernie Ecclestone is to sell his collection of race cars driven by motorsport legends including Michael Schumacher, Niki Lauda and Nelson Piquet.', 'Tyler Kerry was \"a young man full of personality, kindness and compassion\", his uncle says.'], 'section': ['Asia', 'Stoke & Staffordshire', None, 'Sport', 'Essex'], 'content': ['Bushra Bibi led a protest to free Imran Khan - what happened next is a mystery\\n\\nImran Khan\\'s wife, Bushra Bibi, encouraged protesters into the heart of Pakistan\\'s capital, Islamabad\\n\\nA charred lorry, empty tear gas shells and posters of former Pakistan Prime Minister Imran Khan - it was all that remained of a massive protest led by Khan’s wife, Bushra Bibi, that had sent the entire capital into lockdown. Just a day earlier, faith healer Bibi - wrapped in a white shawl, her face covered by a white veil - stood atop a shipping container on the edge of the city as thousands of her husband’s devoted followers waved flags and chanted slogans beneath her. It was the latest protest to flare since Khan, the 72-year-old cricketing icon-turned-politician, was jailed more than a year ago after falling foul of the country\\'s influential military which helped catapult him to power. “My children and my brothers! You have to stand with me,” Bibi cried on Tuesday afternoon, her voice cutting through the deafening roar of the crowd. “But even if you don’t,” she continued, “I will still stand firm. “This is not just about my husband. It is about this country and its leader.” It was, noted some watchers of Pakistani politics, her political debut. But as the sun rose on Wednesday morning, there was no sign of Bibi, nor the thousands of protesters who had marched through the country to the heart of the capital, demanding the release of their jailed leader. While other PMs have fallen out with Pakistan\\'s military in the past, Khan\\'s refusal to stay quiet behind bars is presenting an extraordinary challenge - escalating the standoff and leaving the country deeply divided. Exactly what happened to the so-called “final march”, and Bibi, when the city went dark is still unclear. All eyewitnesses like Samia* can say for certain is that the lights went out suddenly, plunging D Chowk, the square where they had gathered, into blackness.\\n\\nWithin a day of arriving, the protesters had scattered - leaving behind Bibi\\'s burnt-out vehicle\\n\\nAs loud screams and clouds of tear gas blanketed the square, Samia describes holding her husband on the pavement, bloodied from a gun shot to his shoulder. \"Everyone was running for their lives,\" she later told BBC Urdu from a hospital in Islamabad, adding it was \"like doomsday or a war\". \"His blood was on my hands and the screams were unending.” But how did the tide turn so suddenly and decisively? Just hours earlier, protesters finally reached D Chowk late afternoon on Tuesday. They had overcome days of tear gas shelling and a maze of barricaded roads to get to the city centre. Many of them were supporters and workers of the Pakistan Tehreek-e-Insaf (PTI), the party led by Khan. He had called for the march from his jail cell, where he has been for more than a year on charges he says are politically motivated. Now Bibi - his third wife, a woman who had been largely shrouded in mystery and out of public view since their unexpected wedding in 2018 - was leading the charge. “We won’t go back until we have Khan with us,” she declared as the march reached D Chowk, deep in the heart of Islamabad’s government district.\\n\\nThousands had marched for days to reach Islamabad, demanding former Prime Minister Imran Khan be released from jail\\n\\nInsiders say even the choice of destination - a place where her husband had once led a successful sit in - was Bibi’s, made in the face of other party leader’s opposition, and appeals from the government to choose another gathering point. Her being at the forefront may have come as a surprise. Bibi, only recently released from prison herself, is often described as private and apolitical. Little is known about her early life, apart from the fact she was a spiritual guide long before she met Khan. Her teachings, rooted in Sufi traditions, attracted many followers - including Khan himself. Was she making her move into politics - or was her sudden appearance in the thick of it a tactical move to keep Imran Khan’s party afloat while he remains behind bars? For critics, it was a move that clashed with Imran Khan’s oft-stated opposition to dynastic politics. There wasn’t long to mull the possibilities. After the lights went out, witnesses say that police started firing fresh rounds of tear gas at around 21:30 local time (16:30 GMT). The crackdown was in full swing just over an hour later. At some point, amid the chaos, Bushra Bibi left. Videos on social media appeared to show her switching cars and leaving the scene. The BBC couldn’t verify the footage. By the time the dust settled, her container had already been set on fire by unknown individuals. By 01:00 authorities said all the protesters had fled.\\n\\nSecurity was tight in the city, and as night fell, lights were switched off - leaving many in the dark as to what exactly happened next\\n\\nEyewitnesses have described scenes of chaos, with tear gas fired and police rounding up protesters. One, Amin Khan, said from behind an oxygen mask that he joined the march knowing that, \"either I will bring back Imran Khan or I will be shot\". The authorities have have denied firing at the protesters. They also said some of the protesters were carrying firearms. The BBC has seen hospital records recording patients with gunshot injuries. However, government spokesperson Attaullah Tarar told the BBC that hospitals had denied receiving or treating gunshot wound victims. He added that \"all security personnel deployed on the ground have been forbidden\" from having live ammunition during protests. But one doctor told BBC Urdu that he had never done so many surgeries for gunshot wounds in a single night. \"Some of the injured came in such critical condition that we had to start surgery right away instead of waiting for anaesthesia,\" he said. While there has been no official toll released, the BBC has confirmed with local hospitals that at least five people have died. Police say at least 500 protesters were arrested that night and are being held in police stations. The PTI claims some people are missing. And one person in particular hasn’t been seen in days: Bushra Bibi.\\n\\nThe next morning, the protesters were gone - leaving behind just wrecked cars and smashed glass\\n\\nOthers defended her. “It wasn’t her fault,” insisted another. “She was forced to leave by the party leaders.” Political commentators have been more scathing. “Her exit damaged her political career before it even started,” said Mehmal Sarfraz, a journalist and analyst. But was that even what she wanted? Khan has previously dismissed any thought his wife might have her own political ambitions - “she only conveys my messages,” he said in a statement attributed to him on his X account.\\n\\nImran Khan and Bushra Bibi, pictured here arriving at court in May 2023, married in 2018\\n\\nSpeaking to BBC Urdu, analyst Imtiaz Gul calls her participation “an extraordinary step in extraordinary circumstances\". Gul believes Bushra Bibi’s role today is only about “keeping the party and its workers active during Imran Khan’s absence”. It is a feeling echoed by some PTI members, who believe she is “stepping in only because Khan trusts her deeply”. Insiders, though, had often whispered that she was pulling the strings behind the scenes - advising her husband on political appointments and guiding high-stakes decisions during his tenure. A more direct intervention came for the first time earlier this month, when she urged a meeting of PTI leaders to back Khan’s call for a rally. Pakistan’s defence minister Khawaja Asif accused her of “opportunism”, claiming she sees “a future for herself as a political leader”. But Asma Faiz, an associate professor of political science at Lahore University of Management Sciences, suspects the PTI’s leadership may have simply underestimated Bibi. “It was assumed that there was an understanding that she is a non-political person, hence she will not be a threat,” she told the AFP news agency. “However, the events of the last few days have shown a different side of Bushra Bibi.” But it probably doesn’t matter what analysts and politicians think. Many PTI supporters still see her as their connection to Imran Khan. It was clear her presence was enough to electrify the base. “She is the one who truly wants to get him out,” says Asim Ali, a resident of Islamabad. “I trust her. Absolutely!”', 'Walleys Quarry was ordered not to accept any new waste as of Friday\\n\\nA chemist and former senior lecturer in environmental sustainability has said powerful odours from a controversial landfill site may be linked to people doing more DIY during the Covid-19 pandemic. Complaints about Walleys Quarry in Silverdale, Staffordshire – which was ordered to close as of Friday – increased significantly during and after coronavirus lockdowns. Issuing the closure notice, the Environment Agency described management of the site as poor, adding it had exhausted all other enforcement tactics at premises where gases had been noxious and periodically above emission level guidelines - which some campaigners linked to ill health locally. Dr Sharon George, who used to teach at Keele University, said she had been to the site with students and found it to be clean and well-managed, and suggested an increase in plasterboard heading to landfills in 2020 could be behind a spike in stenches.\\n\\n“One of the materials that is particularly bad for producing odours and awful emissions is plasterboard,\" she said. “That’s one of the theories behind why Walleys Quarry got worse at that time.” She said the landfill was in a low-lying area, and that some of the gases that came from the site were quite heavy. “They react with water in the atmosphere, so some of the gases you smell can be quite awful and not very good for our health. “It’s why, on some days when it’s colder and muggy and a bit misty, you can smell it more.” Dr George added: “With any landfill, you’re putting things into the ground – and when you put things into the ground, if they can they will start to rot. When they start to rot they’re going to give off gases.” She believed Walleys Quarry’s proximity to people’s homes was another major factor in the amount of complaints that arose from its operation. “If you’ve got a gas that people can smell, they’re going to report it much more than perhaps a pollutant that might go unnoticed.”\\n\\nRebecca Currie said she did not think the site would ever be closed\\n\\nLocal resident and campaigner Rebecca Currie said the closure notice served to Walleys Quarry was \"absolutely amazing\". Her son Matthew has had breathing difficulties after being born prematurely with chronic lung disease, and Ms Currie says the site has made his symptoms worse. “I never thought this day was going to happen,” she explained. “We fought and fought for years.” She told BBC Midlands Today: “Our community have suffered. We\\'ve got kids who are really poorly, people have moved homes.”\\n\\nComplaints about Walleys Quarry to Newcastle-under-Lyme Borough Council exceeded 700 in November, the highest amount since 2021 according to council leader Simon Tagg. The Environment Agency (EA), which is responsible for regulating landfill sites, said it had concluded further operation at the site could result in \"significant long-term pollution\". A spokesperson for Walley\\'s Quarry Ltd said the firm rejected the EA\\'s accusations of poor management, and would be challenging the closure notice. Dr George said she believed the EA was likely to be erring on the side of caution and public safety, adding safety standards were strict. She said a lack of landfill space in the country overall was one of the broader issues that needed addressing. “As people, we just keep using stuff and then have nowhere to put it, and then when we end up putting it in places like Walleys Quarry that is next to houses, I think that’s where the problems are.”\\n\\nTell us which stories we should cover in Staffordshire', 'What next for the assisted dying bill? What next for the assisted dying bill?', 'Former Formula 1 boss Bernie Ecclestone is to sell his collection of race cars driven by motorsport legends including Michael Schumacher, Niki Lauda and Nelson Piquet.\\n\\nEcclestone, who was in charge of the sport for nearly 40 years until 2017, assembled the collection of 69 iconic F1 and Grand Prix cars over a span of more than five decades.\\n\\nThe collection includes Ferraris driven by world champions Schumacher, Lauda and Mike Hawthorn, as well as Brabham cars raced by Piquet and Carlos Pace, among others.\\n\\n\"All the cars I have bought over the years have fantastic race histories and are rare works of art,\" said 94-year-old Ecclestone.\\n\\nAmong the cars up for sale is also Stirling Moss\\' Vanwall VW10, that became the first British car to win an F1 race and the Constructors\\' Championship in 1958.\\n\\n\"I love all of my cars but the time has come for me to start thinking about what will happen to them should I no longer be here, and that is why I have decided to sell them,\" added Ecclestone.\\n\\n\"After collecting and owning them for so long, I would like to know where they have gone and not leave them for my wife to deal with should I not be around.\"\\n\\nThe former Brabham team boss has appointed specialist sports and race cars sellers Tom Hartley Jnr Ltd to manage the sale.\\n\\n\"There are many eight-figure cars within the collection, and the value of the collection combined is well into the hundreds of millions,\" said Tom Hartley Jnr.\\n\\n\"The collection spans 70 years of racing, but for me the highlight has to be the Ferraris.\\n\\n\"There is the famous \\'Thin Wall Special\\', which was the first Ferrari to ever beat Alfa Romeo, Alberto Ascari\\'s Italian GP-winning 375 F1 and historically significant championship-winning Lauda and Schumacher cars.\"\\n\\nAlso included are the Brabham BT46B, dubbed the \\'fan car\\' and designed by Gordon Murray, which Lauda drew to victory at the 1978 Swedish GP and the BT45C in which the Austrian made his debut for Ecclestone\\'s team the same year.\\n\\nBillionaire Ecclestone took over the ownership of the commercial rights of F1 in the mid-1990s and played a key role in turning the sport into one of the most watched in the world.', 'Tyler Kerry died on a family holiday in Turkey, his uncle Alex Price said\\n\\nA 20-year-old British man has died after being found fatally injured in a lift shaft while on a family holiday in Turkey. Tyler Kerry, from Basildon, Essex, was discovered on Friday morning at the hotel he was staying at near Lara Beach in Antalya. The holidaymaker was described by his family as \"a young man full of personality, kindness and compassion with his whole life ahead of him\". Holiday company Tui said it was supporting his relatives but could not comment further as a police investigation was under way.\\n\\nA UK government spokeswoman said: \"We are assisting the family of a British man who has died in Turkey.\" More than £4,500 has been pledged to a fundraiser set up to cover Mr Kerry\\'s funeral costs. He was holidaying in the seaside city with his grandparents, Collette and Ray Kerry, girlfriend Molly and other relatives.\\n\\nMr Kerry\\'s great uncle, Alex Price, said he was found at the bottom of the lift shaft at 07:00 local time (04:00 GMT). It followed a search led by his brother, Mason, and cousin, Nathan, Mr Price said. Mr Kerry had been staying on the hotel\\'s first floor.\\n\\nMr Kerry was holidaying in the seaside city of Antalya\\n\\n\"An ambulance team attended and attempted to resuscitate him but were unsuccessful,\" Mr Price told the BBC. \"We are unclear about how he came to be in the lift shaft or the events immediately preceding this.\" Mr Price said the family was issued with a death certificate after a post-mortem examination was completed. They hoped his body would be repatriated by Tuesday. Writing on a GoFundMe page, Mr Price added the family was \"completely devastated\". He thanked people for their \"kindness and consideration\" following his nephew\\'s death.\\n\\n\"We will continue to provide around-the-clock support to Tyler’s family during this difficult time,\" a spokeswoman said. \"As there is now a police investigation we are unable to comment further.\"\\n\\nDo you have a story suggestion for Essex?'], 'link': ['http://www.bbc.co.uk/news/articles/cvg02lvj1e7o', 'http://www.bbc.co.uk/news/articles/c5yg1v16nkpo', 'http://www.bbc.co.uk/sounds/play/p0k81svq', 'http://www.bbc.co.uk/sport/formula1/articles/c1lglrj4gqro', 'http://www.bbc.co.uk/news/articles/c1knkx1z8zgo'], 'top_image': ['https://ichef.bbci.co.uk/ace/standard/3840/cpsprodpb/9975/live/b22229e0-ad5a-11ef-83bc-1153ed943d1c.jpg', 'https://ichef.bbci.co.uk/ace/standard/3840/cpsprodpb/0896/live/55209f80-adb2-11ef-8f6c-f1a86bb055ec.jpg', 'https://ichef.bbci.co.uk/images/ic/320x320/p0k81sxn.jpg', 'https://ichef.bbci.co.uk/ace/standard/3840/cpsprodpb/d593/live/232527a0-af40-11ef-804b-43d0a9651a27.jpg', 'https://ichef.bbci.co.uk/ace/standard/1280/cpsprodpb/3eca/live/f8a18ba0-afb6-11ef-9b6a-97311fd9fa8b.jpg']}\n"
     ]
    }
   ],
   "source": [
    "print(news_dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the Data\n",
    "\n",
    "We will use the content of the news articles for our RAG system.\n",
    "\n",
    "The dataset contains a few duplicate records. We are removing them to avoid duplicate results in the retrieval stage of our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1749 unique articles in our database.\n"
     ]
    }
   ],
   "source": [
    "news_articles = news_dataset[\"content\"]\n",
    "unique_articles = set()\n",
    "for article in news_articles:\n",
    "    if article:\n",
    "        unique_articles.add(article)\n",
    "unique_news_articles = list(unique_articles)\n",
    "print(f\"We have {len(unique_news_articles)} unique articles in our database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings using Capella Model Service\n",
    "Embeddings are at the heart of semantic search. They are numerical representations of text that capture the semantic meaning of the words and phrases. Unlike traditional keyword-based search, which looks for exact matches, embeddings allow our search engine to understand the context and nuances of language, enabling it to retrieve documents that are semantically similar to the query, even if they don't contain the exact keywords. By creating embeddings using Capella Model service, we equip our RAG system with the ability to understand and process natural language in a way that is much closer to how humans understand language. This step transforms our raw text data into a format that the Capella vector store can use to find and rank relevant documents.\n",
    "\n",
    "We are using the OpenAI Embeddings via the [LangChain OpenAI provider](https://python.langchain.com/docs/integrations/providers/openai/) with a few extra parameters specific to the Capella Model Services such as disabling the tokenization and handling of longer inputs using the LangChain handler. We provide the model and api_key and the URL for the SDK to those for Capella Model Services. For this tutorial, we are using the [nvidia/llama-3.2-nv-embedqa-1b-v2](https://build.nvidia.com/nvidia/llama-3_2-nv-embedqa-1b-v2) embedding model. If you are using a different model, you would need to change the model name accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created CapellaAIEmbeddings\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        openai_api_key=EMBEDDING_API_KEY,\n",
    "        openai_api_base=CAPELLA_MODEL_SERVICES_ENDPOINT,\n",
    "        check_embedding_ctx_length=False,\n",
    "        tiktoken_enabled=False,\n",
    "        model=EMBEDDING_MODEL_NAME,\n",
    "    )\n",
    "    print(\"Successfully created CapellaAIEmbeddings\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating CapellaAIEmbeddings: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Embeddings Model\n",
    "We can test the embeddings model by generating an embedding for a string using the LangChain OpenAI package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings.embed_query(\"this is a test sentence\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Couchbase Query Vector Store\n",
    "The vector store is set up to store the documents from the dataset. We use `CouchbaseQueryVectorStore` which enables Hyperscale and Composite Vector Indexes for high-performance vector storage and retrieval using Couchbase's Query Service.\n",
    "\n",
    "**Key differences from Search Vector Store:**\n",
    "- Uses Query Service instead of Search Service\n",
    "- Supports Hyperscale indexes that can scale to billions of vectors\n",
    "- Index is created programmatically after data ingestion\n",
    "- No need for a separate JSON index definition file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Couchbase Query Vector Store\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vector_store = CouchbaseQueryVectorStore(\n",
    "        cluster=cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=embeddings,\n",
    "        distance_metric=DistanceStrategy.COSINE\n",
    "    )\n",
    "    print(\"Successfully created Couchbase Query Vector Store\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to create vector store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data to the Vector Store\n",
    "With the Vector store set up, the next step is to populate it with data. We save the BBC articles dataset to the vector store using batch ingestion for efficiency. Each document will have its embeddings generated for semantic search using LangChain.\n",
    "\n",
    "**Important:** With Hyperscale/Composite indexes, data must be ingested BEFORE creating the index. The index creation process analyzes existing vectors to optimize search performance through clustering and quantization.\n",
    "\n",
    "Some articles may exceed the embedding model's maximum token limit (8192 tokens). These documents are automatically skipped during ingestion. For production use, consider splitting longer documents into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 1 articles exceeding length limit\n",
      "Ingesting 1748 articles in batches of 20...\n",
      "\n",
      "Ingestion complete: 1748 documents ingested successfully\n"
     ]
    }
   ],
   "source": [
    "# Filter articles that are within token limits (50000 chars as rough estimate)\n",
    "batch_size = 20  # Smaller batch size for better reliability with remote clusters\n",
    "filtered_articles = [a for a in unique_news_articles if a and len(a) <= 50000]\n",
    "\n",
    "print(f\"Filtered {len(unique_news_articles) - len(filtered_articles)} articles exceeding length limit\")\n",
    "print(f\"Ingesting {len(filtered_articles)} articles in batches of {batch_size}...\")\n",
    "\n",
    "try:\n",
    "    vector_store.add_texts(\n",
    "        texts=filtered_articles,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    print(f\"\\nIngestion complete: {len(filtered_articles)} documents ingested successfully\")\n",
    "except Exception as e:\n",
    "    error_str = str(e).lower()\n",
    "    if \"timeout\" in error_str or \"exceeds maximum\" in error_str or \"token\" in error_str:\n",
    "        # Fall back to individual ingestion for problematic batches\n",
    "        print(f\"Batch ingestion encountered issues, falling back to individual ingestion...\")\n",
    "        skipped_count = 0\n",
    "        ingested_count = 0\n",
    "        \n",
    "        for article in tqdm(filtered_articles, desc=\"Ingesting articles\"):\n",
    "            try:\n",
    "                vector_store.add_texts(texts=[article])\n",
    "                ingested_count += 1\n",
    "            except Exception as inner_e:\n",
    "                inner_error = str(inner_e).lower()\n",
    "                if \"timeout\" in inner_error or \"exceeds maximum\" in inner_error or \"token\" in inner_error:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"Failed to save document: {str(inner_e)[:100]}...\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "        \n",
    "        print(f\"\\nIngestion complete: {ingested_count} documents ingested, {skipped_count} skipped\")\n",
    "    else:\n",
    "        raise Exception(f\"Error during batch ingestion: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Search Performance Testing\n",
    "\n",
    "Now let's demonstrate the performance benefits of Hyperscale Vector Index by testing pure vector search performance. We'll compare:\n",
    "\n",
    "1. **Baseline Performance**: Vector search without Hyperscale index optimization\n",
    "2. **Hyperscale-Optimized Performance**: Same search with Hyperscale index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Index Types Overview\n",
    "\n",
    "Before we start testing, let's understand the index types available:\n",
    "\n",
    "**BHIVE (Hyperscale) Vector Indexes:**\n",
    "- **Best for**: Pure vector searches - content discovery, recommendations, semantic search\n",
    "- **Performance**: High performance with low memory footprint, designed to scale to billions of vectors\n",
    "- **Optimization**: Optimized for concurrent operations, supports simultaneous searches and inserts\n",
    "- **Use when**: You primarily perform vector-only queries without complex scalar filtering\n",
    "\n",
    "**Composite Vector Indexes:**\n",
    "- **Best for**: Filtered vector searches that combine vector search with scalar value filtering\n",
    "- **Performance**: Efficient pre-filtering where scalar attributes reduce the vector comparison scope\n",
    "- **Use when**: Your queries combine vector similarity with scalar filters that eliminate large portions of data\n",
    "\n",
    "For this tutorial, we'll create and test a BHIVE index. See the alternative section below for Composite index configuration.\n",
    "\n",
    "For more information, see [Couchbase Hyperscale and Composite Vector Index Documentation](https://docs.couchbase.com/cloud/vector-index/use-vector-indexes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Search Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vector_search_performance(vector_store, query, label=\"Vector Search\"):\n",
    "    \"\"\"Test pure vector search performance and return timing metrics\"\"\"\n",
    "    print(f\"\\n[{label}] Testing vector search performance\")\n",
    "    print(f\"[{label}] Query: '{query}'\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        results = vector_store.similarity_search_with_score(query, k=3)\n",
    "        end_time = time.time()\n",
    "        search_time = end_time - start_time\n",
    "        \n",
    "        print(f\"[{label}] Vector search completed in {search_time:.4f} seconds\")\n",
    "        print(f\"[{label}] Found {len(results)} documents\")\n",
    "        \n",
    "        if results:\n",
    "            doc, distance = results[0]\n",
    "            print(f\"[{label}] Top result distance: {distance:.6f} (lower = more similar)\")\n",
    "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "            print(f\"[{label}] Top result preview: {preview}\")\n",
    "        \n",
    "        return search_time\n",
    "    except Exception as e:\n",
    "        print(f\"[{label}] Vector search failed: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Baseline Performance (No Hyperscale Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing baseline vector search performance without Hyperscale index optimization...\n",
      "\n",
      "[Baseline Search] Testing vector search performance\n",
      "[Baseline Search] Query: 'What was Pep Guardiola's reaction to Manchester City's current form?'\n",
      "[Baseline Search] Vector search completed in 5.6997 seconds\n",
      "[Baseline Search] Found 3 documents\n",
      "[Baseline Search] Top result distance: 0.491326 (lower = more similar)\n",
      "[Baseline Search] Top result preview: 'We have to find a way' - Guardiola vows to end relegation form\n",
      "\n",
      "This video can not be played To pla...\n",
      "\n",
      "Baseline vector search time (without Hyperscale index): 5.6997 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_query = \"What was Pep Guardiola's reaction to Manchester City's current form?\"\n",
    "print(\"Testing baseline vector search performance without Hyperscale index optimization...\")\n",
    "baseline_time = test_vector_search_performance(vector_store, test_query, \"Baseline Search\")\n",
    "print(f\"\\nBaseline vector search time (without Hyperscale index): {baseline_time:.4f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating BHIVE (Hyperscale) Vector Index\n",
    "\n",
    "Now let's create a BHIVE vector index to enable high-performance vector searches. The index creation is done programmatically through the vector store.\n",
    "\n",
    "**Index Configuration:**\n",
    "- `index_type`: `IndexType.HYPERSCALE` for pure vector search, `IndexType.COMPOSITE` for filtered searches\n",
    "- `index_name`: Unique name for the index\n",
    "- `index_description`: Controls centroids and quantization settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Configuration Details\n",
    "\n",
    "The `index_description` parameter controls vector optimization through centroids and quantization:\n",
    "\n",
    "**Format**: `'IVF[<centroids>],{PQ|SQ}<settings>'`\n",
    "\n",
    "#### IVF (Inverted File Index) - Centroids\n",
    "- Auto-selection: `IVF,SQ8` (Couchbase selects optimal count)\n",
    "- Manual: `IVF1000,SQ8` (1000 centroids)\n",
    "\n",
    "#### Quantization Options\n",
    "- **SQ (Scalar)**: `SQ4`, `SQ6`, `SQ8` - simpler, good for general use\n",
    "- **PQ (Product)**: `PQ32x8` - better precision at similar compression\n",
    "\n",
    "#### Common Examples\n",
    "- `IVF,SQ8` - Recommended default\n",
    "- `IVF1000,SQ6` - Higher compression\n",
    "- `IVF,PQ32x8` - High precision\n",
    "\n",
    "For more details, see [Quantization & Centroid Settings](https://docs.couchbase.com/cloud/vector-index/hyperscale-vector-index.html#algo_settings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BHIVE (Hyperscale) vector index...\n",
      "BHIVE vector index created successfully\n",
      "Waiting for index to become available...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating BHIVE (Hyperscale) vector index...\")\n",
    "try:\n",
    "    vector_store.create_index(\n",
    "        index_type=IndexType.HYPERSCALE,\n",
    "        index_name=\"langchain_bhive_index\",\n",
    "        index_description=\"IVF,SQ8\"\n",
    "    )\n",
    "    print(\"BHIVE vector index created successfully\")\n",
    "    \n",
    "    # Wait for index to become available\n",
    "    print(\"Waiting for index to become available...\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(\"BHIVE vector index already exists, proceeding...\")\n",
    "    else:\n",
    "        print(f\"Error creating BHIVE vector index: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: BHIVE (Hyperscale) Optimized Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing vector search performance with BHIVE (Hyperscale) optimization...\n",
      "\n",
      "[BHIVE] Testing vector search performance\n",
      "[BHIVE] Query: 'What was Pep Guardiola's reaction to Manchester City's current form?'\n",
      "[BHIVE] Vector search completed in 2.1784 seconds\n",
      "[BHIVE] Found 3 documents\n",
      "[BHIVE] Top result distance: 0.491326 (lower = more similar)\n",
      "[BHIVE] Top result preview: 'We have to find a way' - Guardiola vows to end relegation form\n",
      "\n",
      "This video can not be played To pla...\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing vector search performance with BHIVE (Hyperscale) optimization...\")\n",
    "bhive_time = test_vector_search_performance(vector_store, test_query, \"BHIVE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Composite Index Configuration\n",
    "\n",
    "If your use case requires complex filtering with scalar attributes, you can create a Composite index instead:\n",
    "\n",
    "```python\n",
    "vector_store.create_index(\n",
    "    index_type=IndexType.COMPOSITE,  # Instead of IndexType.HYPERSCALE\n",
    "    index_name=\"langchain_composite_index\",\n",
    "    index_description=\"IVF,SQ8\"\n",
    ")\n",
    "```\n",
    "\n",
    "Composite indexes are optimized for queries that combine vector similarity with scalar filters (e.g., filtering by date, category, or other metadata fields)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PERFORMANCE SUMMARY\n",
      "============================================================\n",
      "Baseline Search Time:     5.6997 seconds\n",
      "BHIVE Search Time:        2.1784 seconds (2.62x faster, 61.8% improvement)\n",
      "\n",
      "------------------------------------------------------------\n",
      "Index Recommendation:\n",
      "------------------------------------------------------------\n",
      "- BHIVE (Hyperscale): Best for pure vector searches, scales to billions of vectors\n",
      "- Composite: Best for filtered searches combining vector + scalar filters\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Baseline Search Time:     {baseline_time:.4f} seconds\")\n",
    "\n",
    "if baseline_time and bhive_time:\n",
    "    speedup = baseline_time / bhive_time if bhive_time > 0 else float('inf')\n",
    "    percent_improvement = ((baseline_time - bhive_time) / baseline_time) * 100 if baseline_time > 0 else 0\n",
    "    print(f\"BHIVE Search Time:        {bhive_time:.4f} seconds ({speedup:.2f}x faster, {percent_improvement:.1f}% improvement)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Index Recommendation:\")\n",
    "print(\"-\"*60)\n",
    "print(\"- BHIVE (Hyperscale): Best for pure vector searches, scales to billions of vectors\")\n",
    "print(\"- Composite: Best for filtered searches combining vector + scalar filters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Semantic Search\n",
    "Semantic search in Couchbase involves converting queries and documents into vector representations using an embeddings model. These vectors capture the semantic meaning of the text and are stored directly in Couchbase. When a query is made, Couchbase performs a similarity search by comparing the query vector against the stored document vectors.\n",
    "\n",
    "With Hyperscale indexes, the similarity metric (COSINE) is configured at vector store initialization time via `distance_metric=DistanceStrategy.COSINE`. The search process uses the Query Service for efficient ANN (Approximate Nearest Neighbor) search.\n",
    "\n",
    "**Distance Interpretation**: In vector search using Hyperscale indexes, lower distance values indicate higher similarity, while higher distance values indicate lower similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic Search Results (completed in 1.01 seconds):\n",
      "Score: 0.4913, ID: a468aabf71d14bf285d4365a56da3329, Text: 'We have to find a way' - Guardiola vows to end relegation form\n",
      "\n",
      "This video can not be played To play this video you need to enable JavaScript in your browser. 'Worrying' and 'staggering' - Why do Man...\n",
      "------------------------------------------------------------\n",
      "Score: 0.5177, ID: 14310dbb06c744d3b2c481ba84427f6c, Text: 'I am not good enough' - Guardiola faces daunting and major rebuild\n",
      "\n",
      "This video can not be played To play this video you need to enable JavaScript in your browser. 'I am not good enough' - Guardiola s...\n",
      "------------------------------------------------------------\n",
      "Score: 0.5312, ID: 6d254fcd373240f494686a5af3cdc269, Text: Manchester City boss Pep Guardiola has won 18 trophies since he arrived at the club in 2016\n",
      "\n",
      "Manchester City boss Pep Guardiola says he is \"fine\" despite admitting his sleep and diet are being affecte...\n",
      "------------------------------------------------------------\n",
      "Score: 0.5353, ID: d78f612531d54053906f822c26570d2e, Text: 'Self-doubt, errors & big changes' - inside the crisis at Man City\n",
      "\n",
      "Pep Guardiola has not been through a moment like this in his managerial career. Manchester City have lost nine matches in their past...\n",
      "------------------------------------------------------------\n",
      "Score: 0.5562, ID: 458cce45c01d4c398c2eb84b82d99ffe, Text: What will Trump do about Syria? What will Trump do about Syria?...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"What was Pep Guardiola's reaction to Manchester City's current form?\"\n",
    "\n",
    "try:\n",
    "    # Perform the semantic search\n",
    "    start_time = time.time()\n",
    "    search_results = vector_store.similarity_search_with_score(query, k=5)\n",
    "    search_elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Display search results\n",
    "    print(\n",
    "        f\"\\nSemantic Search Results (completed in {search_elapsed_time:.2f} seconds):\"\n",
    "    )\n",
    "    for doc, score in search_results:\n",
    "        print(f\"Score: {score:.4f}, ID: {doc.id}, Text: {doc.page_content[:200]}...\")\n",
    "        print(\"---\"*20)\n",
    "\n",
    "except CouchbaseException as e:\n",
    "    raise RuntimeError(f\"Error performing semantic search: {str(e)}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Unexpected error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) with Couchbase and LangChain\n",
    "Couchbase and LangChain can be seamlessly integrated to create RAG (Retrieval-Augmented Generation) chains, enhancing the process of generating contextually relevant responses. In this setup, Couchbase serves as the vector store, where embeddings of documents are stored. When a query is made, LangChain retrieves the most relevant documents from Couchbase by comparing the query's embedding with the stored document embeddings using our Hyperscale-optimized search. These documents, which provide contextual information, are then passed to a large language model using LangChain.\n",
    "\n",
    "The language model, equipped with the context from the retrieved documents, generates a response that is both informed and contextually accurate. This integration allows the RAG chain to leverage Couchbase's efficient storage and retrieval capabilities with Hyperscale performance, while the LLM handles the generation of responses based on the context provided by the retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Large Language Model (LLM) in Capella Model Services\n",
    "We'll be using the [mistralai/mistral-7b-instruct-v0.3](https://build.nvidia.com/mistralai/mistral-7b-instruct-v03) large language model via the Capella Model Services inside the same network as the Capella operational database to process user queries and generate meaningful responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    llm = ChatOpenAI(openai_api_base=CAPELLA_MODEL_SERVICES_ENDPOINT, openai_api_key=LLM_API_KEY, model=LLM_MODEL_NAME, temperature=0)\n",
    "    logging.info(\"Successfully created the Chat model in Capella Model Services\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating Chat model in Capella Model Services: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I don\\'t have real-time data or the ability to follow live events. However, Pep Guardiola, the manager of Manchester City, has expressed his usual balance of optimism and desire for improvement. Even though City has faced some challenges in the 2021/2022 season, he continues to emphasize the need for patience, hard work, and a focus on continuous improvement.\\n\\nIn a press conference, Guardiola noted, \"In football, you have to have patience. When I arrived, we were fifth and I said, \\'okay, we are not far away.\\' Now, we are not far away again.\" He also added, \"We have to find our best level, and when we find it, we are going to remain for a long time at the top.\"\\n\\nWhile the team has experienced ups and downs, Guardiola maintains his belief in the players and their ability to turn things around.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 200, 'prompt_tokens': 21, 'total_tokens': 221, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'mistralai/mistral-7b-instruct-v0.3', 'system_fingerprint': None, 'id': 'chat-6965a5d9e8c14fbc8e26879b7687d5ba', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c0691-6024-7f90-a128-d025ac07d6bd-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 21, 'output_tokens': 200, 'total_tokens': 221, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What was Pep Guardiola's reaction to Manchester City's current form?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Couchbase Cache\n",
    "\n",
    "We set up a Couchbase-based cache to store and retrieve LLM responses. This cache accelerates repeated queries by storing precomputed results, significantly reducing response time for frequently asked questions.\n",
    "\n",
    "When a query is first processed, the RAG chain retrieves relevant documents, generates a response using the LLM, and stores this response in the cache. For subsequent identical queries, the cached response is returned directly, bypassing the expensive LLM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created and configured Couchbase cache\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    cache = CouchbaseCache(\n",
    "        cluster=cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=CACHE_COLLECTION,\n",
    "    )\n",
    "    set_llm_cache(cache)\n",
    "    print(\"Successfully created and configured Couchbase cache\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to create cache: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful bot. If you cannot answer based on the context provided, respond with a generic answer. Answer the question as truthfully as possible using the context below:\n",
    "    {context}\n",
    "    Question: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "rag_chain = (\n",
    "    {\"context\": vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "logging.info(\"Successfully created RAG chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Response: Pep Guardiola expressed concern and frustration over Manchester City's recent form, particularly their troubles in conceding goals and their current struggle to secure wins. He acknowledged their performances have not been satisfactory and that there is a need for improvements, especially in defense and avoiding mistakes at both ends. He also mentioned the recent dip in form has affected his personal life, causing sleep and diet issues.\n",
      "RAG response generated in 5.58 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get responses\n",
    "query = \"What was Pep Guardiola's reaction to Manchester City's recent form?\"\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    rag_response = rag_chain.invoke(query)\n",
    "    rag_elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"RAG Response: {rag_response}\")\n",
    "    print(f\"RAG response generated in {rag_elapsed_time:.2f} seconds\")\n",
    "except Exception as e:\n",
    "    print(\"Error occurred:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Cache Performance\n",
    "\n",
    "This tutorial uses **two levels of caching**:\n",
    "\n",
    "1. **Client-side CouchbaseCache** (configured above): Stores LLM responses in Couchbase, providing fast retrieval for identical queries at the application level.\n",
    "\n",
    "2. **Server-side Capella Model Services caching**: The model outputs can be [cached](https://docs.couchbase.com/ai/build/model-service/configure-value-adds.html#caching) (both semantic and standard cache) at the model service level. This is configured in the Capella Model Services UI when deploying models.\n",
    "\n",
    "The following example demonstrates caching in action - notice how repeated queries are significantly faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1: Who inaugurated the reopening of the Notre Dam Cathedral in Paris?\n",
      "Response: The reopening of the Notre-Dame Cathedral in Paris was inaugurated by French President Emmanuel Macron.\n",
      "Time taken: 2.35 seconds\n",
      "\n",
      "Query 2: What was Pep Guardiola's reaction to Manchester City's recent form?\n",
      "Response: Pep Guardiola expressed concern and frustration over Manchester City's recent form, particularly their troubles in conceding goals and their current struggle to secure wins. He acknowledged their performances have not been satisfactory and that there is a need for improvements, especially in defense and avoiding mistakes at both ends. He also mentioned the recent dip in form has affected his personal life, causing sleep and diet issues.\n",
      "Time taken: 1.88 seconds\n",
      "\n",
      "Query 3: Who inaugurated the reopening of the Notre Dam Cathedral in Paris?\n",
      "Response: The reopening of the Notre-Dame Cathedral in Paris was inaugurated by French President Emmanuel Macron.\n",
      "Time taken: 1.06 seconds\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "        \"Who inaugurated the reopening of the Notre Dam Cathedral in Paris?\",\n",
    "        \"What was Pep Guardiola's reaction to Manchester City's recent form?\", \n",
    "        \"Who inaugurated the reopening of the Notre Dam Cathedral in Paris?\", # Repeated query\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    try:\n",
    "        print(f\"\\nQuery {i}: {query}\")\n",
    "        start_time = time.time()\n",
    "        response = rag_chain.invoke(query)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating RAG response: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that repeated queries benefit from caching. The CouchbaseCache stores responses for exact query matches, while Capella Model Services can additionally provide semantic caching for similar (but not identical) queries.\n",
    "\n",
    "Caching is particularly valuable in scenarios where users may submit similar queries multiple times or where certain pieces of information are frequently requested. By storing these in a cache, we can significantly reduce the time it takes to respond to these queries, improving the user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Guardrails in Capella Model Services\n",
    "Capella Model services also have the ability to moderate the user inputs and the responses generated by the LLM. Capella Model Services can be configured to use the [Llama 3.1 NemoGuard 8B safety model](https://build.nvidia.com/nvidia/llama-3_1-nemoguard-8b-content-safety/modelcard) guardrails model from Meta. The categories to be blocked can be configured in the model creation flow. More information about Guardrails usage can be found in the [documentation](https://docs.couchbase.com/ai/build/model-service/configure-guardrails-security.html#guardrails).\n",
    "\n",
    "Here is an example of the Guardrails in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Response: I'm really sorry for any confusion, but it's important to clarify that I can't assist with that question. Discussing or providing information about making explosives is illegal and dangerous. It's essential to prioritize safety and respect the law in all circumstances. It's best to avoid searching for such information online. If you have a different question, I'd be happy to help!\n",
      "RAG response generated in 2.44 seconds\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I create a bomb?\"\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    rag_response = rag_chain.invoke(query)\n",
    "    rag_elapsed_time = time.time() - start_time\n",
    "\n",
    "    print(f\"RAG Response: {rag_response}\")\n",
    "    print(f\"RAG response generated in {rag_elapsed_time:.2f} seconds\")\n",
    "except Exception as e:\n",
    "    print(\"Guardrails violation\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardrails can be quite useful in preventing users from hijacking the model into doing things that you might not want the application to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've built a high-performance semantic search engine using Couchbase BHIVE/Composite indexes with Capella Model Services and LangChain. For the Search Vector Index alternative, see the [search_based tutorial](https://github.com/couchbase-examples/vector-search-cookbook/blob/main/capella-model-services/langchain/search_based/RAG_with_Capella_Model_Services_and_LangChain.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
