{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this guide, we will walk you through building a Retrieval Augmented Generation (RAG) application with Haystack orchestrating Capella Model Services and Couchbase Capella. We will use the models hosted on Capella Model Services for response generation and generating embeddings.\n",
    "\n",
    "This notebook demonstrates how to build a RAG system using:\n",
    "- The [BBC News dataset](https://huggingface.co/datasets/RealTimeData/bbc_news_alltime) containing news articles\n",
    "- Couchbase Capella Hyperscale and Composite Vector Indexes for vector search\n",
    "- Haystack framework for the RAG pipeline\n",
    "- Capella Model Services for embeddings and text generation\n",
    "\n",
    "We leverage Couchbase's Hyperscale and Composite Vector Indexes to enable efficient semantic search at scale. Hyperscale indexes prioritize high-throughput vector similarity across billions of vectors with a compact on-disk footprint, while Composite indexes blend scalar predicates with a vector column to narrow candidate sets before similarity search. For a deeper dive into how these indexes work, see the [overview of Capella vector indexes](https://docs.couchbase.com/cloud/vector-index/vectors-and-indexes-overview.html).\n",
    "\n",
    "Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval. This tutorial shows how to combine Capella Model Services and Haystack with Couchbase's Hyperscale and Composite Vector Indexes to deliver a production-ready RAG workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you start\n",
    "\n",
    "## Create and Deploy Your Operational cluster on Capella\n",
    "\n",
    "To get started with Couchbase Capella, create an account and use it to deploy an operational cluster.\n",
    "\n",
    "To know more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Couchbase Capella Configuration\n",
    "\n",
    "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met:\n",
    "\n",
    "* Have a multi-node Capella cluster running the Data, Query, Index, and Search services.\n",
    "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the bucket (Read and Write) used in the application.\n",
    "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running.\n",
    "\n",
    "### Deploy Models\n",
    "\n",
    "To create the RAG application, use an embedding model for Vector Search and an LLM for generating responses. \n",
    " \n",
    "Capella Model Service lets you create both models in the same VPC as your database. It offers the Llama 3.1 Instruct model (8 Billion parameters) for LLM and the mistral model for embeddings. \n",
    "\n",
    "Use the Capella AI Services interface to create these models. You can cache responses and set guardrails for LLM outputs.\n",
    "\n",
    "For more details, see the [documentation](https://preview2.docs-test.couchbase.com/ai/get-started/about-ai-services.html#model). These models work with [Haystack OpenAI integration](https://haystack.deepset.ai/integrations/openai)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Necessary Libraries\n",
    "To build our RAG system, we need a set of libraries. The libraries we install handle everything from connecting to databases to performing AI tasks. Each library has a specific role: Couchbase libraries manage database operations, Haystack handles AI model integrations and pipeline management, and we will use the OpenAI SDK (compatible with Capella Model Services) for generating embeddings and calling language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=2.1.4 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (2.3.3)\n",
      "Requirement already satisfied: datasets>=2.14.5 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (4.4.1)\n",
      "Requirement already satisfied: setuptools>=75.8.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (80.9.0)\n",
      "Requirement already satisfied: couchbase-haystack==2.* in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: transformers>=4.49.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from transformers[torch]>=4.49.0->-r requirements.txt (line 5)) (4.57.3)\n",
      "Requirement already satisfied: tensorflow>=2.18.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (2.20.0)\n",
      "Requirement already satisfied: backports-datetime-fromisoformat in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from couchbase-haystack==2.*->-r requirements.txt (line 4)) (2.0.3)\n",
      "Requirement already satisfied: couchbase==4.* in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from couchbase-haystack==2.*->-r requirements.txt (line 4)) (4.5.0)\n",
      "Requirement already satisfied: haystack-ai>=2.3.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from couchbase-haystack==2.*->-r requirements.txt (line 4)) (2.21.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from pandas>=2.1.4->-r requirements.txt (line 1)) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from pandas>=2.1.4->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from pandas>=2.1.4->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from pandas>=2.1.4->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: filelock in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets>=2.14.5->-r requirements.txt (line 2)) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets>=2.14.5->-r requirements.txt (line 2)) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets>=2.14.5->-r requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets>=2.14.5->-r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets>=2.14.5->-r requirements.txt (line 2)) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets>=2.14.5->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets>=2.14.5->-r requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets>=2.14.5->-r requirements.txt (line 2)) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.5->-r requirements.txt (line 2)) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets>=2.14.5->-r requirements.txt (line 2)) (0.36.0)\n",
      "Requirement already satisfied: packaging in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets>=2.14.5->-r requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from datasets>=2.14.5->-r requirements.txt (line 2)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from transformers>=4.49.0->transformers[torch]>=4.49.0->-r requirements.txt (line 5)) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from transformers>=4.49.0->transformers[torch]>=4.49.0->-r requirements.txt (line 5)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from transformers>=4.49.0->transformers[torch]>=4.49.0->-r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (6.33.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (3.12.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorflow>=2.18.0->-r requirements.txt (line 6)) (0.5.4)\n",
      "Requirement already satisfied: torch>=2.2 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from transformers[torch]>=4.49.0->-r requirements.txt (line 5)) (2.9.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from transformers[torch]>=4.49.0->-r requirements.txt (line 5)) (1.12.0)\n",
      "Requirement already satisfied: psutil in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from accelerate>=0.26.0->transformers[torch]>=4.49.0->-r requirements.txt (line 5)) (7.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow>=2.18.0->-r requirements.txt (line 6)) (0.45.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.5->-r requirements.txt (line 2)) (3.13.2)\n",
      "Requirement already satisfied: docstring-parser in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (0.17.0)\n",
      "Requirement already satisfied: filetype in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: haystack-experimental in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (0.14.3)\n",
      "Requirement already satisfied: jinja2 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (4.25.1)\n",
      "Requirement already satisfied: lazy-imports in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: more-itertools in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (10.8.0)\n",
      "Requirement already satisfied: networkx in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (3.6.1)\n",
      "Requirement already satisfied: openai>=1.99.2 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (2.9.0)\n",
      "Requirement already satisfied: posthog!=3.12.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (7.0.1)\n",
      "Requirement already satisfied: pydantic in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (2.12.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (9.1.2)\n",
      "Requirement already satisfied: anyio in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.14.5->-r requirements.txt (line 2)) (4.12.0)\n",
      "Requirement already satisfied: certifi in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.14.5->-r requirements.txt (line 2)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.14.5->-r requirements.txt (line 2)) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.14.5->-r requirements.txt (line 2)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.14.5->-r requirements.txt (line 2)) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.14.5->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: rich in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow>=2.18.0->-r requirements.txt (line 6)) (14.2.0)\n",
      "Requirement already satisfied: namex in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow>=2.18.0->-r requirements.txt (line 6)) (0.1.0)\n",
      "Requirement already satisfied: optree in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow>=2.18.0->-r requirements.txt (line 6)) (0.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.14.5->-r requirements.txt (line 2)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.14.5->-r requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow>=2.18.0->-r requirements.txt (line 6)) (3.10)\n",
      "Requirement already satisfied: pillow in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow>=2.18.0->-r requirements.txt (line 6)) (12.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow>=2.18.0->-r requirements.txt (line 6)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow>=2.18.0->-r requirements.txt (line 6)) (3.1.4)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from torch>=2.2->transformers[torch]>=4.49.0->-r requirements.txt (line 5)) (1.14.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.5->-r requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.5->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.5->-r requirements.txt (line 2)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.5->-r requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.5->-r requirements.txt (line 2)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.5->-r requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.5->-r requirements.txt (line 2)) (1.22.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from openai>=1.99.2->haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from openai>=1.99.2->haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from openai>=1.99.2->haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from posthog!=3.12.0->haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from pydantic->haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from pydantic->haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from pydantic->haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (0.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]>=4.49.0->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow>=2.18.0->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from jsonschema->haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from jsonschema->haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from jsonschema->haystack-ai>=2.3.0->couchbase-haystack==2.*->-r requirements.txt (line 4)) (0.30.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow>=2.18.0->-r requirements.txt (line 6)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow>=2.18.0->-r requirements.txt (line 6)) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow>=2.18.0->-r requirements.txt (line 6)) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries\n",
    "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, Haystack components for RAG pipeline, embedding generation, and dataset loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/svenkat/Library/Application Support/hatch/env/virtual/haystack/5m_kFhwO/haystack/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.exceptions import CouchbaseException\n",
    "from couchbase.options import ClusterOptions, KnownConfigProfiles, QueryOptions\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from haystack import Pipeline, Document, GeneratedAnswer\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "from haystack.components.builders.answer_builder import AnswerBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.utils import Secret\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from couchbase_haystack import (\n",
    "    CouchbaseQueryDocumentStore, \n",
    "    CouchbaseQueryEmbeddingRetriever,\n",
    "    QueryVectorSearchType, \n",
    "    QueryVectorSearchSimilarity,\n",
    "    CouchbasePasswordAuthenticator,\n",
    "    CouchbaseClusterOptions\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Sensitive Information\n",
    "In this section, we prompt the user to input essential configuration settings needed. These settings include sensitive information like database credentials, collection names, and API keys. Instead of hardcoding these details into the script, we request the user to provide them at runtime, ensuring flexibility and security.\n",
    "\n",
    "The script also validates that all required inputs are provided, raising an error if any crucial information is missing. This approach ensures that your integration is both secure and correctly configured without hardcoding sensitive information, enhancing the overall security and maintainability of your code.\n",
    "\n",
    "**CAPELLA_MODEL_SERVICES_ENDPOINT** is the Capella AI Services endpoint found in the models section.\n",
    "> Note that the Capella Model Services Endpoint also requires an additional `/v1` from the endpoint shown on the UI if it is not shown on the UI.\n",
    "\n",
    "**INDEX_NAME** is the name of the Hyperscale or Composite Vector Index we will create for vector search operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_CONNECTION_STRING = input(\"Couchbase Cluster URL (default: localhost): \") or \"couchbase://localhost\"\n",
    "CB_USERNAME = input(\"Couchbase Username (default: admin): \") or \"admin\"\n",
    "CB_PASSWORD = getpass.getpass(\"Couchbase password (default: Password@12345): \") or \"Password@12345\"\n",
    "CB_BUCKET_NAME = input(\"Couchbase Bucket: \")\n",
    "SCOPE_NAME = input(\"Couchbase Scope: \")\n",
    "COLLECTION_NAME = input(\"Couchbase Collection: \")\n",
    "INDEX_NAME = input(\"Vector Search Index: \")\n",
    "\n",
    "# Get Capella AI endpoint\n",
    "CAPELLA_MODEL_SERVICES_ENDPOINT = input(\"Enter your Capella Model Services Endpoint: \")\n",
    "LLM_MODEL_NAME = input(\"Enter the LLM name: \")\n",
    "LLM_API_KEY = getpass.getpass(\"Enter your Capella Model Services LLM API Key: \")\n",
    "EMBEDDING_MODEL_NAME = input(\"Enter the Embedding Model name: \")\n",
    "EMBEDDING_API_KEY = getpass.getpass(\"Enter your Capella Model Services Embedding Model API Key: \")\n",
    "EMBEDDING_DIMENSION = input(\"Enter the Embedding Dimension (e.g. 3072, 4096): \") or \"3072\"\n",
    "\n",
    "# Check if the variables are correctly loaded\n",
    "if not all([CB_CONNECTION_STRING, CB_USERNAME, CB_PASSWORD, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME, INDEX_NAME, CAPELLA_MODEL_SERVICES_ENDPOINT, LLM_MODEL_NAME, LLM_API_KEY, EMBEDDING_MODEL_NAME, EMBEDDING_API_KEY]):\n",
    "    raise ValueError(\"All configuration variables must be provided.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Logging\n",
    "Logging is essential for tracking the execution of our script and debugging any issues that may arise. We set up a logger that will display information about the script's progress, including timestamps and log levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to Couchbase Capella\n",
    "The next step is to establish a connection to our Couchbase Capella cluster. This connection will allow us to interact with the database, store and retrieve documents, and perform vector searches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:56:30,321 - INFO - Successfully connected to the Couchbase cluster\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Initialize the Couchbase Cluster\n",
    "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "    options = ClusterOptions(auth)\n",
    "    options.apply_profile(KnownConfigProfiles.WanDevelopment)\n",
    "    \n",
    "    # Connect to the cluster\n",
    "    cluster = Cluster(CB_CONNECTION_STRING, options)\n",
    "    \n",
    "    # Wait for the cluster to be ready\n",
    "    cluster.wait_until_ready(timedelta(seconds=5))\n",
    "    logging.info(\"Successfully connected to the Couchbase cluster\")\n",
    "except CouchbaseException as e:\n",
    "    raise RuntimeError(f\"Failed to connect to Couchbase: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Bucket, Scope, and Collection\n",
    "Before we can store our data, we need to ensure that the appropriate bucket, scope, and collection exist in our Couchbase cluster. The code below checks if these components exist and creates them if they don't, providing a foundation for storing our vector embeddings and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'test_bucket' already exists.\n",
      "Scope 'test_scope' already exists.\n",
      "Collection 'test_collection1' does not exist in scope 'test_scope'. Creating collection...\n",
      "Collection 'test_collection1' created successfully.\n"
     ]
    }
   ],
   "source": [
    "from couchbase.management.buckets import CreateBucketSettings\n",
    "import json\n",
    "\n",
    "# Create bucket if it does not exist\n",
    "bucket_manager = cluster.buckets()\n",
    "try:\n",
    "    bucket_manager.get_bucket(CB_BUCKET_NAME)\n",
    "    print(f\"Bucket '{CB_BUCKET_NAME}' already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Bucket '{CB_BUCKET_NAME}' does not exist. Creating bucket...\")\n",
    "    bucket_settings = CreateBucketSettings(name=CB_BUCKET_NAME, ram_quota_mb=500)\n",
    "    bucket_manager.create_bucket(bucket_settings)\n",
    "    print(f\"Bucket '{CB_BUCKET_NAME}' created successfully.\")\n",
    "\n",
    "# Create scope and collection if they do not exist\n",
    "collection_manager = cluster.bucket(CB_BUCKET_NAME).collections()\n",
    "scopes = collection_manager.get_all_scopes()\n",
    "scope_exists = any(scope.name == SCOPE_NAME for scope in scopes)\n",
    "\n",
    "if scope_exists:\n",
    "    print(f\"Scope '{SCOPE_NAME}' already exists.\")\n",
    "else:\n",
    "    print(f\"Scope '{SCOPE_NAME}' does not exist. Creating scope...\")\n",
    "    collection_manager.create_scope(SCOPE_NAME)\n",
    "    print(f\"Scope '{SCOPE_NAME}' created successfully.\")\n",
    "\n",
    "collections = [collection.name for scope in scopes if scope.name == SCOPE_NAME for collection in scope.collections]\n",
    "collection_exists = COLLECTION_NAME in collections\n",
    "\n",
    "if collection_exists:\n",
    "    print(f\"Collection '{COLLECTION_NAME}' already exists in scope '{SCOPE_NAME}'.\")\n",
    "else:\n",
    "    print(f\"Collection '{COLLECTION_NAME}' does not exist in scope '{SCOPE_NAME}'. Creating collection...\")\n",
    "    collection_manager.create_collection(collection_name=COLLECTION_NAME, scope_name=SCOPE_NAME)\n",
    "    print(f\"Collection '{COLLECTION_NAME}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the BBC News Dataset\n",
    "To build a RAG engine, we need data to search through. We use the [BBC Realtime News dataset](https://huggingface.co/datasets/RealTimeData/bbc_news_alltime), a dataset with up-to-date BBC news articles grouped by month. This dataset contains articles that were created after the LLM was trained. It will showcase the use of RAG to augment the LLM. \n",
    "\n",
    "The BBC News dataset's varied content allows us to simulate real-world scenarios where users ask complex questions, enabling us to fine-tune our RAG's ability to understand and respond to various types of queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the BBC News dataset with 2687 rows\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    news_dataset = load_dataset('RealTimeData/bbc_news_alltime', '2024-12', split=\"train\")\n",
    "    print(f\"Loaded the BBC News dataset with {len(news_dataset)} rows\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading BBC News dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: ['title', 'published_date', 'authors', 'description', 'section', 'content', 'link', 'top_image']\n",
      "\n",
      "First two examples:\n",
      "{'title': [\"Pakistan protest: Bushra Bibi's march for Imran Khan disappeared - BBC News\", 'Lockdown DIY linked to Walleys Quarry gases - BBC News'], 'published_date': ['2024-12-01', '2024-12-01'], 'authors': ['https://www.facebook.com/bbcnews', 'https://www.facebook.com/bbcnews'], 'description': [\"Imran Khan's third wife guided protesters to the heart of the capital - and then disappeared.\", 'An academic says an increase in plasterboard sent to landfill could be behind a spike in smells.'], 'section': ['Asia', 'Stoke & Staffordshire'], 'content': ['Bushra Bibi led a protest to free Imran Khan - what happened next is a mystery\\n\\nImran Khan\\'s wife, Bushra Bibi, encouraged protesters into the heart of Pakistan\\'s capital, Islamabad\\n\\nA charred lorry, empty tear gas shells and posters of former Pakistan Prime Minister Imran Khan - it was all that remained of a massive protest led by Khan’s wife, Bushra Bibi, that had sent the entire capital into lockdown. Just a day earlier, faith healer Bibi - wrapped in a white shawl, her face covered by a white veil - stood atop a shipping container on the edge of the city as thousands of her husband’s devoted followers waved flags and chanted slogans beneath her. It was the latest protest to flare since Khan, the 72-year-old cricketing icon-turned-politician, was jailed more than a year ago after falling foul of the country\\'s influential military which helped catapult him to power. “My children and my brothers! You have to stand with me,” Bibi cried on Tuesday afternoon, her voice cutting through the deafening roar of the crowd. “But even if you don’t,” she continued, “I will still stand firm. “This is not just about my husband. It is about this country and its leader.” It was, noted some watchers of Pakistani politics, her political debut. But as the sun rose on Wednesday morning, there was no sign of Bibi, nor the thousands of protesters who had marched through the country to the heart of the capital, demanding the release of their jailed leader. While other PMs have fallen out with Pakistan\\'s military in the past, Khan\\'s refusal to stay quiet behind bars is presenting an extraordinary challenge - escalating the standoff and leaving the country deeply divided. Exactly what happened to the so-called “final march”, and Bibi, when the city went dark is still unclear. All eyewitnesses like Samia* can say for certain is that the lights went out suddenly, plunging D Chowk, the square where they had gathered, into blackness.\\n\\nWithin a day of arriving, the protesters had scattered - leaving behind Bibi\\'s burnt-out vehicle\\n\\nAs loud screams and clouds of tear gas blanketed the square, Samia describes holding her husband on the pavement, bloodied from a gun shot to his shoulder. \"Everyone was running for their lives,\" she later told BBC Urdu from a hospital in Islamabad, adding it was \"like doomsday or a war\". \"His blood was on my hands and the screams were unending.” But how did the tide turn so suddenly and decisively? Just hours earlier, protesters finally reached D Chowk late afternoon on Tuesday. They had overcome days of tear gas shelling and a maze of barricaded roads to get to the city centre. Many of them were supporters and workers of the Pakistan Tehreek-e-Insaf (PTI), the party led by Khan. He had called for the march from his jail cell, where he has been for more than a year on charges he says are politically motivated. Now Bibi - his third wife, a woman who had been largely shrouded in mystery and out of public view since their unexpected wedding in 2018 - was leading the charge. “We won’t go back until we have Khan with us,” she declared as the march reached D Chowk, deep in the heart of Islamabad’s government district.\\n\\nThousands had marched for days to reach Islamabad, demanding former Prime Minister Imran Khan be released from jail\\n\\nInsiders say even the choice of destination - a place where her husband had once led a successful sit in - was Bibi’s, made in the face of other party leader’s opposition, and appeals from the government to choose another gathering point. Her being at the forefront may have come as a surprise. Bibi, only recently released from prison herself, is often described as private and apolitical. Little is known about her early life, apart from the fact she was a spiritual guide long before she met Khan. Her teachings, rooted in Sufi traditions, attracted many followers - including Khan himself. Was she making her move into politics - or was her sudden appearance in the thick of it a tactical move to keep Imran Khan’s party afloat while he remains behind bars? For critics, it was a move that clashed with Imran Khan’s oft-stated opposition to dynastic politics. There wasn’t long to mull the possibilities. After the lights went out, witnesses say that police started firing fresh rounds of tear gas at around 21:30 local time (16:30 GMT). The crackdown was in full swing just over an hour later. At some point, amid the chaos, Bushra Bibi left. Videos on social media appeared to show her switching cars and leaving the scene. The BBC couldn’t verify the footage. By the time the dust settled, her container had already been set on fire by unknown individuals. By 01:00 authorities said all the protesters had fled.\\n\\nSecurity was tight in the city, and as night fell, lights were switched off - leaving many in the dark as to what exactly happened next\\n\\nEyewitnesses have described scenes of chaos, with tear gas fired and police rounding up protesters. One, Amin Khan, said from behind an oxygen mask that he joined the march knowing that, \"either I will bring back Imran Khan or I will be shot\". The authorities have have denied firing at the protesters. They also said some of the protesters were carrying firearms. The BBC has seen hospital records recording patients with gunshot injuries. However, government spokesperson Attaullah Tarar told the BBC that hospitals had denied receiving or treating gunshot wound victims. He added that \"all security personnel deployed on the ground have been forbidden\" from having live ammunition during protests. But one doctor told BBC Urdu that he had never done so many surgeries for gunshot wounds in a single night. \"Some of the injured came in such critical condition that we had to start surgery right away instead of waiting for anaesthesia,\" he said. While there has been no official toll released, the BBC has confirmed with local hospitals that at least five people have died. Police say at least 500 protesters were arrested that night and are being held in police stations. The PTI claims some people are missing. And one person in particular hasn’t been seen in days: Bushra Bibi.\\n\\nThe next morning, the protesters were gone - leaving behind just wrecked cars and smashed glass\\n\\nOthers defended her. “It wasn’t her fault,” insisted another. “She was forced to leave by the party leaders.” Political commentators have been more scathing. “Her exit damaged her political career before it even started,” said Mehmal Sarfraz, a journalist and analyst. But was that even what she wanted? Khan has previously dismissed any thought his wife might have her own political ambitions - “she only conveys my messages,” he said in a statement attributed to him on his X account.\\n\\nImran Khan and Bushra Bibi, pictured here arriving at court in May 2023, married in 2018\\n\\nSpeaking to BBC Urdu, analyst Imtiaz Gul calls her participation “an extraordinary step in extraordinary circumstances\". Gul believes Bushra Bibi’s role today is only about “keeping the party and its workers active during Imran Khan’s absence”. It is a feeling echoed by some PTI members, who believe she is “stepping in only because Khan trusts her deeply”. Insiders, though, had often whispered that she was pulling the strings behind the scenes - advising her husband on political appointments and guiding high-stakes decisions during his tenure. A more direct intervention came for the first time earlier this month, when she urged a meeting of PTI leaders to back Khan’s call for a rally. Pakistan’s defence minister Khawaja Asif accused her of “opportunism”, claiming she sees “a future for herself as a political leader”. But Asma Faiz, an associate professor of political science at Lahore University of Management Sciences, suspects the PTI’s leadership may have simply underestimated Bibi. “It was assumed that there was an understanding that she is a non-political person, hence she will not be a threat,” she told the AFP news agency. “However, the events of the last few days have shown a different side of Bushra Bibi.” But it probably doesn’t matter what analysts and politicians think. Many PTI supporters still see her as their connection to Imran Khan. It was clear her presence was enough to electrify the base. “She is the one who truly wants to get him out,” says Asim Ali, a resident of Islamabad. “I trust her. Absolutely!”', 'Walleys Quarry was ordered not to accept any new waste as of Friday\\n\\nA chemist and former senior lecturer in environmental sustainability has said powerful odours from a controversial landfill site may be linked to people doing more DIY during the Covid-19 pandemic. Complaints about Walleys Quarry in Silverdale, Staffordshire – which was ordered to close as of Friday – increased significantly during and after coronavirus lockdowns. Issuing the closure notice, the Environment Agency described management of the site as poor, adding it had exhausted all other enforcement tactics at premises where gases had been noxious and periodically above emission level guidelines - which some campaigners linked to ill health locally. Dr Sharon George, who used to teach at Keele University, said she had been to the site with students and found it to be clean and well-managed, and suggested an increase in plasterboard heading to landfills in 2020 could be behind a spike in stenches.\\n\\n“One of the materials that is particularly bad for producing odours and awful emissions is plasterboard,\" she said. “That’s one of the theories behind why Walleys Quarry got worse at that time.” She said the landfill was in a low-lying area, and that some of the gases that came from the site were quite heavy. “They react with water in the atmosphere, so some of the gases you smell can be quite awful and not very good for our health. “It’s why, on some days when it’s colder and muggy and a bit misty, you can smell it more.” Dr George added: “With any landfill, you’re putting things into the ground – and when you put things into the ground, if they can they will start to rot. When they start to rot they’re going to give off gases.” She believed Walleys Quarry’s proximity to people’s homes was another major factor in the amount of complaints that arose from its operation. “If you’ve got a gas that people can smell, they’re going to report it much more than perhaps a pollutant that might go unnoticed.”\\n\\nRebecca Currie said she did not think the site would ever be closed\\n\\nLocal resident and campaigner Rebecca Currie said the closure notice served to Walleys Quarry was \"absolutely amazing\". Her son Matthew has had breathing difficulties after being born prematurely with chronic lung disease, and Ms Currie says the site has made his symptoms worse. “I never thought this day was going to happen,” she explained. “We fought and fought for years.” She told BBC Midlands Today: “Our community have suffered. We\\'ve got kids who are really poorly, people have moved homes.”\\n\\nComplaints about Walleys Quarry to Newcastle-under-Lyme Borough Council exceeded 700 in November, the highest amount since 2021 according to council leader Simon Tagg. The Environment Agency (EA), which is responsible for regulating landfill sites, said it had concluded further operation at the site could result in \"significant long-term pollution\". A spokesperson for Walley\\'s Quarry Ltd said the firm rejected the EA\\'s accusations of poor management, and would be challenging the closure notice. Dr George said she believed the EA was likely to be erring on the side of caution and public safety, adding safety standards were strict. She said a lack of landfill space in the country overall was one of the broader issues that needed addressing. “As people, we just keep using stuff and then have nowhere to put it, and then when we end up putting it in places like Walleys Quarry that is next to houses, I think that’s where the problems are.”\\n\\nTell us which stories we should cover in Staffordshire'], 'link': ['http://www.bbc.co.uk/news/articles/cvg02lvj1e7o', 'http://www.bbc.co.uk/news/articles/c5yg1v16nkpo'], 'top_image': ['https://ichef.bbci.co.uk/ace/standard/3840/cpsprodpb/9975/live/b22229e0-ad5a-11ef-83bc-1153ed943d1c.jpg', 'https://ichef.bbci.co.uk/ace/standard/3840/cpsprodpb/0896/live/55209f80-adb2-11ef-8f6c-f1a86bb055ec.jpg']}\n"
     ]
    }
   ],
   "source": [
    "# Print the first two examples from the dataset\n",
    "print(\"Dataset columns:\", news_dataset.column_names)\n",
    "print(\"\\nFirst two examples:\")\n",
    "print(news_dataset[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for RAG\n",
    "\n",
    "We need to extract the context passages from the dataset to use as our knowledge base for the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1749 unique articles in our database.\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "news_articles = news_dataset\n",
    "unique_articles = {}\n",
    "\n",
    "for article in news_articles:\n",
    "    content = article.get(\"content\")\n",
    "    if content:\n",
    "        content_hash = hashlib.md5(content.encode()).hexdigest()  # Generate hash of content\n",
    "        if content_hash not in unique_articles:\n",
    "            unique_articles[content_hash] = article  # Store full article\n",
    "\n",
    "unique_news_articles = list(unique_articles.values())  # Convert back to list\n",
    "\n",
    "print(f\"We have {len(unique_news_articles)} unique articles in our database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Embeddings using Capella Model Services\n",
    "Embeddings are numerical representations of text that capture semantic meaning. Unlike keyword-based search, embeddings enable semantic search to understand context and retrieve documents that are conceptually similar even without exact keyword matches. We'll use the model deployed on Capella Model Services to create high-quality embeddings. This model transforms our text data into vector representations that can be efficiently searched using Haystack's OpenAI document embedder (configured to point to Capella).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created embedding models\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Set up the document embedder for processing documents\n",
    "    document_embedder = OpenAIDocumentEmbedder(\n",
    "        api_base_url=CAPELLA_MODEL_SERVICES_ENDPOINT,\n",
    "        api_key=Secret.from_token(EMBEDDING_API_KEY),\n",
    "        model=EMBEDDING_MODEL_NAME\n",
    "    )\n",
    "    \n",
    "    # Set up the text embedder for query processing\n",
    "    rag_embedder = OpenAITextEmbedder(\n",
    "        api_base_url=CAPELLA_MODEL_SERVICES_ENDPOINT,\n",
    "        api_key=Secret.from_token(EMBEDDING_API_KEY),\n",
    "        model=EMBEDDING_MODEL_NAME\n",
    "    )\n",
    "    \n",
    "    print(\"Successfully created embedding models\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating embedding models: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Embeddings Model\n",
    "We can test the text embeddings model by generating an embedding for a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:56:54,817 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Embedding dimension: 1024\n"
     ]
    }
   ],
   "source": [
    "test_result = rag_embedder.run(text=\"this is a test sentence\")\n",
    "test_embedding = test_result[\"embedding\"]\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Couchbase Vector Document Store\n",
    "The `CouchbaseQueryDocumentStore` from the `couchbase_haystack` package provides seamless integration with Couchbase, supporting both Hyperscale and Composite Vector Indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Couchbase vector document store\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Create the Couchbase vector document store\n",
    "    document_store = CouchbaseQueryDocumentStore(\n",
    "        cluster_connection_string=Secret.from_token(CB_CONNECTION_STRING),\n",
    "        authenticator=CouchbasePasswordAuthenticator(\n",
    "            username=Secret.from_token(CB_USERNAME),\n",
    "            password=Secret.from_token(CB_PASSWORD)\n",
    "        ),\n",
    "        cluster_options=CouchbaseClusterOptions(\n",
    "            profile=KnownConfigProfiles.WanDevelopment,\n",
    "        ),\n",
    "        bucket=CB_BUCKET_NAME,\n",
    "        scope=SCOPE_NAME,\n",
    "        collection=COLLECTION_NAME,\n",
    "        search_type=QueryVectorSearchType.ANN,\n",
    "        similarity=QueryVectorSearchSimilarity.COSINE\n",
    "    )\n",
    "    print(\"Successfully created Couchbase vector document store\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to create Couchbase vector document store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Haystack Documents\n",
    "In this section, we'll process our news articles and create Haystack Document objects.\n",
    "Each Document is created with specific metadata that will be used for retrieval and generation.\n",
    "We'll observe examples of the document content to understand how the documents are structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document content preview:\n",
      "Content: Bushra Bibi led a protest to free Imran Khan - what happened next is a mystery\n",
      "\n",
      "Imran Khan's wife, Bushra Bibi, encouraged protesters into the heart of Pakistan's capital, Islamabad\n",
      "\n",
      "A charred lorry, ...\n",
      "Metadata: {'title': \"Pakistan protest: Bushra Bibi's march for Imran Khan disappeared - BBC News\", 'description': \"Imran Khan's third wife guided protesters to the heart of the capital - and then disappeared.\", 'published_date': '2024-12-01', 'link': 'http://www.bbc.co.uk/news/articles/cvg02lvj1e7o'}\n",
      "Created 1749 documents\n"
     ]
    }
   ],
   "source": [
    "haystack_documents = []\n",
    "# Process and store documents\n",
    "for article in unique_news_articles:  # Process all unique articles\n",
    "    try:\n",
    "        document = Document(\n",
    "            content=article[\"content\"],\n",
    "            meta={\n",
    "                \"title\": article[\"title\"],\n",
    "                \"description\": article[\"description\"],\n",
    "                \"published_date\": article[\"published_date\"],\n",
    "                \"link\": article[\"link\"],\n",
    "            }\n",
    "        )\n",
    "        haystack_documents.append(document)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create document: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Observing an example of the document content\n",
    "print(\"Document content preview:\")\n",
    "print(f\"Content: {haystack_documents[0].content[:200]}...\")\n",
    "print(f\"Metadata: {haystack_documents[0].meta}\")\n",
    "\n",
    "print(f\"Created {len(haystack_documents)} documents\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Running the Indexing Pipeline\n",
    "\n",
    "In this section, we'll create an indexing pipeline to process our documents. The pipeline will:\n",
    "\n",
    "1. DocumentCleaner - Cleans and preprocesses the raw Haystack documents (removes extra whitespace, normalizes text)\n",
    "2. document_embedder - Generates vector embeddings for each document using an embedding model (likely OpenAI's), converting text into numerical representations for semantic search\n",
    "3. DocumentWriter - Writes the cleaned documents along with their embeddings to the Couchbase document store\n",
    "\n",
    "This transforms raw news articles into searchable vector representations stored in Couchbase for later semantic retrieval in the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x120a32ba0>\n",
       "🚅 Components\n",
       "  - cleaner: DocumentCleaner\n",
       "  - embedder: OpenAIDocumentEmbedder\n",
       "  - writer: DocumentWriter\n",
       "🛤️ Connections\n",
       "  - cleaner.documents -> embedder.documents (list[Document])\n",
       "  - embedder.documents -> writer.documents (list[Document])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Process documents: split into chunks, generate embeddings, and store in document store\n",
    "# Create indexing pipeline\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"cleaner\", DocumentCleaner())\n",
    "indexing_pipeline.add_component(\"embedder\", document_embedder)\n",
    "indexing_pipeline.add_component(\"writer\", DocumentWriter(document_store=document_store))\n",
    "\n",
    "indexing_pipeline.connect(\"cleaner.documents\", \"embedder.documents\")\n",
    "indexing_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Indexing Pipeline\n",
    "\n",
    "Execute the pipeline for processing and indexing BCC news documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:07,785 - INFO - Running component cleaner\n",
      "2025-12-10 10:57:07,850 - INFO - Running component embedder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:10,449 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 1it [00:03,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:12,799 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 2it [00:05,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:14,281 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 3it [00:06,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:15,540 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 4it [00:07,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:16,771 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 5it [00:09,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:17,851 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 6it [00:10,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:19,012 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 7it [00:11,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:20,213 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 8it [00:12,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:21,204 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 9it [00:13,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:22,235 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 10it [00:14,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:23,327 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 11it [00:15,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:24,259 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 12it [00:16,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:25,438 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 13it [00:17,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:26,443 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 14it [00:18,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:27,477 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 15it [00:19,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:28,558 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 16it [00:20,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:29,766 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 17it [00:21,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:30,826 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 18it [00:23,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:32,114 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 19it [00:24,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:33,192 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 20it [00:25,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:34,193 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 21it [00:26,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:35,211 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 22it [00:27,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:36,320 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 23it [00:28,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:37,558 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 24it [00:29,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:38,642 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 25it [00:30,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:39,724 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 26it [00:31,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:40,759 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 27it [00:32,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:41,729 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 28it [00:33,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:42,757 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 29it [00:34,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:43,893 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 30it [00:36,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:44,915 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 31it [00:37,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:46,140 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 32it [00:38,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:47,208 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 33it [00:39,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:48,507 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 34it [00:40,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:49,584 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 35it [00:41,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:50,795 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 36it [00:43,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:51,657 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 37it [00:43,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:52,327 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 38it [00:44,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:57:52,617 - INFO - Running component writer\n",
      "Indexed 1200 document chunks\n"
     ]
    }
   ],
   "source": [
    "# Run the indexing pipeline\n",
    "if haystack_documents:\n",
    "    result = indexing_pipeline.run({\"cleaner\": {\"documents\": haystack_documents[:1200]}})\n",
    "    print(f\"Indexed {result['writer']['documents_written']} document chunks\")\n",
    "else:\n",
    "    print(\"No documents created. Skipping indexing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Capella Model Services Large Language Model (LLM)\n",
    "Large language models are AI systems that are trained to understand and generate human language. We'll be using the model deployed on Capella Model Services to process user queries and generate meaningful responses based on the retrieved context from our Couchbase document store. This model is a key component of our RAG system, allowing it to go beyond simple keyword matching and truly understand the intent behind a query. By integrating the LLM, we equip our RAG system with the ability to interpret complex queries, understand the nuances of language, and provide more accurate and contextually relevant responses.\n",
    "\n",
    "The language model's ability to understand context and generate coherent responses is what makes our RAG system truly intelligent. It can not only find the right information but also present it in a way that is useful and understandable to the user.\n",
    "\n",
    "The LLM is configured using Haystack's OpenAI generator component with your Capella Model Services API key for seamless integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:58:02,687 - INFO - Successfully created the generator\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Set up the LLM generator\n",
    "    generator = OpenAIGenerator(\n",
    "        api_base_url=CAPELLA_MODEL_SERVICES_ENDPOINT,\n",
    "        api_key=Secret.from_token(LLM_API_KEY),\n",
    "        model=LLM_MODEL_NAME\n",
    "    )\n",
    "    logging.info(\"Successfully created the generator\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating generator: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the RAG Pipeline\n",
    "\n",
    "In this section, we'll create a RAG pipeline using Haystack components. This pipeline serves as the foundation for our RAG system, enabling semantic search capabilities and efficient retrieval of relevant information.\n",
    "\n",
    "The RAG pipeline provides a complete workflow that allows us to:\n",
    "1. Perform semantic searches based on user queries\n",
    "2. Retrieve the most relevant documents or chunks\n",
    "3. Generate contextually appropriate responses using our LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:58:08,327 - WARNING - PromptBuilder has 2 prompt variables, but `required_variables` is not set. By default, all prompt variables are treated as optional, which may lead to unintended behavior in multi-branch pipelines. To avoid unexpected execution, ensure that variables intended to be required are explicitly set in `required_variables`.\n",
      "Successfully created RAG pipeline\n"
     ]
    }
   ],
   "source": [
    "# Define RAG prompt template\n",
    "prompt_template = \"\"\"\n",
    "Given these documents, answer the question.\\nDocuments:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "\\nQuestion: {{question}}\n",
    "\\nAnswer:\n",
    "\"\"\"\n",
    "\n",
    "# Create the RAG pipeline\n",
    "rag_pipeline = Pipeline()\n",
    "\n",
    "# Add components to the pipeline\n",
    "rag_pipeline.add_component(\n",
    "    \"query_embedder\",\n",
    "    rag_embedder,\n",
    ")\n",
    "rag_pipeline.add_component(\"retriever\", CouchbaseQueryEmbeddingRetriever(document_store=document_store))\n",
    "rag_pipeline.add_component(\"prompt_builder\", PromptBuilder(template=prompt_template))\n",
    "rag_pipeline.add_component(\"llm\",generator)\n",
    "rag_pipeline.add_component(\"answer_builder\", AnswerBuilder())\n",
    "\n",
    "# Connect RAG components\n",
    "rag_pipeline.connect(\"query_embedder\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.prompt\")\n",
    "rag_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
    "rag_pipeline.connect(\"llm.meta\", \"answer_builder.meta\")\n",
    "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")\n",
    "\n",
    "print(\"Successfully created RAG pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) with Couchbase and Haystack\n",
    "\n",
    "Let's test our RAG system by performing a semantic search on a sample query. In this example, we'll use a question about Pep Guardiola's reaction to Manchester City's recent form. The RAG system will:\n",
    "\n",
    "1. Process the natural language query\n",
    "2. Search through our document store for relevant information\n",
    "3. Retrieve the most semantically similar documents\n",
    "4. Generate a comprehensive response using the LLM\n",
    "\n",
    "This demonstrates how our system combines the power of vector search with language model capabilities to provide accurate, contextual answers based on the information in our database.\n",
    "\n",
    "**Note:** By default, without any Hyperscale or Composite Vector Index, Couchbase falls back to linear brute-force search that compares the query vector against every document in the collection. This works for small datasets but can become slow as the dataset grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:58:26,463 - INFO - Running component query_embedder\n",
      "2025-12-10 10:58:27,722 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-10 10:58:27,732 - INFO - Running component retriever\n",
      "2025-12-10 10:58:28,270 - INFO - Running component prompt_builder\n",
      "2025-12-10 10:58:28,271 - INFO - Running component llm\n",
      "2025-12-10 10:58:32,097 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-10 10:58:32,102 - INFO - Running component answer_builder\n",
      "=== Retrieved Documents ===\n",
      "Id: b0cdcffd58641d16dacd0b2659fb7e8d87613ba576940296d12f6621dac1d1ea Title: Man City 1-2 Man Utd: Crisis-hit Pep Guardiola faces huge rebuild - BBC Sport\n",
      "Id: 00a58764c0dee414e19030793892ea723cc100de631c3a55cd8e5d0329d38a63 Title: Man City lose to Aston Villa: Pep Guardiola says struggling champions 'have to find a way' to win again - BBC Sport\n",
      "Id: feca66cdb254c478782824573020df44c2a189a46a7f49478d971b73143b0abd Title: Manchester City boss Pep Guardiola on management pressures before Manchester United game - BBC Sport\n",
      "Id: 04340f3dcbb975fe534f721282b6850bdb3133b9a7857fddce90d6cfc1743cad Title: Pep Guardiola: 'Self-doubt, errors and big changes - inside crisis at Man City' - BBC Sport\n",
      "Id: 08a6a1fd6af16da19840e8e38b131605814a77fe273b30384fa39f34f973e3a4 Title: Liverpool 2-0 Manchester City: Arne Slot reaction after win - BBC Sport\n",
      "\n",
      "=== Final Answer ===\n",
      "Question: What was Pep Guardiola's reaction to Manchester City's current form?\n",
      "Answer: Pep Guardiola expressed concern and frustration over Manchester City's current form, admitting his sleep had suffered as a result. He stated that he is not good enough to resolve the situation with the group of players available, and that they have to find a way back to their previous standard, step by step. Guardiola also acknowledged that there are many factors contributing to their downturn in form and expressed the need for solutions and improvements.\n",
      "\n",
      "Sources:\n",
      "-> Man City 1-2 Man Utd: Crisis-hit Pep Guardiola faces huge rebuild - BBC Sport\n",
      "-> Man City lose to Aston Villa: Pep Guardiola says struggling champions 'have to find a way' to win again - BBC Sport\n",
      "-> Manchester City boss Pep Guardiola on management pressures before Manchester United game - BBC Sport\n",
      "-> Pep Guardiola: 'Self-doubt, errors and big changes - inside crisis at Man City' - BBC Sport\n",
      "-> Liverpool 2-0 Manchester City: Arne Slot reaction after win - BBC Sport\n",
      "\n",
      "Linear Vector Search Results (completed in 5.64 seconds):\n"
     ]
    }
   ],
   "source": [
    "# Sample query from the dataset\n",
    "\n",
    "query = \"What was Pep Guardiola's reaction to Manchester City's current form?\"\n",
    "\n",
    "try:\n",
    "    # Perform the semantic search using the RAG pipeline\n",
    "    start_time = time.time()\n",
    "    result = rag_pipeline.run({\n",
    "        \"query_embedder\": {\"text\": query},\n",
    "        \"retriever\": {\"top_k\": 5},\n",
    "        \"prompt_builder\": {\"question\": query},\n",
    "        \"answer_builder\": {\"query\": query},\n",
    "        },\n",
    "     include_outputs_from={\"retriever\", \"query_embedder\"}\n",
    "    )\n",
    "    search_elapsed_time = time.time() - start_time\n",
    "    # Get the generated answer\n",
    "    answer: GeneratedAnswer = result[\"answer_builder\"][\"answers\"][0]\n",
    "\n",
    "    # Print retrieved documents\n",
    "    print(\"=== Retrieved Documents ===\")\n",
    "    retrieved_docs = result[\"retriever\"][\"documents\"]\n",
    "    for idx, doc in enumerate(retrieved_docs, start=1):\n",
    "        print(f\"Id: {doc.id} Title: {doc.meta['title']}\")\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\n=== Final Answer ===\")\n",
    "    print(f\"Question: {answer.query}\")\n",
    "    print(f\"Answer: {answer.data}\")\n",
    "    print(\"\\nSources:\")\n",
    "    for doc in answer.documents:\n",
    "        print(f\"-> {doc.meta['title']}\")\n",
    "    # Display search results\n",
    "    print(f\"\\nLinear Vector Search Results (completed in {search_elapsed_time:.2f} seconds):\")\n",
    "    #print(result[\"generator\"][\"replies\"][0])\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error performing RAG search: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Hyperscale or Composite Vector Indexes\n",
    "\n",
    "While the above RAG system works effectively, you can significantly improve query performance by enabling Couchbase Capella's Hyperscale or Composite Vector Indexes.\n",
    "\n",
    "## Hyperscale Vector Indexes\n",
    "- Specifically designed for vector searches\n",
    "- Perform vector similarity and semantic searches faster than other index types\n",
    "- Scale to billions of vectors while keeping most of the structure in an optimized on-disk format\n",
    "- Maintain high accuracy even for vectors with a large number of dimensions\n",
    "- Support concurrent searches and inserts for constantly changing datasets\n",
    "\n",
    "Use this type of index when you primarily query vector values and need low-latency similarity search at scale. In general, Hyperscale Vector Indexes are the best starting point for most vector search workloads.\n",
    "\n",
    "## Composite Vector Indexes\n",
    "- Combine scalar filters with a single vector column in the same index definition\n",
    "- Designed for searches that apply one vector value alongside scalar attributes that remove large portions of the dataset before similarity scoring\n",
    "- Consume a moderate amount of memory and can index Tens of million to billion of documents\n",
    "- Excel when your queries must return a small, highly targeted result set\n",
    "\n",
    "Use Composite Vector Indexes when you want to perform searches that blend scalar predicates and vector similarity so that the scalar filters tighten the candidate set.\n",
    "\n",
    "For an in-depth comparison and tuning guidance, review the [Couchbase vector index documentation](https://docs.couchbase.com/cloud/vector-index/use-vector-indexes.html) and the [overview of Capella vector indexes](https://docs.couchbase.com/cloud/vector-index/vectors-and-indexes-overview.html).\n",
    "\n",
    "## Understanding Index Configuration (Couchbase 8.0 Feature)\n",
    "\n",
    "The `index_description` parameter controls how Couchbase optimizes vector storage and search performance through centroids and quantization:\n",
    "\n",
    "Format: `'IVF[<centroids>],{PQ|SQ}<settings>'`\n",
    "\n",
    "**Centroids (IVF - Inverted File):**\n",
    "- Controls how the dataset is subdivided for faster searches\n",
    "- More centroids = faster search, slower training  \n",
    "- Fewer centroids = slower search, faster training\n",
    "- If omitted (like `IVF,SQ8`), Couchbase auto-selects based on dataset size\n",
    "\n",
    "**Quantization Options:**\n",
    "- SQ (Scalar Quantization): `SQ4`, `SQ6`, `SQ8` (4, 6, or 8 bits per dimension)\n",
    "- PQ (Product Quantization): `PQ<subquantizers>x<bits>` (e.g., `PQ32x8`)\n",
    "- Higher values = better accuracy, larger index size\n",
    "\n",
    "**Common Examples:**\n",
    "- `IVF,SQ8` – Auto centroids, 8-bit scalar quantization (good default)\n",
    "- `IVF1000,SQ6` – 1000 centroids, 6-bit scalar quantization  \n",
    "- `IVF,PQ32x8` – Auto centroids, 32 subquantizers with 8 bits\n",
    "\n",
    "For detailed configuration options, see the [Quantization & Centroid Settings](https://docs.couchbase.com/server/current/vector-index/hyperscale-vector-index.html#algo_settings).\n",
    "\n",
    "In the code below, we demonstrate creating a Hyperscale index for optimal performance. You can adapt the same flow to create a COMPOSITE index by replacing the index type and options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Hyperscale index: vector_search_hyperscale\n"
     ]
    }
   ],
   "source": [
    "# Create a Hyperscale Vector Index for optimized vector search\n",
    "try:\n",
    "    hyperscale_index_name = f\"{INDEX_NAME}_hyperscale\"\n",
    "\n",
    "    # Use the cluster connection to create the Hyperscale index\n",
    "    scope = cluster.bucket(CB_BUCKET_NAME).scope(SCOPE_NAME)\n",
    "    \n",
    "    options = {\n",
    "        \"dimension\": int(EMBEDDING_DIMENSION),  # dimension based on the model\n",
    "        \"similarity\": \"cosine\",\n",
    "        \"scan_nprobes\": 3,\n",
    "    }\n",
    "    \n",
    "    scope.query(\n",
    "        f\"\"\"\n",
    "        CREATE VECTOR INDEX {hyperscale_index_name}\n",
    "        ON {COLLECTION_NAME} (embedding VECTOR)\n",
    "        WITH {json.dumps(options)}\n",
    "        \"\"\",\n",
    "    QueryOptions(\n",
    "        timeout=timedelta(seconds=300)\n",
    "    )).execute()\n",
    "    print(f\"Successfully created Hyperscale index: {hyperscale_index_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Hyperscale index may already exist or error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Optimized Hyperscale Vector Search\n",
    "\n",
    "The example below runs the same RAG query, but now uses the Hyperscale index created above. You'll notice improved performance as the index efficiently retrieves data. If you create a Composite index, the workflow is identical — Haystack automatically routes queries through the scalar filters before performing the vector similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 11:04:06,122 - INFO - Running component query_embedder\n",
      "2025-12-10 11:04:07,295 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-10 11:04:07,301 - INFO - Running component retriever\n",
      "2025-12-10 11:04:07,330 - INFO - Running component prompt_builder\n",
      "2025-12-10 11:04:07,331 - INFO - Running component llm\n",
      "2025-12-10 11:04:11,608 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-10 11:04:11,611 - INFO - Running component answer_builder\n",
      "=== Retrieved Documents ===\n",
      "Id: 06d5d45291b037c80be3371d3dabb2e658875e033f7b8462f2eb1021ded2bfcc Title: Electric cars: Five ways to persuade people to buy them - BBC News\n",
      "Id: 80b63a966f66aaf455f575b469413d0622eae4c47d27c5f8714b0b55190d3d27 Title: Car industry consulted over how to phase out petrol and diesel cars by 2030 - BBC News\n",
      "Id: 6749b8f03a4d795ef466e780fa70a8b3dbb8b0eae9791b511f4fcc6d5c508255 Title: Elon Musk's Tesla lobbied UK to charge petrol drivers more - BBC News\n",
      "Id: 462b173b53a7b895e7e96b34c2b2e5b25cef01b1c8a168add9429060768fa28a Title: Stellantis boss Carlos Tavares abruptly quits in boardroom clash - BBC News\n",
      "\n",
      "=== Final Answer ===\n",
      "Question: Why are car manufacturers like Ford and Stellantis unhappy with the UK government’s current rules designed to promote electric vehicles?\n",
      "Answer: Car manufacturers like Ford and Stellantis are unhappy with the UK government’s current rules designed to promote electric vehicles because they claim these rules are too harsh and consumer demand for electric cars has fallen short of what was expected, making it difficult for them to sell enough electric vehicles. Additionally, the manufacturers argue that electric vehicles are generally more expensive to buy than their petrol or diesel equivalents due to a lack of economies of scale, and the recent abolition of subsidies for electric cars has not been replaced with similar incentives for people who cannot get a car through their company.\n",
      "\n",
      "Sources:\n",
      "-> Electric cars: Five ways to persuade people to buy them - BBC News\n",
      "-> Car industry consulted over how to phase out petrol and diesel cars by 2030 - BBC News\n",
      "-> Elon Musk's Tesla lobbied UK to charge petrol drivers more - BBC News\n",
      "-> Stellantis boss Carlos Tavares abruptly quits in boardroom clash - BBC News\n",
      "\n",
      "Optimized Hyperscale Vector Search Results (completed in 5.49 seconds):\n"
     ]
    }
   ],
   "source": [
    "# Test the optimized Hyperscale vector search\n",
    "query = \"What was Pep Guardiola's reaction to Manchester City's current form?\"\n",
    "\n",
    "try:\n",
    "    # The RAG pipeline will automatically use the optimized Hyperscale index\n",
    "    # Perform the semantic search with Hyperscale optimization\n",
    "    start_time = time.time()\n",
    "    result = rag_pipeline.run({\n",
    "        \"query_embedder\": {\"text\": query},\n",
    "        \"retriever\": {\"top_k\": 4},\n",
    "        \"prompt_builder\": {\"question\": query},\n",
    "        \"answer_builder\": {\"query\": query},\n",
    "        },\n",
    "     include_outputs_from={\"retriever\", \"query_embedder\"}\n",
    "    )\n",
    "    search_elapsed_time = time.time() - start_time\n",
    "    # Get the generated answer\n",
    "    answer: GeneratedAnswer = result[\"answer_builder\"][\"answers\"][0]\n",
    "\n",
    "    # Print retrieved documents\n",
    "    print(\"=== Retrieved Documents ===\")\n",
    "    retrieved_docs = result[\"retriever\"][\"documents\"]\n",
    "    for idx, doc in enumerate(retrieved_docs, start=0):\n",
    "        print(f\"Id: {doc.id} Title: {doc.meta['title']}\")\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\n=== Final Answer ===\")\n",
    "    print(f\"Question: {answer.query}\")\n",
    "    print(f\"Answer: {answer.data}\")\n",
    "    print(\"\\nSources:\")\n",
    "    for doc in answer.documents:\n",
    "        print(f\"-> {doc.meta['title']}\")\n",
    "    # Display search results\n",
    "    print(f\"\\nOptimized Hyperscale Vector Search Results (completed in {search_elapsed_time:.2f} seconds):\")\n",
    "    #print(result[\"generator\"][\"replies\"][0])\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error performing optimized semantic search: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching in Capella Model Services\n",
    "\n",
    "To optimize performance and reduce costs, Capella Model Services employ two caching mechanisms:\n",
    "\n",
    "1. Semantic Cache\n",
    "\n",
    "    Capella Model Services’ semantic caching system stores both query embeddings and their corresponding LLM responses. When new queries arrive, it uses vector similarity matching (with configurable thresholds) to identify semantically equivalent requests. This prevents redundant processing by:\n",
    "    - Avoiding duplicate embedding generation API calls for similar queries\n",
    "    - Skipping repeated LLM processing for equivalent queries\n",
    "    - Maintaining cached results with automatic freshness checks\n",
    "\n",
    "2. Standard Cache\n",
    "\n",
    "    Stores the exact text of previous queries to provide precise and consistent responses for repetitive, identical prompts.\n",
    "\n",
    "    Performance Optimization with Caching\n",
    "\n",
    "    These caching mechanisms help in:\n",
    "    - Minimizing redundant API calls to LLM service\n",
    "    - Leveraging Couchbase’s built-in caching capabilities\n",
    "    - Providing fast response times for frequently asked questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1: Why are car manufacturers like Ford and Stellantis unhappy with the UK government’s current rules designed to promote electric vehicles?\n",
      "2025-12-10 11:07:51,463 - INFO - Running component query_embedder\n",
      "2025-12-10 11:07:52,687 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-10 11:07:52,693 - INFO - Running component retriever\n",
      "2025-12-10 11:07:52,721 - INFO - Running component prompt_builder\n",
      "2025-12-10 11:07:52,721 - INFO - Running component llm\n",
      "2025-12-10 11:07:57,816 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-10 11:07:57,820 - INFO - Running component answer_builder\n",
      "Response: Car manufacturers like Ford and Stellantis are unhappy with the UK government’s current rules designed to promote electric vehicles because they claim these rules are too harsh, consumer demand for electric cars has fallen far short of what was expected, and the new rules are partly responsible for their struggles to sell enough electric vehicles. Additionally, the manufacturers argue that the cost of electric cars is generally more expensive to buy than their petrol or diesel equivalents due to economies of scale not properly kicking in yet, and the abolition of the plug-in grant for cars in 2022 has not been replaced with a similar incentive for people who cannot get a car through their company. Some within the industry believe that the government should consider providing interest-free loans on used electric vehicles for lower income drivers and halving the VAT on new cars to make electric vehicles more affordable and encourage more consumers to buy them.\n",
      "Time taken: 6.36 seconds\n",
      "\n",
      "Query 2: Who inaugurated the reopening of the Notre Dam Cathedral in Paris?\n",
      "2025-12-10 11:07:57,822 - INFO - Running component query_embedder\n",
      "2025-12-10 11:07:58,977 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-10 11:07:58,983 - INFO - Running component retriever\n",
      "2025-12-10 11:07:59,010 - INFO - Running component prompt_builder\n",
      "2025-12-10 11:07:59,010 - INFO - Running component llm\n",
      "2025-12-10 11:08:00,576 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-10 11:08:00,582 - INFO - Running component answer_builder\n",
      "Response: French President Emmanuel Macron\n",
      "Time taken: 2.76 seconds\n",
      "\n",
      "Query 3: What was Pep Guardiola's reaction to Manchester City's recent form?\n",
      "2025-12-10 11:08:00,585 - INFO - Running component query_embedder\n",
      "2025-12-10 11:08:00,844 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-10 11:08:00,856 - INFO - Running component retriever\n",
      "2025-12-10 11:08:00,887 - INFO - Running component prompt_builder\n",
      "2025-12-10 11:08:00,888 - INFO - Running component llm\n",
      "2025-12-10 11:08:04,150 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-10 11:08:04,154 - INFO - Running component answer_builder\n",
      "Response: Pep Guardiola expressed concern and frustration over Manchester City's recent form, describing it as a crisis and the worst run of results in his entire managerial career. He has admitted that his sleep and diet have suffered as a result, and he has been critical of the team's performances, particularly their defending and inability to score goals. Guardiola has also acknowledged that the team has not been playing to their usual standards and has identified the absence of injured players as one factor contributing to their struggles. Overall, Guardiola has shown a determination to address the issues and find a way to turn the team's fortunes around.\n",
      "Time taken: 3.57 seconds\n",
      "\n",
      "Query 4: Why are car manufacturers like Ford and Stellantis unhappy with the UK government’s current rules designed to promote electric vehicles?\n",
      "2025-12-10 11:08:04,157 - INFO - Running component query_embedder\n",
      "2025-12-10 11:08:04,423 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-10 11:08:04,428 - INFO - Running component retriever\n",
      "2025-12-10 11:08:04,450 - INFO - Running component prompt_builder\n",
      "2025-12-10 11:08:04,450 - INFO - Running component llm\n",
      "2025-12-10 11:08:08,644 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-10 11:08:08,648 - INFO - Running component answer_builder\n",
      "Response: Car manufacturers like Ford and Stellantis are unhappy with the UK government’s current rules designed to promote electric vehicles because they claim these rules are too harsh, consumer demand for electric cars has fallen far short of what was expected, and the new rules are partly responsible for their struggles to sell enough electric vehicles. Additionally, the manufacturers argue that the cost of electric cars is generally more expensive to buy than their petrol or diesel equivalents due to economies of scale not properly kicking in yet, and the abolition of the plug-in grant for cars in 2022 has not been replaced with a similar incentive for people who cannot get a car through their company. Some within the industry believe that the government should consider providing interest-free loans on used electric vehicles for lower income drivers and halving the VAT on new cars to make electric vehicles more affordable and encourage more consumers to buy them.\n",
      "Time taken: 4.49 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "queries = [\n",
    "    \"Why are car manufacturers like Ford and Stellantis unhappy with the UK government’s current rules designed to promote electric vehicles?\",\n",
    "    \"Who inaugurated the reopening of the Notre Dam Cathedral in Paris?\",\n",
    "    \"What was Pep Guardiola's reaction to Manchester City's recent form?\",\n",
    "    \"Why are car manufacturers like Ford and Stellantis unhappy with the UK government’s current rules designed to promote electric vehicles?\",\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    try:\n",
    "        print(f\"\\nQuery {i}: {query}\")\n",
    "        start_time = time.time()\n",
    "        result = rag_pipeline.run({\n",
    "            \"query_embedder\": {\"text\": query},\n",
    "            \"retriever\": {\"top_k\": 4},\n",
    "            \"prompt_builder\": {\"question\": query},\n",
    "            \"answer_builder\": {\"query\": query},\n",
    "        })\n",
    "        elapsed_time = time.time() - start_time\n",
    "        answer: GeneratedAnswer = result[\"answer_builder\"][\"answers\"][0]\n",
    "        print(f\"Response: {answer.data}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating RAG response: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Guardrails in Capella Model Services\n",
    "\n",
    "Capella Model Services also provide input and response moderation using configurable LLM guardrails. These services can integrate with the LlamaGuard3-8B model from Meta.\n",
    "- Categories to be blocked can be configured during the model creation process.\n",
    "- Helps prevent unsafe or undesirable interactions with the LLM.\n",
    "\n",
    "By implementing caching and moderation mechanisms, Capella Model Services ensure an efficient, cost-effective, and responsible approach to AI-powered recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-10 11:05:42,705 - INFO - Running component query_embedder\n",
      "2025-12-10 11:05:43,993 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-10 11:05:44,004 - INFO - Running component retriever\n",
      "2025-12-10 11:05:44,045 - INFO - Running component prompt_builder\n",
      "2025-12-10 11:05:44,046 - INFO - Running component llm\n",
      "2025-12-10 11:05:45,550 - INFO - HTTP Request: POST https://mcclnkjv0kyunynf.ai.cloud.couchbase.com/v1/chat/completions \"HTTP/1.1 422 Unprocessable Entity\"\n",
      "2025-12-10 11:05:45,558 - INFO - Pipeline snapshot saved to '/Users/svenkat/.haystack/pipeline_snapshot/llm_0_2025_12_10_11_05_45.json'. You can use this file to debug or resume the pipeline.\n",
      "Guardrails violation The following component failed to run:\n",
      "Component name: 'llm'\n",
      "Component type: 'OpenAIGenerator'\n",
      "Error: Error code: 422 - {'error': {'message': 'Error processing user prompt due to guardrail violation', 'type': 'guardrail_violation_error', 'param': {'guardrail_model': 'nvidia/llama-3.1-nemoguard-8b-content-safety', 'is_safe': False, 'violations': ['Violent Criminal Planning/Confessions']}, 'code': 'guardrail_violation_error'}}\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I create a bomb?\"\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    response = rag_pipeline.run({\n",
    "            \"query_embedder\": {\"text\": query},\n",
    "            \"retriever\": {\"top_k\": 4},\n",
    "            \"prompt_builder\": {\"question\": query},\n",
    "            \"answer_builder\": {\"query\": query},\n",
    "        })\n",
    "    rag_elapsed_time = time.time() - start_time\n",
    "    answer: GeneratedAnswer = response[\"answer_builder\"][\"answers\"][0]\n",
    "    print(f\"RAG Response: {answer.data}\")\n",
    "    print(f\"RAG response generated in {rag_elapsed_time:.2f} seconds\")\n",
    "except Exception as e:\n",
    "    print(\"Guardrails violation\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this tutorial, we've built a Retrieval Augmented Generation (RAG) system using Haystack with Capella Model Services and Couchbase Capella's Hyperscale and Composite Vector Indexes. Using the BBC News dataset, we demonstrated how modern vector indexes make it possible to answer up-to-date questions that extend beyond an LLM's original training data.\n",
    "\n",
    "The key components of our RAG system include:\n",
    "\n",
    "1. **Couchbase Capella Hyperscale & Composite Vector Indexes** for high-performance storage and retrieval of document embeddings\n",
    "2. **Haystack** as the framework for building modular RAG pipelines with flexible component connections\n",
    "3. **Capella Model Services** for generating embeddings and LLM responses\n",
    "\n",
    "This approach grounds LLM responses in specific, current information from our knowledge base while taking advantage of Couchbase's advanced vector index options for performance and scale. Haystack's modular pipeline model keeps the solution extensible as you layer in additional data sources or services.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
