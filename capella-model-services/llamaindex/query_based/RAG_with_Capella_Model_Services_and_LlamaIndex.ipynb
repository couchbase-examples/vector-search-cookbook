{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this guide, we will walk you through building a Retrieval Augmented Generation (RAG) application with LlamaIndex orchestrating Capella Model Services and Couchbase Capella. We will use the models hosted on Capella Model Services for response generation and generating embeddings.\n",
    "\n",
    "This notebook demonstrates how to build a RAG system using:\n",
    "- The [BBC News dataset](https://huggingface.co/datasets/RealTimeData/bbc_news_alltime) containing news articles\n",
    "- Couchbase Capella as the vector store\n",
    "- Couchbase Hyperscale and Composite Vector Indexes for vector search\n",
    "- LlamaIndex framework for the RAG pipeline\n",
    "- Capella Model Services for embeddings and text generation\n",
    "\n",
    "We leverage Couchbase's Hyperscale and Composite Vector Indexes to enable efficient semantic search at scale. Hyperscale indexes prioritize high-throughput vector similarity across billions of vectors with a compact on-disk footprint, while Composite indexes blend scalar predicates with a vector column to narrow candidate sets before similarity search.\n",
    "\n",
    "Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval. This tutorial will equip you with the knowledge to create a fully functional RAG system using Capella Model Services and LlamaIndex with Couchbase's advanced vector search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you start\n",
    "\n",
    "## Create and Deploy Your Operational cluster on Capella\n",
    "\n",
    "To get started with Couchbase Capella, create an account and use it to deploy an operational cluster.\n",
    "\n",
    "To know more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Couchbase Capella Configuration\n",
    "\n",
    "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met:\n",
    "\n",
    "* Have a multi-node Capella cluster running the Data, Query, Index, and Search services.\n",
    "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the bucket (Read and Write) used in the application.\n",
    "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running.\n",
    "\n",
    "### Deploy Models\n",
    "\n",
    "In order to create the RAG application, we need an embedding model to ingest the documents for Vector Search and a large language model (LLM) for generating the responses based on the context. \n",
    "\n",
    "Capella Model Service allows you to create both the embedding model and the LLM in the same VPC as your database. There are multiple options for both the Embedding & Large Language Models, along with Value Adds to the models.\n",
    "\n",
    "Create the models using the Capella Model Services interface. While creating the model, it is possible to cache the responses (both standard and semantic cache) and apply guardrails to the LLM responses.\n",
    "\n",
    "For more details, please refer to the [documentation](https://docs.couchbase.com/ai/build/model-service/model-service.html). These models are compatible with the [Haystack OpenAI integration](https://haystack.deepset.ai/integrations/openai).\n",
    "\n",
    "After the models are deployed, please create the API keys for them and whitelist the keys on the IP on which the tutorial is being run. For more details, please refer to the documentation on [generating the API keys](https://docs.couchbase.com/ai/api-guide/api-start.html#model-service-keys)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Necessary Libraries\n",
    "To build our RAG system, we need a set of libraries. The libraries we install handle everything from connecting to databases to performing AI tasks. Each library has a specific role: Couchbase libraries manage database operations, LlamaIndex handles AI model integrations, and we will use the OpenAI SDK (compatible with Capella Model Services) for generating embeddings and calling language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install datasets llama-index-vector-stores-couchbase==0.6.0 llama-index-embeddings-openai==0.5.1 llama-index-llms-openai==0.5.6 llama-index==0.14.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries\n",
    "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.exceptions import CouchbaseException\n",
    "from couchbase.options import ClusterOptions, KnownConfigProfiles, QueryOptions\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from llama_index.core import Settings, Document\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.couchbase import CouchbaseQueryVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.vector_stores.couchbase import CouchbaseQueryVectorStore, QueryVectorSearchSimilarity, QueryVectorSearchType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Sensitive Information\n",
    "In this section, we prompt the user to input essential configuration settings needed. These settings include sensitive information like database credentials, collection names, and API keys. Instead of hardcoding these details into the script, we request the user to provide them at runtime, ensuring flexibility and security.\n",
    "\n",
    "The script also validates that all required inputs are provided, raising an error if any crucial information is missing. This approach ensures that your integration is both secure and correctly configured without hardcoding sensitive information, enhancing the overall security and maintainability of your code.\n",
    "\n",
    "**CAPELLA_MODEL_SERVICES_ENDPOINT** is the Capella AI Model Services endpoint found in the models section.\n",
    "> Note that the Capella Model Services Endpoint also requires an additional `/v1` from the endpoint shown on the UI if it is not shown on the UI.\n",
    "\n",
    "**INDEX_NAME** is the name of the Hyperscale or Composite Vector Index we will create for vector search operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_CONNECTION_STRING = input(\"Couchbase Cluster URL (default: localhost): \") or \"localhost\"\n",
    "CB_USERNAME = input(\"Couchbase Username (default: admin): \") or \"admin\"\n",
    "CB_PASSWORD = getpass.getpass(\"Couchbase password (default: Password@12345): \") or \"Password@12345\"\n",
    "CB_BUCKET_NAME = input(\"Couchbase Bucket: \")\n",
    "SCOPE_NAME = input(\"Couchbase Scope: \")\n",
    "COLLECTION_NAME = input(\"Couchbase Collection: \")\n",
    "INDEX_NAME = input(\"Vector Search Index: \") or \"vector_search\" # need to be matched with the search index name in the search_index.json file\n",
    "\n",
    "# Get Capella AI endpoint\n",
    "CAPELLA_MODEL_SERVICES_ENDPOINT = input(\"Enter your Capella Model Services Endpoint: \")\n",
    "LLM_MODEL_NAME = input(\"Enter the LLM name: \")\n",
    "LLM_API_KEY = getpass.getpass(\"Enter your Capella Model Services LLM API Key: \")\n",
    "EMBEDDING_MODEL_NAME = input(\"Enter the Embedding Model name: \")\n",
    "EMBEDDING_API_KEY = getpass.getpass(\"Enter your Capella Model Services Embedding Model API Key: \")\n",
    "EMBEDDING_DIMENSION = input(\"Enter the Embedding Dimension (e.g. 3072, 4096): \") or \"3072\"\n",
    "\n",
    "# Check if the variables are correctly loaded\n",
    "if not all([CB_CONNECTION_STRING, CB_USERNAME, CB_PASSWORD, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME, INDEX_NAME, CAPELLA_MODEL_SERVICES_ENDPOINT, LLM_MODEL_NAME, LLM_API_KEY, EMBEDDING_MODEL_NAME, EMBEDDING_API_KEY]):\n",
    "    raise ValueError(\"All configuration variables must be provided.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Logging\n",
    "Logging is essential for tracking the execution of our script and debugging any issues that may arise. We set up a logger that will display information about the script's progress, including timestamps and log levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to Couchbase Capella\n",
    "The next step is to establish a connection to our Couchbase Capella cluster. This connection will allow us to interact with the database, store and retrieve documents, and perform vector searches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize the Couchbase Cluster\n",
    "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "    options = ClusterOptions(auth)\n",
    "    options.apply_profile(KnownConfigProfiles.WanDevelopment)\n",
    "    # Connect to the cluster\n",
    "    cluster = Cluster(CB_CONNECTION_STRING, options)\n",
    "    \n",
    "    # Wait for the cluster to be ready\n",
    "    cluster.wait_until_ready(timedelta(seconds=5))\n",
    "\n",
    "    logging.info(\"Successfully connected to the Couchbase cluster\")\n",
    "except CouchbaseException as e:\n",
    "    raise RuntimeError(f\"Failed to connect to Couchbase: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Bucket, Scope, and Collection\n",
    "Before we can store our data, we need to ensure that the appropriate bucket, scope, and collection exist in our Couchbase cluster. The code below checks if these components exist and creates them if they don't, providing a foundation for storing our vector embeddings and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from couchbase.management.buckets import CreateBucketSettings\n",
    "import json\n",
    "\n",
    "# Create bucket if it does not exist\n",
    "bucket_manager = cluster.buckets()\n",
    "try:\n",
    "    bucket_manager.get_bucket(CB_BUCKET_NAME)\n",
    "    print(f\"Bucket '{CB_BUCKET_NAME}' already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Bucket '{CB_BUCKET_NAME}' does not exist. Creating bucket...\")\n",
    "    bucket_settings = CreateBucketSettings(name=CB_BUCKET_NAME, ram_quota_mb=500)\n",
    "    bucket_manager.create_bucket(bucket_settings)\n",
    "    print(f\"Bucket '{CB_BUCKET_NAME}' created successfully.\")\n",
    "\n",
    "# Create scope and collection if they do not exist\n",
    "collection_manager = cluster.bucket(CB_BUCKET_NAME).collections()\n",
    "scopes = collection_manager.get_all_scopes()\n",
    "scope_exists = any(scope.name == SCOPE_NAME for scope in scopes)\n",
    "\n",
    "if scope_exists:\n",
    "    print(f\"Scope '{SCOPE_NAME}' already exists.\")\n",
    "else:\n",
    "    print(f\"Scope '{SCOPE_NAME}' does not exist. Creating scope...\")\n",
    "    collection_manager.create_scope(SCOPE_NAME)\n",
    "    print(f\"Scope '{SCOPE_NAME}' created successfully.\")\n",
    "\n",
    "collections = [collection.name for scope in scopes if scope.name == SCOPE_NAME for collection in scope.collections]\n",
    "collection_exists = COLLECTION_NAME in collections\n",
    "\n",
    "if collection_exists:\n",
    "    print(f\"Collection '{COLLECTION_NAME}' already exists in scope '{SCOPE_NAME}'.\")\n",
    "else:\n",
    "    print(f\"Collection '{COLLECTION_NAME}' does not exist in scope '{SCOPE_NAME}'. Creating collection...\")\n",
    "    collection_manager.create_collection(collection_name=COLLECTION_NAME, scope_name=SCOPE_NAME)\n",
    "    print(f\"Collection '{COLLECTION_NAME}' created successfully.\")\n",
    "\n",
    "scope = cluster.bucket(CB_BUCKET_NAME).scope(SCOPE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Vector Search\n",
    "In this section, we'll set up the Couchbase vector store using Couchbase's vector search capabilities for high-performance vector search. Unlike FTS-based vector search, this vector search provides optimized performance for pure vector similarity operations and can scale to billions of vectors with low memory footprint.\n",
    "\n",
    "Couchbase vector search supports two main index types:\n",
    "- **Hyperscale Vector Indexes**: Best for pure vector searches with high performance and concurrent operations\n",
    "- **Composite Vector Indexes**: Best for filtered vector searches combining vector similarity with scalar filtering\n",
    "\n",
    "For this tutorial, we'll use the Query vector store.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the BBC News Dataset\n",
    "To build a RAG engine, we need data to search through. We use the [BBC Realtime News dataset](https://huggingface.co/datasets/RealTimeData/bbc_news_alltime), a dataset with up-to-date BBC news articles grouped by month. This dataset contains articles that were created after the LLM was trained. It will showcase the use of RAG to augment the LLM. \n",
    "\n",
    "The BBC News dataset's varied content allows us to simulate real-world scenarios where users ask complex questions, enabling us to fine-tune our RAG's ability to understand and respond to various types of queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    news_dataset = load_dataset('RealTimeData/bbc_news_alltime', '2024-12', split=\"train\")\n",
    "    print(f\"Loaded the BBC News dataset with {len(news_dataset)} rows\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading TREC dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first two examples from the dataset\n",
    "print(\"Dataset columns:\", news_dataset.column_names)\n",
    "print(\"\\nFirst two examples:\")\n",
    "print(news_dataset[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for RAG\n",
    "\n",
    "We need to extract the context passages from the dataset to use as our knowledge base for the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "news_articles = news_dataset\n",
    "unique_articles = {}\n",
    "\n",
    "for article in news_articles:\n",
    "    content = article.get(\"content\")\n",
    "    if content:\n",
    "        content_hash = hashlib.md5(content.encode()).hexdigest()  # Generate hash of content\n",
    "        if content_hash not in unique_articles:\n",
    "            unique_articles[content_hash] = article  # Store full article\n",
    "\n",
    "unique_news_articles = list(unique_articles.values())  # Convert back to list\n",
    "\n",
    "print(f\"We have {len(unique_news_articles)} unique articles in our database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Embeddings using Capella Model Services\n",
    "Embeddings are numerical representations of text that capture semantic meaning. Unlike keyword-based search, embeddings enable semantic search to understand context and retrieve documents that are conceptually similar even without exact keyword matches. We'll use the model deployed on Capella Model Services to create high-quality embeddings. This model transforms our text data into vector representations that can be efficiently searched, with a batch size of 30 for optimal processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Set up the embedding model\n",
    "    embed_model = OpenAIEmbedding(\n",
    "        api_base=CAPELLA_MODEL_SERVICES_ENDPOINT,\n",
    "        api_key=EMBEDDING_API_KEY,\n",
    "        embed_batch_size=30,\n",
    "        model_name=EMBEDDING_MODEL_NAME\n",
    "    )\n",
    "    \n",
    "    # Configure LlamaIndex to use this embedding model\n",
    "    Settings.embed_model = embed_model\n",
    "    print(\"Successfully created embedding model\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating embedding model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Embeddings Model\n",
    "We can test the embeddings model by generating an embedding for a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding = embed_model.get_text_embedding(\"this is a test sentence\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Couchbase Vector Store\n",
    "The vector store is set up to store the documents from the dataset using Couchbase's Global Secondary Index vector search capabilities. This vector store is optimized for high-performance vector similarity search operations and can scale to billions of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create the Couchbase vector store\n",
    "    vector_store = CouchbaseQueryVectorStore(\n",
    "        cluster=cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        search_type=QueryVectorSearchType.ANN,\n",
    "        similarity=QueryVectorSearchSimilarity.COSINE,\n",
    "        nprobes=10\n",
    "    )\n",
    "    print(\"Successfully created vector store\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to create vector store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating LlamaIndex Documents\n",
    "In this section, we'll process our news articles and create LlamaIndex Document objects.\n",
    "Each Document is created with specific metadata and formatting templates to control what the LLM and embedding model see.\n",
    "We'll observe examples of the formatted content to understand how the documents are structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "llama_documents = []\n",
    "# Process and store documents\n",
    "for article in unique_news_articles:  # Limit to first 100 for demo\n",
    "    try:\n",
    "        document = Document(\n",
    "            text=article[\"content\"],\n",
    "            metadata={\n",
    "                \"title\": article[\"title\"],\n",
    "                \"description\": article[\"description\"],\n",
    "                \"published_date\": article[\"published_date\"],\n",
    "                \"link\": article[\"link\"],\n",
    "            },\n",
    "            excluded_llm_metadata_keys=[\"description\"],\n",
    "            excluded_embed_metadata_keys=[\"description\", \"published_date\", \"link\"],\n",
    "            metadata_template=\"{key}=>{value}\",\n",
    "            text_template=\"Metadata: \\n{metadata_str}\\n-----\\nContent: {content}\",\n",
    "        )\n",
    "        llama_documents.append(document)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save document to vector store: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Observing an example of what the LLM and Embedding model receive as input\n",
    "print(\"The LLM sees this:\")\n",
    "print(llama_documents[0].get_content(metadata_mode=MetadataMode.LLM))\n",
    "print(\"The Embedding model sees this:\")\n",
    "print(llama_documents[0].get_content(metadata_mode=MetadataMode.EMBED))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Running the Ingestion Pipeline\n",
    "\n",
    "In this section, we'll create an ingestion pipeline to process our documents. The pipeline will:\n",
    "\n",
    "1. Split the documents into smaller chunks (nodes) using the SentenceSplitter\n",
    "2. Generate embeddings for each node using our embedding model\n",
    "3. Store these nodes with their embeddings in our Couchbase vector store\n",
    "\n",
    "This process transforms our raw documents into a searchable knowledge base that can be queried semantically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Process documents: split into nodes, generate embeddings, and store in vector database\n",
    "# Step 3: Create and Run IndexPipeline\n",
    "index_pipeline = IngestionPipeline(\n",
    "    transformations=[SentenceSplitter(),embed_model], \n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "index_pipeline.run(documents=llama_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Capella Model Services Large Language Model (LLM)\n",
    "Large language models are AI systems that are trained to understand and generate human language. We'll be using the model deployed on Capella Model Services to process user queries and generate meaningful responses based on the retrieved context from our Couchbase vector store. This model is a key component of our RAG system, allowing it to go beyond simple keyword matching and truly understand the intent behind a query. By integrating the LLM, we equip our RAG system with the ability to interpret complex queries, understand the nuances of language, and provide more accurate and contextually relevant responses.\n",
    "\n",
    "The language model's ability to understand context and generate coherent responses is what makes our RAG system truly intelligent. It can not only find the right information but also present it in a way that is useful and understandable to the user.\n",
    "\n",
    "The LLM is configured using LlamaIndex's OpenAI-like provider with your Capella Model Services API key for seamless integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Set up the LLM\n",
    "    llm = OpenAILike(\n",
    "        api_base=CAPELLA_MODEL_SERVICES_ENDPOINT,\n",
    "        api_key=LLM_API_KEY,\n",
    "        model=LLM_MODEL_NAME,\n",
    "    )\n",
    "    # Configure LlamaIndex to use this LLM\n",
    "    Settings.llm = llm\n",
    "    logging.info(\"Successfully created the OpenAI LLM\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating OpenAI LLM: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Vector Store Index\n",
    "\n",
    "In this section, we'll create a VectorStoreIndex from our Couchbase vector store. This index serves as the foundation for our RAG system, enabling semantic search capabilities and efficient retrieval of relevant information.\n",
    "\n",
    "The VectorStoreIndex provides a high-level interface to interact with our vector store, allowing us to:\n",
    "1. Perform semantic searches based on user queries\n",
    "2. Retrieve the most relevant documents or chunks\n",
    "3. Generate contextually appropriate responses using our LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "rag = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) with Couchbase and LlamaIndex\n",
    "\n",
    "Let's test our RAG system by performing a semantic search on a sample query. In this example, we'll use a question about Pep Guardiola's reaction to Manchester City's recent form. The RAG system will:\n",
    "\n",
    "1. Process the natural language query\n",
    "2. Search through our vector database for relevant information\n",
    "3. Retrieve the most semantically similar documents\n",
    "4. Generate a comprehensive response using the LLM\n",
    "\n",
    "This demonstrates how our system combines the power of vector search with language model capabilities to provide accurate, contextual answers based on the information in our database.\n",
    "\n",
    "**Note:** By default, without any Hyperscale or Composite Vector Index, Couchbase uses linear brute force search which compares the query vector against every document in the collection. This works for small datasets but can become slow as the dataset grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample query from the dataset\n",
    "\n",
    "query = \"Who will Daniel Dubois fight in Saudi Arabia on 22 February?\"\n",
    "\n",
    "try:\n",
    "    # Perform the semantic search\n",
    "    start_time = time.time()\n",
    "    response = rag.query(query)\n",
    "    search_elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Display search results\n",
    "    print(f\"\\nSemantic Search Results (completed in {search_elapsed_time:.2f} seconds):\")\n",
    "    print(response)\n",
    "\n",
    "except RecursionError as e:\n",
    "    raise RuntimeError(f\"Error performing semantic search: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Vector Search\n",
    "\n",
    "While the above RAG system works effectively, we can significantly improve query performance by leveraging Couchbase's advanced vector search capabilities.\n",
    "\n",
    "Couchbase offers three types of vector indexes, but for vector search we focus on two main types:\n",
    "\n",
    "**Hyperscale Vector Indexes**\n",
    "- Best for pure vector searches - content discovery, recommendations, semantic search\n",
    "- High performance with low memory footprint - designed to scale to billions of vectors\n",
    "- Optimized for concurrent operations - supports simultaneous searches and inserts\n",
    "- Use when: You primarily perform vector-only queries without complex scalar filtering\n",
    "- Ideal for: Large-scale semantic search, recommendation systems, content discovery\n",
    "\n",
    "**Composite Vector Indexes**\n",
    "- Best for filtered vector searches - combines vector search with scalar value filtering\n",
    "- Efficient pre-filtering - scalar attributes reduce the vector comparison scope\n",
    "- Use when: Your queries combine vector similarity with scalar filters that eliminate large portions of data\n",
    "- Ideal for: Compliance-based filtering, user-specific searches, time-bounded queries\n",
    "\n",
    "**Choosing the Right Index Type**\n",
    "- Start with Hyperscale Vector Index for pure vector searches and large datasets\n",
    "- Use Composite Vector Index when scalar filters significantly reduce your search space\n",
    "- Consider your dataset size: Hyperscale scales to billions, Composite works well for tens of millions to billions\n",
    "\n",
    "For more details, see the [Couchbase Vector Index documentation](https://docs.couchbase.com/server/current/vector-index/use-vector-indexes.html).\n",
    "\n",
    "## Understanding Index Configuration (Couchbase 8.0 Feature)\n",
    "\n",
    "The index_description parameter controls how Couchbase optimizes vector storage and search performance through centroids and quantization:\n",
    "\n",
    "Format: `'IVF[<centroids>],{PQ|SQ}<settings>'`\n",
    "\n",
    "**Centroids (IVF - Inverted File):**\n",
    "- Controls how the dataset is subdivided for faster searches\n",
    "- More centroids = faster search, slower training  \n",
    "- Fewer centroids = slower search, faster training\n",
    "- If omitted (like IVF,SQ8), Couchbase auto-selects based on dataset size\n",
    "\n",
    "**Quantization Options:**\n",
    "- SQ (Scalar Quantization): SQ4, SQ6, SQ8 (4, 6, or 8 bits per dimension)\n",
    "- PQ (Product Quantization): PQ<subquantizers>x<bits> (e.g., PQ32x8)\n",
    "- Higher values = better accuracy, larger index size\n",
    "\n",
    "**Common Examples:**\n",
    "- IVF,SQ8 - Auto centroids, 8-bit scalar quantization (good default)\n",
    "- IVF1000,SQ6 - 1000 centroids, 6-bit scalar quantization  \n",
    "- IVF,PQ32x8 - Auto centroids, 32 subquantizers with 8 bits\n",
    "\n",
    "For detailed configuration options, see the [Quantization & Centroid Settings](https://docs.couchbase.com/server/current/vector-index/hyperscale-vector-index.html#algo_settings).\n",
    "\n",
    "In the code below, we demonstrate creating a Hyperscale index for optimal performance. You can adapt the same flow to create a COMPOSITE index by replacing the index type and options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Hyperscale Vector Index for optimized vector search\n",
    "try:\n",
    "    hyperscale_index_name = f\"{INDEX_NAME}_hyperscale\"\n",
    "\n",
    "    options = {\n",
    "        \"dimension\": int(EMBEDDING_DIMENSION),\n",
    "        \"description\": \"IVF,PQ32x8\",\n",
    "        \"similarity\": \"cosine\",\n",
    "        \"scan_nprobes\": 3,\n",
    "    }\n",
    "    scope.query(\n",
    "        f\"\"\"\n",
    "        CREATE VECTOR INDEX {hyperscale_index_name}\n",
    "        ON {COLLECTION_NAME} (embedding VECTOR)\n",
    "        WITH {json.dumps(options)}\n",
    "        \"\"\",\n",
    "    QueryOptions(\n",
    "        timeout=timedelta(seconds=300)\n",
    "    )).execute()\n",
    "    print(f\"Successfully created Hyperscale index: {hyperscale_index_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Hyperscale index may already exist or error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below shows running the same RAG query, but now using the Hyperscale index we created above. You'll notice improved performance as the index efficiently retrieves data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the optimized vector search with Hyperscale index\n",
    "query = \"Who will Daniel Dubois fight in Saudi Arabia on 22 February?\"\n",
    "try:\n",
    "    # Create a new query engine using the optimized vector store\n",
    "    optimized_rag = index.as_query_engine()\n",
    "    \n",
    "    # Perform the semantic search with Hyperscale optimization\n",
    "    start_time = time.time()\n",
    "    response = optimized_rag.query(query)\n",
    "    search_elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Display search results\n",
    "    print(f\"\\nOptimized Vector Search Results (completed in {search_elapsed_time:.2f} seconds):\")\n",
    "    print(response)\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error performing optimized semantic search: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching in Capella Model Services\n",
    "\n",
    "To optimize performance and reduce costs, Capella Model Services employ two caching mechanisms:\n",
    "\n",
    "1. Semantic Cache\n",
    "\n",
    "    Capella Model Services’ semantic caching system stores both query embeddings and their corresponding LLM responses. When new queries arrive, it uses vector similarity matching (with configurable thresholds) to identify semantically equivalent requests. This prevents redundant processing by:\n",
    "    - Avoiding duplicate embedding generation API calls for similar queries\n",
    "    - Skipping repeated LLM processing for equivalent queries\n",
    "    - Maintaining cached results with automatic freshness checks\n",
    "\n",
    "2. Standard Cache\n",
    "\n",
    "    Stores the exact text of previous queries to provide precise and consistent responses for repetitive, identical prompts.\n",
    "\n",
    "    Performance Optimization with Caching\n",
    "\n",
    "    These caching mechanisms help in:\n",
    "    - Minimizing redundant API calls to LLM service\n",
    "    - Leveraging Couchbase’s built-in caching capabilities\n",
    "    - Providing fast response times for frequently asked questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "queries = [\n",
    "    \"Why are car manufacturers like Ford and Stellantis unhappy with the UK government’s current rules designed to promote electric vehicles?\",\n",
    "    \"Who inaugurated the reopening of the Notre Dam Cathedral in Paris?\",\n",
    "    \"What was Pep Guardiola's reaction to Manchester City's recent form?\",\n",
    "    \"Why are car manufacturers like Ford and Stellantis unhappy with the UK government’s current rules designed to promote electric vehicles?\",\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    try:\n",
    "        print(f\"\\nQuery {i}: {query}\")\n",
    "        start_time = time.time()\n",
    "        response = rag.query(query)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating RAG response: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Guardrails in Capella Model Services\n",
    "Capella Model services also have the ability to moderate the user inputs and the responses generated by the LLM. Capella Model Services can be configured to use the [Llama 3.1 NemoGuard 8B safety model](https://build.nvidia.com/nvidia/llama-3_1-nemoguard-8b-content-safety/modelcard) guardrails model from Meta. The categories to be blocked can be configured in the model creation flow. More information about Guardrails usage can be found in the [documentation](https://docs.couchbase.com/ai/build/model-service/configure-guardrails-security.html#guardrails).\n",
    " \n",
    "Here is an example of the Guardrails in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can I create a bomb?\"\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    response = rag.query(query)\n",
    "    rag_elapsed_time = time.time() - start_time\n",
    "    print(f\"RAG Response: {response}\")\n",
    "    print(f\"RAG response generated in {rag_elapsed_time:.2f} seconds\")\n",
    "except Exception as e:\n",
    "    print(\"Guardrails violation\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this tutorial, we've built a Retrieval Augmented Generation (RAG) system using Couchbase Capella's vector search, Capella Model Services, and LlamaIndex. We used the BBC News dataset, which contains real-time news articles, to demonstrate how RAG can be used to answer questions about current events and provide up-to-date information that extends beyond the LLM's training data.\n",
    "\n",
    "The key components of our RAG system include:\n",
    "\n",
    "1. **Couchbase Capella Vector Search** as the high-performance vector database for storing and retrieving document embeddings\n",
    "2. **LlamaIndex** as the framework for connecting our data to the LLM\n",
    "3. **Capella Model Services** for generating embeddings and LLM responses\n",
    "4. **Vector Indexes** (Hyperscale/Composite) for optimized vector search performance\n",
    "\n",
    "This approach allows us to enhance the capabilities of large language models by grounding their responses in specific, up-to-date information from our knowledge base, while leveraging Couchbase's advanced vector search for optimal performance and scalability.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
