{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNdImxzypDlm"
   },
   "source": "## Introduction\nIn this guide, we will walk you through building a powerful semantic search engine using Couchbase as the backend database, [OpenAI](https://openai.com) as the embedding and LLM provider, and [PydanticAI](https://ai.pydantic.dev) as an agent orchestrator. Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval. This tutorial is designed to be beginner-friendly, with clear, step-by-step instructions that will equip you with the knowledge to create a fully functional semantic search system from scratch. Alternatively if you want to perform semantic search using the Hyperscale or Composite Vector Index, please take a look at [this.](https://developer.couchbase.com/tutorial-pydantic-ai-couchbase-rag-with-hyperscale-or-composite-vector-index)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## How to run this tutorial\n\nThis tutorial is available as a Jupyter Notebook (`.ipynb` file) that you can run interactively.\n\nYou can either download the notebook file and run it on [Google Colab](https://colab.research.google.com/) or run it on your system by setting up the Python environment."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Before you start\n\n### Create and Deploy Your Free Tier Operational cluster on Capella\n\nTo get started with Couchbase Capella, create an account and use it to deploy a forever free tier operational cluster. This account provides you with an environment where you can explore and learn about Capella with no time constraint.\n\nTo know more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html).\n\n#### Couchbase Capella Configuration\n\nWhen running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met.\n\n* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the travel-sample bucket (Read and Write) used in the application.\n* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NH2o6pqa69oG"
   },
   "source": "## Setting the Stage: Installing Necessary Libraries\nTo build our semantic search engine, we need a robust set of tools. The libraries we install handle everything from connecting to databases to performing complex machine learning tasks. Each library has a specific role: Couchbase libraries manage database operations, LangChain handles AI model integrations, and OpenAI provides advanced AI models for generating embeddings and understanding natural language. By setting up these libraries, we ensure our environment is equipped to handle the data-intensive and computationally complex tasks required for semantic search."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DYhPj0Ta8l_A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet -U datasets==3.5.0 langchain-couchbase==0.3.0 langchain-openai==0.3.13 python-dotenv==1.1.0 pydantic-ai==0.1.1 ipywidgets==8.1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pp7GtNg8mB9"
   },
   "source": "## Importing Necessary Libraries\nThe script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading. These libraries provide essential functions for working with data, managing database connections, and processing machine learning models."
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "8GzS6tfL8mFP"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from uuid import uuid4\n",
    "from datetime import timedelta\n",
    "\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.exceptions import (InternalServerFailureException,\n",
    "                                  QueryIndexAlreadyExistsException)\n",
    "from couchbase.management.buckets import CreateBucketSettings\n",
    "from couchbase.management.search import SearchIndex\n",
    "from couchbase.options import ClusterOptions\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from langchain_couchbase.vectorstores import CouchbaseSearchVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pydantic_ai import Agent, RunContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBnMp5vb8mIb"
   },
   "source": "## Setup Logging\nLogging is configured to track the progress of the script and capture any errors or warnings. This is crucial for debugging and understanding the flow of execution. The logging output includes timestamps, log levels (e.g., INFO, ERROR), and messages that describe what is happening in the script."
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Yv8kWcuf8mLx"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9G5a0en8mPA"
   },
   "source": "## Loading Sensitive Information\nIn this section, we prompt the user to input essential configuration settings needed. These settings include sensitive information like API keys, database credentials, and specific configuration names. Instead of hardcoding these details into the script, we request the user to provide them at runtime, ensuring flexibility and security.\n\nThe script also validates that all required inputs are provided, raising an error if any crucial information is missing. This approach ensures that your integration is both secure and correctly configured without hardcoding sensitive information, enhancing the overall security and maintainability of your code."
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "PFGyHll18mSe"
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or getpass.getpass('Enter your OpenAI API Key: ')\n",
    "\n",
    "CB_HOST = os.getenv('CB_HOST') or input('Enter your Couchbase host (default: couchbase://localhost): ') or 'couchbase://localhost'\n",
    "CB_USERNAME = os.getenv('CB_USERNAME') or input('Enter your Couchbase username (default: Administrator): ') or 'Administrator'\n",
    "CB_PASSWORD = os.getenv('CB_PASSWORD') or getpass.getpass('Enter your Couchbase password (default: password): ') or 'password'\n",
    "CB_BUCKET_NAME = os.getenv('CB_BUCKET_NAME') or input('Enter your Couchbase bucket name (default: vector-search-testing): ') or 'vector-search-testing'\n",
    "INDEX_NAME = os.getenv('INDEX_NAME') or input('Enter your index name (default: vector_search_pydantic_ai): ') or 'vector_search_pydantic_ai'\n",
    "SCOPE_NAME = os.getenv('SCOPE_NAME') or input('Enter your scope name (default: shared): ') or 'shared'\n",
    "COLLECTION_NAME = os.getenv('COLLECTION_NAME') or input('Enter your collection name (default: pydantic_ai): ') or 'pydantic_ai'\n",
    "\n",
    "# Check if the variables are correctly loaded\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"Missing OpenAI API Key\")\n",
    "\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtGrYzUY8mV3"
   },
   "source": "## Connecting to the Couchbase Cluster\nConnecting to a Couchbase cluster is the foundation of our project. Couchbase will serve as our primary data store, handling all the storage and retrieval operations required for our semantic search engine. By establishing this connection, we enable our application to interact with the database, allowing us to perform operations such as storing embeddings, querying data, and managing collections. This connection is the gateway through which all data will flow, so ensuring it's set up correctly is paramount."
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Zb3kK-7W8mZK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:54:19,537 - INFO - Successfully connected to Couchbase\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "    options = ClusterOptions(auth)\n",
    "    cluster = Cluster(CB_HOST, options)\n",
    "    cluster.wait_until_ready(timedelta(seconds=5))\n",
    "    logging.info(\"Successfully connected to Couchbase\")\n",
    "except Exception as e:\n",
    "    raise ConnectionError(f\"Failed to connect to Couchbase: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_Gpy32N8mcZ"
   },
   "source": "## Setting Up Collections in Couchbase\n\nThe setup_collection() function handles creating and configuring the hierarchical data organization in Couchbase:\n\n1. Bucket Creation:\n   - Checks if specified bucket exists, creates it if not\n   - Sets bucket properties like RAM quota (1024MB) and replication (disabled)\n   - Note: You will not be able to create a bucket on Capella\n\n2. Scope Management:  \n   - Verifies if requested scope exists within bucket\n   - Creates new scope if needed (unless it's the default \"_default\" scope)\n\n3. Collection Setup:\n   - Checks for collection existence within scope\n   - Creates collection if it doesn't exist\n   - Waits 2 seconds for collection to be ready\n\nAdditional Tasks:\n- Creates primary index on collection for query performance\n- Clears any existing documents for clean state\n- Implements comprehensive error handling and logging"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ACZcwUnG8mf2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:54:23,668 - INFO - Bucket 'vector-search-testing' does not exist. Creating it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:54:25,721 - INFO - Bucket 'vector-search-testing' created successfully.\n",
      "2025-04-11 13:54:25,728 - INFO - Scope 'shared' does not exist. Creating it...\n",
      "2025-04-11 13:54:25,777 - INFO - Scope 'shared' created successfully.\n",
      "2025-04-11 13:54:25,796 - INFO - Collection 'pydantic_ai' does not exist. Creating it...\n",
      "2025-04-11 13:54:27,843 - INFO - Collection 'pydantic_ai' created successfully.\n",
      "2025-04-11 13:54:28,120 - INFO - Primary index present or created successfully.\n",
      "2025-04-11 13:54:28,133 - INFO - All documents cleared from the collection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<couchbase.collection.Collection at 0x16febe640>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup_collection(cluster, bucket_name, scope_name, collection_name):\n    try:\n        # Check if bucket exists, create if it doesn't\n        try:\n            bucket = cluster.bucket(bucket_name)\n            logging.info(f\"Bucket '{bucket_name}' exists.\")\n        except Exception as e:\n            logging.info(f\"Bucket '{bucket_name}' does not exist. Creating it...\")\n            bucket_settings = CreateBucketSettings(\n                name=bucket_name,\n                bucket_type='couchbase',\n                ram_quota_mb=1024,\n                flush_enabled=True,\n                num_replicas=0\n            )\n            cluster.buckets().create_bucket(bucket_settings)\n            time.sleep(2)  # Wait for bucket creation to complete and become available\n            bucket = cluster.bucket(bucket_name)\n            logging.info(f\"Bucket '{bucket_name}' created successfully.\")\n\n        bucket_manager = bucket.collections()\n\n        # Check if scope exists, create if it doesn't\n        scopes = bucket_manager.get_all_scopes()\n        scope_exists = any(scope.name == scope_name for scope in scopes)\n        \n        if not scope_exists and scope_name != \"_default\":\n            logging.info(f\"Scope '{scope_name}' does not exist. Creating it...\")\n            bucket_manager.create_scope(scope_name)\n            logging.info(f\"Scope '{scope_name}' created successfully.\")\n\n        # Check if collection exists, create if it doesn't\n        collection_exists = any(\n            scope.name == scope_name and collection_name in [col.name for col in scope.collections]\n            for scope in scopes\n        )\n\n        if not collection_exists:\n            logging.info(f\"Collection '{collection_name}' does not exist. Creating it...\")\n            bucket_manager.create_collection(scope_name, collection_name)\n            time.sleep(2)\n            logging.info(f\"Collection '{collection_name}' created successfully.\")\n        else:\n            logging.info(f\"Collection '{collection_name}' already exists.Skipping creation.\")\n\n        collection = bucket.scope(scope_name).collection(collection_name)\n        time.sleep(2)  # Give the collection time to be ready for queries\n\n        # Ensure primary index exists\n        try:\n            cluster.query(f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{bucket_name}`.`{scope_name}`.`{collection_name}`\").execute()\n            logging.info(\"Primary index present or created successfully.\")\n        except Exception as e:\n            logging.warning(f\"Error creating primary index: {str(e)}\")\n\n        # Clear all documents in the collection\n        try:\n            query = f\"DELETE FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n            cluster.query(query).execute()\n            logging.info(\"All documents cleared from the collection.\")\n        except Exception as e:\n            logging.warning(f\"Error while clearing documents: {str(e)}. The collection might be empty.\")\n\n        return collection\n    except Exception as e:\n        raise RuntimeError(f\"Error setting up collection: {str(e)}\")\n\nsetup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMJ7RRYp8mjV"
   },
   "source": "## Loading Couchbase Vector Search Index\n\nSemantic search requires an efficient way to retrieve relevant documents based on a user's query. This is where the Couchbase **Vector Search Index** comes into play. In this step, we load the Vector Search Index definition from a JSON file, which specifies how the index should be structured. This includes the fields to be indexed, the dimensions of the vectors, and other parameters that determine how the search engine processes queries based on vector similarity.\n\nThis vector search index configuration requires specific default settings to function properly. This tutorial uses the bucket named `vector-search-testing` with the scope `shared` and collection `pydantic_ai`. The configuration is set up for vectors with exactly `1536 dimensions`, using dot product similarity and optimized for recall. If you want to use a different bucket, scope, or collection, you will need to modify the index configuration accordingly.\n\nFor more information on creating a vector search index, please follow the [instructions](https://docs.couchbase.com/cloud/vector-search/create-vector-search-index-ui.html)."
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "y7xiCrOc8mmj"
   },
   "outputs": [],
   "source": [
    "# If you are running this script locally (not in Google Colab), uncomment the following line\n",
    "# and provide the path to your index definition file.\n",
    "\n",
    "# index_definition_path = '/path_to_your_index_file/pydantic_ai_index.json'  # Local setup: specify your file path here\n",
    "\n",
    "# # Version for Google Colab\n",
    "# def load_index_definition_colab():\n",
    "#     from google.colab import files\n",
    "#     print(\"Upload your index definition file\")\n",
    "#     uploaded = files.upload()\n",
    "#     index_definition_path = list(uploaded.keys())[0]\n",
    "\n",
    "#     try:\n",
    "#         with open(index_definition_path, 'r') as file:\n",
    "#             index_definition = json.load(file)\n",
    "#         return index_definition\n",
    "#     except Exception as e:\n",
    "#         raise ValueError(f\"Error loading index definition from {index_definition_path}: {str(e)}\")\n",
    "\n",
    "# Version for Local Environment\n",
    "def load_index_definition_local(index_definition_path):\n",
    "    try:\n",
    "        with open(index_definition_path, 'r') as file:\n",
    "            index_definition = json.load(file)\n",
    "        return index_definition\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading index definition from {index_definition_path}: {str(e)}\")\n",
    "\n",
    "# Usage\n",
    "# Uncomment the appropriate line based on your environment\n",
    "# index_definition = load_index_definition_colab()\n",
    "index_definition = load_index_definition_local('pydantic_ai_index.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_ddPQ_Y8mpm"
   },
   "source": "## Creating or Updating Search Indexes\n\nWith the index definition loaded, the next step is to create or update the **Vector Search Index** in Couchbase. This step is crucial because it optimizes our database for vector similarity search operations, allowing us to perform searches based on the semantic content of documents rather than just keywords. By creating or updating a Vector Search Index, we enable our search engine to handle complex queries that involve finding semantically similar documents using vector embeddings, which is essential for a robust semantic search engine."
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "bHEpUu1l8msx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:54:41,157 - INFO - Creating new index 'vector-search-testing.shared.vector_search_pydantic_ai'...\n",
      "2025-04-11 13:54:41,316 - INFO - Index 'vector-search-testing.shared.vector_search_pydantic_ai' successfully created/updated.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    scope_index_manager = cluster.bucket(CB_BUCKET_NAME).scope(SCOPE_NAME).search_indexes()\n",
    "\n",
    "    # Check if index already exists\n",
    "    existing_indexes = scope_index_manager.get_all_indexes()\n",
    "    index_name = index_definition[\"name\"]\n",
    "\n",
    "    if index_name in [index.name for index in existing_indexes]:\n",
    "        logging.info(f\"Index '{index_name}' found\")\n",
    "    else:\n",
    "        logging.info(f\"Creating new index '{index_name}'...\")\n",
    "\n",
    "    # Create SearchIndex object from JSON definition\n",
    "    search_index = SearchIndex.from_json(index_definition)\n",
    "\n",
    "    # Upsert the index (create if not exists, update if exists)\n",
    "    scope_index_manager.upsert_index(search_index)\n",
    "    logging.info(f\"Index '{index_name}' successfully created/updated.\")\n",
    "\n",
    "except QueryIndexAlreadyExistsException:\n",
    "    logging.info(f\"Index '{index_name}' already exists. Skipping creation/update.\")\n",
    "\n",
    "except InternalServerFailureException as e:\n",
    "    error_message = str(e)\n",
    "    logging.error(f\"InternalServerFailureException raised: {error_message}\")\n",
    "\n",
    "    try:\n",
    "        # Accessing the response_body attribute from the context\n",
    "        error_context = e.context\n",
    "        response_body = error_context.response_body\n",
    "        if response_body:\n",
    "            error_details = json.loads(response_body)\n",
    "            error_message = error_details.get('error', '')\n",
    "\n",
    "            if \"collection: 'pydantic_ai' doesn't belong to scope: 'shared'\" in error_message:\n",
    "                raise ValueError(\"Collection 'pydantic_ai' does not belong to scope 'shared'. Please check the collection and scope names.\")\n",
    "\n",
    "    except ValueError as ve:\n",
    "        logging.error(str(ve))\n",
    "        raise\n",
    "\n",
    "    except Exception as json_error:\n",
    "        logging.error(f\"Failed to parse the error message: {json_error}\")\n",
    "        raise RuntimeError(f\"Internal server error while creating/updating search index: {error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FvxRsg38m3G"
   },
   "source": "## Creating OpenAI Embeddings\nEmbeddings are at the heart of semantic search. They are numerical representations of text that capture the semantic meaning of the words and phrases. Unlike traditional keyword-based search, which looks for exact matches, embeddings allow our search engine to understand the context and nuances of language, enabling it to retrieve documents that are semantically similar to the query, even if they don't contain the exact keywords. By creating embeddings using OpenAI, we equip our search engine with the ability to understand and process natural language in a way that's much closer to how humans understand language. This step transforms our raw text data into a format that the search engine can use to find and rank relevant documents."
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_75ZyCRh8m6m"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:55:10,426 - INFO - Successfully created OpenAIEmbeddings\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        api_key=OPENAI_API_KEY,\n",
    "    )\n",
    "    logging.info(\"Successfully created OpenAIEmbeddings\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating OpenAIEmbeddings: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IwZMUnF8m-N"
   },
   "source": "## Setting Up the Couchbase Vector Store\nThe vector store is set up to manage the embeddings created in the previous step. The vector store is essentially a database optimized for storing and retrieving high-dimensional vectors. In this case, the vector store is built on top of Couchbase, allowing the script to store the embeddings in a way that can be efficiently searched."
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "DwIJQjYT9RV_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:55:12,849 - INFO - Successfully created vector store\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vector_store = CouchbaseSearchVectorStore(\n",
    "        cluster=cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=embeddings,\n",
    "        index_name=INDEX_NAME,\n",
    "    )\n",
    "    logging.info(\"Successfully created vector store\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to create vector store: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load the BBC News Dataset\nTo build a search engine, we need data to search through. We use the BBC News dataset from RealTimeData, which provides real-world news articles. This dataset contains news articles from BBC covering various topics and time periods. Loading the dataset is a crucial step because it provides the raw material that our search engine will work with. The quality and diversity of the news articles make it an excellent choice for testing and refining our search engine, ensuring it can handle real-world news content effectively.\n\nThe BBC News dataset allows us to work with authentic news articles, enabling us to build and test a search engine that can effectively process and retrieve relevant news content. The dataset is loaded using the Hugging Face datasets library, specifically accessing the \"RealTimeData/bbc_news_alltime\" dataset with the \"2024-12\" version."
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:55:22,967 - INFO - Successfully loaded the BBC News dataset with 2687 rows.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the BBC News dataset with 2687 rows\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    news_dataset = load_dataset(\n",
    "        \"RealTimeData/bbc_news_alltime\", \"2024-12\", split=\"train\"\n",
    "    )\n",
    "    print(f\"Loaded the BBC News dataset with {len(news_dataset)} rows\")\n",
    "    logging.info(f\"Successfully loaded the BBC News dataset with {len(news_dataset)} rows.\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading the BBC News dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cleaning up the Data\nWe will use the content of the news articles for our RAG system.\n\nThe dataset contains a few duplicate records. We are removing them to avoid duplicate results in the retrieval stage of our RAG system."
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1749 unique articles in our database.\n"
     ]
    }
   ],
   "source": [
    "news_articles = news_dataset[\"content\"]\n",
    "unique_articles = set()\n",
    "for article in news_articles:\n",
    "    if article:\n",
    "        unique_articles.add(article)\n",
    "unique_news_articles = list(unique_articles)\n",
    "print(f\"We have {len(unique_news_articles)} unique articles in our database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Saving Data to the Vector Store\nWith the Vector store set up, the next step is to populate it with data. We save the BBC articles dataset to the vector store. For each document, we will generate the embeddings for the article to use with the semantic search using LangChain. Here one of the articles is larger than the maximum tokens that we can use for our embedding model. If we want to ingest that document, we could split the document and ingest it in parts. However, since it is only a single document for simplicity, we ignore that document from the ingestion process."
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the current logging level\n",
    "current_logging_level = logging.getLogger().getEffectiveLevel()\n",
    "\n",
    "# # Set logging level to CRITICAL to suppress lower level logs\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "articles = [article for article in unique_news_articles if article and len(article) <= 50000]\n",
    "\n",
    "try:\n",
    "    vector_store.add_texts(\n",
    "        texts=articles\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to save documents to vector store: {str(e)}\")\n",
    "\n",
    "# Restore the original logging level\n",
    "logging.getLogger().setLevel(current_logging_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## PydanticAI: An Introduction\nFrom [PydanticAI](https://ai.pydantic.dev/)'s website:\n\n> PydanticAI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI.\n\nPydanticAI allows us to define agents and tools easily to create Gen-AI apps in an innovative and painless manner. Some of its features are:\n- Built by the Pydantic Team: Built by the team behind Pydantic (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more).\n\n- Model-agnostic: Supports OpenAI, Anthropic, Gemini, Deepseek, Ollama, Groq, Cohere, and Mistral, and there is a simple interface to implement support for other models.\n\n- Type-safe: Designed to make type checking as powerful and informative as possible for you.\n\n- Python-centric Design: Leverages Python's familiar control flow and agent composition to build your AI-driven projects, making it easy to apply standard Python best practices you'd use in any other (non-AI) project.\n\n- Structured Responses: Harnesses the power of Pydantic to validate and structure model outputs, ensuring responses are consistent across runs.\n\n- Dependency Injection System: Offers an optional dependency injection system to provide data and services to your agent's system prompts, tools and result validators. This is useful for testing and eval-driven iterative development.\n\n- Streamed Responses: Provides the ability to stream LLM outputs continuously, with immediate validation, ensuring rapid and accurate results.\n\n- Graph Support: Pydantic Graph provides a powerful way to define graphs using typing hints, this is useful in complex applications where standard control flow can degrade to spaghetti code.\n\n### Building a RAG Agent using PydanticAI\n\nPydanticAI makes heavy use of dependency injection to provide data and services to your agent's system prompts and tools. We define dependencies using a `dataclass`, which serves as a container for our dependencies.\n\nIn our case, the only dependency for our agent to work in the `CouchbaseSearchVectorStore` instance. However, we will still use a `dataclass` as it is good practice. In the future, in case we wish to add more dependencies, we can just add more fields to the `dataclass` `Deps`.\n\nWe also initialize an agent as a GPT-4o model. PydanticAI supports many different LLM providers, including Anthropic, Google, Cohere, etc. which can also be used. While initializing the agent, we also pass the type of the dependencies. This is mainly used for type checking, and not actually used at runtime."
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Deps:\n",
    "    vector_store: CouchbaseSearchVectorStore\n",
    "\n",
    "agent = Agent(\"openai:gpt-4o\", deps_type=Deps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Defining the Vector Store as a Tool\nPydanticAI has the concept of `function tools`, which are functions that can be called by LLMs to retrieve extra information that can help form a better response.\n\nWe can perform RAG by creating a tool which retrieves documents that are semantically similar to the query, and allowing the agent to call the tool when required. We can add the function as a tool using the `@agent.tool` decorator.\n\nNotice that we also add the `context` parameter, which contains the dependencies that are passed to the tool (in this case, the only dependency is the vector store)."
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@agent.tool\n",
    "async def retrieve(context: RunContext[Deps], search_query: str) -> str:\n",
    "    \"\"\"Retrieve news data based on a search query.\n",
    "\n",
    "    Args:\n",
    "        context: The call context\n",
    "        search_query: The search query\n",
    "    \"\"\"\n",
    "    search_results = context.deps.vector_store.similarity_search_with_score(search_query, k=5)\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"# Documents:\\n{doc.page_content}\"\n",
    "        for doc, score in search_results\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a function that allows us to define our dependencies and run our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_agent(question: str):\n",
    "    deps = Deps(\n",
    "        vector_store=vector_store,\n",
    "    )\n",
    "    answer = await agent.run(question, deps=deps)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Running our Agent\nWe have now finished setting up our vector store and agent! The system is now ready to accept queries."
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 13:56:53,839 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-11 13:56:54,485 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-11 13:57:01,928 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Agent Output ====================\n",
      "Pep Guardiola has expressed a mix of determination and concern regarding Manchester City's current form. He acknowledged the personal impact of the team's downturn, admitting that the situation has affected his sleep and diet due to the worst run of results he has ever faced in his managerial career. Guardiola described his state of mind as \"ugly,\" noting the team's precarious position in competitions and the need to defend better and avoid mistakes.\n",
      "\n",
      "Despite these challenges, Guardiola remains committed to finding solutions, emphasizing the need to improve defensive concepts and restore the team's intensity and form. He acknowledged the errors from some of the best players in the world and expressed a need for the team to stay positive and for players to have the necessary support to overcome their current struggles.\n",
      "\n",
      "Moreover, Guardiola expressed a pragmatic view of the situation, accepting that the team must \"survive\" the season and acknowledging a potential need for a significant rebuild to address the challenges they're facing. As a testament to his commitment, he noted his intention to continue shaping the club during his newly extended contract period. Throughout, he reiterated his belief in the team and emphasized the need to find a way forward.\n"
     ]
    }
   ],
   "source": [
    "query = \"What was manchester city manager pep guardiola's reaction to the team's current form?\"\n",
    "output = await run_agent(query)\n",
    "\n",
    "print(\"=\" * 20, \"Agent Output\", \"=\" * 20)\n",
    "print(output.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Inspecting the Agent\nWe can use the `all_messages()` method in the output object to observe how the agent and tools work.\n\nIn the cell below, we see an extremely detailed list of all the model's messages and tool calls, which happens step by step:\n1. The `UserPromptPart`, which consists of the query the user sends to the agent.\n2. The agent calls the `retrieve` tool in the `ToolCallPart` message. This includes the `search_query` argument. Couchbase uses this `search_query` to perform semantic search over all the ingested news articles.\n3. The `retrieve` tool returns a `ToolReturnPart` object with all the context required for the model to answer the user's query. The retrieve documents were truncated, because a large amount of context was retrieved. \n4. The final message is the LLM generated response with the added context, which is sent back to the user."
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "('ModelRequest(parts=[UserPromptPart(content=\"What was manchester city manager '\n",
      " 'pep guardiola\\'s reaction to the team\\'s current form?\", '\n",
      " 'timestamp=datetime.datetime(2025, 4, 11, 8, 26, 52, 836357, '\n",
      " \"tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request')\")\n",
      "==================================================\n",
      "Step 2:\n",
      "(\"ModelResponse(parts=[ToolCallPart(tool_name='retrieve', \"\n",
      " 'args=\\'{\"search_query\":\"Pep Guardiola reaction to Manchester City current '\n",
      " 'form\"}\\', tool_call_id=\\'call_oo4Jjn93VkRJ3q9PnAwkt3xm\\', '\n",
      " \"part_kind='tool-call')], model_name='gpt-4o-2024-08-06', \"\n",
      " 'timestamp=datetime.datetime(2025, 4, 11, 8, 26, 53, '\n",
      " \"tzinfo=datetime.timezone.utc), kind='response')\")\n",
      "==================================================\n",
      "Step 3:\n",
      "(\"ModelRequest(parts=[ToolReturnPart(tool_name='retrieve', content='# \"\n",
      " 'Documents:\\\\nManchester City boss Pep Guardiola has won 18 trophies since he '\n",
      " 'arrived at the club in 2016\\\\n\\\\nManchester City boss Pep Guardiola says he '\n",
      " 'is \"fine\" despite admitting his sleep and diet are being affected by the '\n",
      " 'worst run of results in his entire managerial career. In an interview with '\n",
      " 'former Italy international Luca Toni for Amazon Prime Sport before '\n",
      " \"Wednesday\\\\'s Champions League defeat by Juventus, Guardiola touched on the \"\n",
      " \"personal impact City\\\\'s sudden downturn in form has had. Guardiola said his \"\n",
      " 'state of mind was \"ugly\", that his sleep was \"worse\" and he was eating '\n",
      " \"lighter as his digestion had suffered. City go into Sunday\\\\'s derby against \"\n",
      "\n... (output truncated for brevity)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for idx, message in enumerate(output.all_messages(), start=1):\n",
    "    print(f\"Step {idx}:\")\n",
    "    pprint(message.__repr__())\n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}