{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44480f12-3bd0-4fe9-9493-25bd6a2712bb",
   "metadata": {},
   "source": [
    "# Auto-Vectorization Using Couchbase Capella AI Services\n",
    "\n",
    "This comprehensive tutorial demonstrates how to use Couchbase Capella's new AI Services auto-vectorization feature to automatically convert your data into vector embeddings and perform semantic search using LangChain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502eb13e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Create and Deploy Operational Cluster on Capella\n",
    "To get started with Couchbase Capella, create an account and use it to deploy a cluster. \n",
    "\n",
    "Make sure that you deploy a `Multi-node` cluster with `data`, `index`, `query` and `eventing` services enabled. To know more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html).\n",
    " ### Couchbase Capella Configuration\n",
    " When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met:\n",
    "   * Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the travel-sample bucket (Read and Write) used in the application.\n",
    "   * [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4369c925-adbc-4c7d-9ea6-04ff020cb1a6",
   "metadata": {},
   "source": [
    "# 2. Data Upload and Preparation\n",
    "\n",
    "There are various techniques that exist to insert data into the cluster. To read about the techniques, please follow the [sample-data import](https://docs.couchbase.com/cloud/clusters/data-service/import-data-documents.html#import-sample-data) guide.\n",
    "\n",
    "After data upload is complete, follow the next steps to achieve vectorization for your required fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3afd3f-9949-4f5e-b96a-1aac1a3aea29",
   "metadata": {},
   "source": [
    "# 3. Deploying the Model\n",
    "Now, before we actually create embeddings for the documents, we need to deploy a model that will create the embeddings for us.\n",
    "## 3.1: Selecting the Model \n",
    "1. To select the model, you first need to navigate to the \"<B>AI Services</B>\" tab, then select \"<B>Models</B>\" and click on \"<B>Deploy New Model</B>\".\n",
    "   \n",
    "   <img src=\"./img/importing_model.png\" width=\"950px\" height=\"500px\" style=\"padding: 5px; border-radius: 10px 20px 30px 40px; border: 2px solid #555;\">\n",
    "\n",
    "2. Enter the <B>model name</B>, and choose the model that you want to deploy. After selecting your model, choose the <B>model infrastructure</B> and <B>region</B> where the model will be deployed.\n",
    "   \n",
    "   <img src=\"./img/deploying_model.png\" width=\"800px\" height=\"800px\" style=\"padding: 5px; border-radius: 10px 20px 30px 40px; border: 2px solid #555;\">\n",
    "\n",
    "## 3.2 Access Control to the Model\n",
    "\n",
    "1. After deploying the model, go to the \"<B>Models</B>\" tab in the <B>AI Services</B> and click on \"<B>Setup Access</B>\".\n",
    "\n",
    "    <img src=\"./img/model_setup_access.png\" width=\"1100px\" height=\"400px\" style=\"padding: 5px; border-radius: 10px 20px 30px 40px; border: 2px solid #555;\">\n",
    "\n",
    "2. Enter your <B>API key name</B>, <B>expiration time</B> and the <B>IP address</B> from which you will be accessing the model.\n",
    "\n",
    "    <img src=\"./img/model_api_key_form.png\" width=\"1100px\" height=\"600px\" style=\"padding: 5px; border-radius: 10px 20px 30px 40px; border: 2px solid #555;\">\n",
    "\n",
    "3. Download your API key\n",
    "\n",
    "   <img src=\"./img/download_api_key_details.png\" width=\"1200px\" height=\"800px\" style=\"padding: 5px; border-radius: 10px 20px 30px 40px; border: 2px solid #555;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf6525-d4e6-45fb-8839-fc7c20081675",
   "metadata": {},
   "source": [
    "# 4. Deploying AutoVectorization Workflow\n",
    "\n",
    "Now, we are at the step that will help us create the embeddings/vectors. To proceed with the vectorization process, please follow the steps below:\n",
    "\n",
    "1. For deploying the autovectorization, you need to go to the <B>`AI Services`</B> tab, then click on <B>`Workflows`</B>, and then click on <B>`Create New Workflow`</B>.\n",
    "\n",
    "   <img src=\"./img/workflow.png\" width=\"1000px\" height=\"500px\" style=\"padding: 5px; border-radius: 10px 20px 30px 40px; border: 2px solid #555;\">\n",
    "   \n",
    "2. Start your workflow deployment by giving it a name and selecting where your data will be provided to the auto-vectorization service. There are currently 3 options: <B>`pre-processed data (JSON format) from Capella`</B>, <B>`pre-processed data (JSON format) from external sources (S3 buckets)`</B> and <B>`unstructured data from external sources (S3 buckets)`</B>. For this tutorial, we will choose the first option, which is pre-processed data from Capella.\n",
    "\n",
    "   <img src=\"./img/start_workflow.png\" width=\"1000px\" height=\"500px\" style=\"padding: 5px; border-radius: 10px 20px 30px 40px; border: 2px solid #555;\">\n",
    "\n",
    "3. Now, select the <B>`cluster`</B>, <B>`bucket`</B>, <B>`scope`</B> and <B>`collection`</B> from which you want to select the documents and get the data vectorized.\n",
    "\n",
    "   <img src=\"./img/vector_data_source.png\" width=\"1000px\" height=\"500px\" style=\"padding: 5px; border-radius: 10px 20px 30px 40px; border: 2px solid #555;\">\n",
    "\n",
    "4. <B>Field Mapping</B> will be used to tell the AutoVectorize service which data will be converted to embeddings.\n",
    "\n",
    "   There are two options:\n",
    "\n",
    "   - <B>All source fields</B> - This feature will convert all your fields inside the document to a single vector field.\n",
    "   \n",
    "     <img src=\"./img/vector_all_field_mapping.png\" width=\"900px\" height=\"400px\" style=\"padding: 5px; border-radius: 10px 20px 30px 40px; border: 2px solid #555;\">\n",
    "\n",
    "\n",
    "   - <B>Custom source fields</B> - This feature will convert specific fields chosen by the user to a single vector field. In the image below, we have chosen <B>`address`</B>, <B>`description`</B> and <B>`id`</B> as the fields to be converted to a vector with the name <B>`vec_addr_decr_id_mapping`</B>.\n",
    "  \n",
    "       <img src=\"./img/vector_custom_field_mapping.png\" width=\"900px\" height=\"400px\" style=\"padding: 5px; border-radius: 10px 20px 30px 40px; border: 2px solid #555;\">\n",
    "  \n",
    "5. After choosing the type of mapping, it is required to either create an index on the new vector_embedding field or the creation of a vector index can be skipped, which is not recommended as the functionality of vector searching will be lost.\n",
    "\n",
    "   <img src=\"./img/vector_index.png\" width=\"1000px\" height=\"500px\" style=\"padding: 5px; border-radius: 10px 20px 30px 40px; border: 2px solid #555;\">\n",
    "\n",
    "6. Below screenshot highlights the whole process which were mentioned above, and click next afterwards as shown below.\n",
    "\n",
    "   <img src=\"./img/vector_index_page.png\" width=\"900px\" height=\"1200px\" style=\"padding: 5px; border-radius: 10px 20px 30px 40px; border: 2px solid #555;\">\n",
    "\n",
    "\n",
    "7. Select the model which will be used to create the embeddings. There are two options to create the embeddings, `capella based` and `external model`.\n",
    "   \n",
    "   <img src=\"./img/Select_embedding_model.png\" width=\"500px\" height=\"500px\" style=\"padding: 5px; border-radius: 10px 20px 30px 40px; border: 2px solid #555;\">\n",
    "\n",
    "   - For this tutorial, capella based embedding model is used as can be seen in the image above. API credentials can be uploaded using the file downloaded in `step 2.2` or it can be entered manually as well.\n",
    "   - Choices between private and insecure networking is available to choose.\n",
    "   - A click on `Next` will land you at the final page of the workflow.\n",
    "\n",
    "\n",
    "\n",
    "8.  <B>`Workflow Summary`</B> will display all the necessary details of the workflow including `Data Source`, `Model Service` and `Billing Overview` as shown in image below.\n",
    "\n",
    "    <img src=\"./img/workflow_summary.png\" width=\"500px\" height=\"500px\" style=\"padding: 5px; border-radius: 10px 20px 30px 40px; border: 2px solid #555;\">\n",
    "\n",
    "\n",
    "\n",
    "9. <B>`Hurray! Workflow Deployed`</B> Now in the `workflow` tab we can see the workflow deployed and can check the status of our workflow run.\n",
    "\n",
    "      <img src=\"./img/workflow_deployed.png\" width=\"1150px\" height=\"500px\" style=\"padding: 5px; border-radius: 10px 20px 30px 40px; border: 2px solid #555;\">\n",
    "\n",
    "After this step, your vector embeddings for the selected fields should be ready, and you can check them out in the Capella UI. In the next step, we will demonstrate how we can use the generated vectors to perform vector search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50204a4",
   "metadata": {},
   "source": [
    "# 5. Vector Search\n",
    "\n",
    "The following code cells implement semantic vector search against the embeddings generated by the AutoVectorization workflow. These searches are powered by **Couchbase's Search service**.\n",
    "\n",
    "Before you proceed, make sure the following packages are installed by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d38e3de",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain-couchbase langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1854af3",
   "metadata": {},
   "source": [
    "`langchain-couchbase - Version: 0.4.0` \\\n",
    "`pip install langchain-openai - Version: 0.3.34`\n",
    "\n",
    "Now, please proceed to execute the cells in order to run the vector similarity search.\n",
    "\n",
    "# Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30955126-0053-4cec-9dec-e4c05a8de7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from couchbase.cluster import Cluster\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.options import ClusterOptions\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_couchbase.vectorstores import CouchbaseSearchVectorStore\n",
    "\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be1f01",
   "metadata": {},
   "source": [
    "# Cluster Connection Setup\n",
    "   - Defines the secure connection string, user credentials, and creates a `Cluster` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e4c9e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"couchbases://cb.f-znsfdbilcp-ja4.sandbox.nonprod-project-avengers.com\"                                              # Replace this with Connection String\n",
    "username = \"testing\"                                                          # Replace this with your username\n",
    "password = \"Testing@1\"                                                          # Replace this with your password\n",
    "auth = PasswordAuthenticator(username, password)\n",
    "\n",
    "options = ClusterOptions(auth, tls_verify='none')\n",
    "cluster = Cluster(endpoint, options)\n",
    "\n",
    "cluster.wait_until_ready(timedelta(seconds=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeb8a4f",
   "metadata": {},
   "source": [
    "# Selection of Buckets / Scope / Collection / Index / Embedder\n",
    "   - Sets the bucket, scope, and collection where the documents (with vector fields) live.\n",
    "   - `index_name` specifies the **Capella Search index name**. This is the Search index created automatically during the workflow setup (step 4.5) or manually as described in the same step. You can find this index name in the **Search** tab of your Capella cluster.\n",
    "   - `embedder` instantiates the NVIDIA embedding model that will transform the user's natural language query into a vector at search time.\n",
    "       - `open_api_key` is the api key token created in `step 3.2 -3`.\n",
    "       - `open_api_base` is the Capella model services endpoint found in the models section.\n",
    "       - for more details visit [openAIEmbeddings](https://docs.langchain.com/oss/python/integrations/text_embedding/openai).\n",
    "\n",
    "`Note that the Capella AI Endpoint also requires an additional /v1 from the endpoint if not shown on the UI`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"travel-sample\"\n",
    "scope_name = \"inventory\"\n",
    "collection_name = \"hotel\"\n",
    "index_name = \"hyperscale_autovec_workflow_vec_addr_descr_id\"  # This is the name of the search index that was created in step 4.5 and can also be seen in the search tab of the cluster.\n",
    "                                                          # It should be noted that hyperscale_workflow_name_index_fieldname is the naming convention for the index created by AutoVectorization workflow where\n",
    "                                                          # fieldname is the name of the field being indexed.\n",
    "\n",
    "#  Using the OpenAI SDK for the embeddings with the capella model services and they are compatible with the OpenAIEmbeddings class in Langchain\n",
    "embedder = OpenAIEmbeddings(\n",
    "    model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\",                        # This is the model that will be used to create the embedding of the query.\n",
    "    openai_api_key=\"CAPELLA_MODEL_KEY\",\n",
    "    openai_api_base=\"CAPELLA_MODEL_ENDPOINT/v1\",\n",
    "    check_embedding_ctx_length=False,\n",
    "    tiktoken_enabled=False,                                                            \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda36710",
   "metadata": {},
   "source": [
    "# VectorStore Construction\n",
    "   - Creates a [CouchbaseSearchVectorStore](https://couchbase-ecosystem.github.io/langchain-couchbase/langchain_couchbase.html#couchbase-search-vector-store) instance that interfaces with **Couchbase's Search service** to perform vector similarity searches.\n",
    "   - The vector store:\n",
    "     * Knows where to read documents (`bucket/scope/collection`).\n",
    "     * References the Search index (`index_name`) that contains vector field mappings.\n",
    "     * Knows the embedding field (the vector produced by the Auto-Vectorization workflow).\n",
    "     * Uses the provided embedder to embed queries on-demand for similarity search.\n",
    "   - If your AutoVectorization workflow produced a different vector field name, update `embedding_key` accordingly.\n",
    "   - If you mapped multiple fields into a single vector, you can choose any representative field for `text_key`, or modify the VectorStore wrapper to concatenate fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50b85f78",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index hyperscale_autovec_workflow_vec_addr_descr_id does not exist.  Please create the index before searching.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m vector_store = \u001b[43mCouchbaseSearchVectorStore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcluster\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcluster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscope_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscope_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_key\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maddress\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# Your document's text field\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_key\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvec_addr_descr_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# This is the field in which your vector (embedding) is stored in the cluster.\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vector-search-cookbook/.venv/lib/python3.14/site-packages/langchain_couchbase/vectorstores/search_vector_store.py:267\u001b[39m, in \u001b[36mCouchbaseSearchVectorStore.__init__\u001b[39m\u001b[34m(self, cluster, bucket_name, scope_name, collection_name, embedding, index_name, text_key, embedding_key, scoped_index)\u001b[39m\n\u001b[32m    265\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_index_exists()\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vector-search-cookbook/.venv/lib/python3.14/site-packages/langchain_couchbase/vectorstores/search_vector_store.py:265\u001b[39m, in \u001b[36mCouchbaseSearchVectorStore.__init__\u001b[39m\u001b[34m(self, cluster, bucket_name, scope_name, collection_name, embedding, index_name, text_key, embedding_key, scoped_index)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# Check if the index exists. Throws ValueError if it doesn't\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_index_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vector-search-cookbook/.venv/lib/python3.14/site-packages/langchain_couchbase/vectorstores/search_vector_store.py:192\u001b[39m, in \u001b[36mCouchbaseSearchVectorStore._check_index_exists\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    188\u001b[39m     all_indexes = [\n\u001b[32m    189\u001b[39m         index.name \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._scope.search_indexes().get_all_indexes()\n\u001b[32m    190\u001b[39m     ]\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._index_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_indexes:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    193\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._index_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    194\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m Please create the index before searching.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    195\u001b[39m         )\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    197\u001b[39m     all_indexes = [\n\u001b[32m    198\u001b[39m         index.name \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cluster.search_indexes().get_all_indexes()\n\u001b[32m    199\u001b[39m     ]\n",
      "\u001b[31mValueError\u001b[39m: Index hyperscale_autovec_workflow_vec_addr_descr_id does not exist.  Please create the index before searching."
     ]
    }
   ],
   "source": [
    "vector_store = CouchbaseSearchVectorStore(\n",
    "    cluster=cluster,\n",
    "    bucket_name=bucket_name,\n",
    "    scope_name=scope_name,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedder,\n",
    "    index_name=index_name,\n",
    "    text_key=\"address\",                  # Your document's text field\n",
    "    embedding_key=\"vec_addr_descr_id\"    # This is the field in which your vector (embedding) is stored in the cluster.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be207963",
   "metadata": {},
   "source": [
    "# Performing a Similarity Search\n",
    "   - Defines a natural language query (e.g., \"Woodhead Road\").\n",
    "   - Calls `similarity_search(k=3)` to retrieve the top 3 most semantically similar documents.\n",
    "   - Prints ranked results, extracting a `title` (if present) and the chosen `text_key` (here `address`).\n",
    "   - Change `query` to any descriptive phrase (e.g., \"beach resort\", \"airport hotel near NYC\").\n",
    "   - Adjust `k` for more or fewer results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177fd6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What hotels are there in USA?\"\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "# Print out the top-k results\n",
    "for rank, doc in enumerate(results, start=1):\n",
    "    title = doc.metadata.get(\"title\", \"<no title>\")\n",
    "    address_text = doc.page_content\n",
    "    print(f\"{rank}. {title} — Address: {address_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e0d863",
   "metadata": {},
   "source": [
    "## 6. Results and Interpretation\n",
    "\n",
    "As we can see, 3 (or `k`) ranked results are printed in the output.\n",
    "\n",
    "### What Each Part Means\n",
    "- Leading number (1, 2, 3): The result rank (1 = most similar to your query).\n",
    "- Title: Pulled from `doc.metadata.get(\"title\", \"<no title>\")`. If your documents don't contain a `title` field, you will see `<no title>`.\n",
    "- Address text: This is the value of the field you configured as `text_key` (in this tutorial: `address`). It represents the human-readable content we chose to display.\n",
    "\n",
    "### How the Ranking Works\n",
    "1. Your natural language query (e.g., `\"Woodhead Road\"`) is embedded using the NVIDIA model (`nvidia/llama-3.2-nv-embedqa-1b-v2`).\n",
    "2. The vector store compares the query embedding to stored document embeddings in the field you configured (`embedding_key = \"vec_addr_descr_id\"`).\n",
    "3. Results are sorted by vector similarity. Higher similarity = closer semantic meaning.\n",
    "\n",
    "\n",
    "> Your vector search pipeline is working if the returned documents feel meaningfully related to your natural language query—even when exact keywords do not match. Feel free to experiment with increasingly descriptive queries to observe the semantic power of the embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
