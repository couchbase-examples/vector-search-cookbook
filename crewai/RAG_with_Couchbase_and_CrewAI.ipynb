{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this guide, we will walk you through building a powerful semantic search engine using [Couchbase](https://www.couchbase.com) as the backend database and [CrewAI](https://github.com/crewAIInc/crewAI) for agent-based RAG operations. CrewAI allows us to create specialized agents that can work together to handle different aspects of the RAG workflow, from document retrieval to response generation. This tutorial is designed to be beginner-friendly, with clear, step-by-step instructions that will equip you with the knowledge to create a fully functional semantic search system from scratch.\n",
    "\n",
    "How to run this tutorial\n",
    "----------------------\n",
    "This tutorial is available as a Jupyter Notebook (.ipynb file) that you can run \n",
    "interactively. You can access the original notebook here.\n",
    "\n",
    "You can either:\n",
    "- Download the notebook file and run it on [Google Colab](https://colab.research.google.com)\n",
    "- Run it on your system by setting up the Python environment\n",
    "\n",
    "Before you start\n",
    "---------------\n",
    "\n",
    "1. Create and Deploy Your Free Tier Operational cluster on [Capella](https://cloud.couchbase.com/sign-up)\n",
    "   - To get started with [Couchbase Capella](https://cloud.couchbase.com), create an account and use it to deploy \n",
    "     a forever free tier operational cluster\n",
    "   - This account provides you with an environment where you can explore and learn \n",
    "     about Capella with no time constraint\n",
    "   - To know more, please follow the [Getting Started Guide](https://docs.couchbase.com/cloud/get-started/create-account.html)\n",
    "\n",
    "2. Couchbase Capella Configuration\n",
    "   When running Couchbase using Capella, the following prerequisites need to be met:\n",
    "   - Create the database credentials to access the [travel-sample bucket](https://docs.couchbase.com/server/current/manage/manage-settings/install-sample-buckets.html) (Read and Write) \n",
    "     used in the application\n",
    "   - Allow access to the Cluster from the IP on which the application is running by following the [Network Security documentation](https://docs.couchbase.com/cloud/security/security.html#public-access)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Stage: Installing Necessary Libraries\n",
    "\n",
    "We'll install the following key libraries:\n",
    "- `datasets`: For loading and managing our training data\n",
    "- `langchain-couchbase`: To integrate Couchbase with LangChain for vector storage and caching\n",
    "- `langchain-openai`: For accessing OpenAI's embedding and chat models\n",
    "- `crewai`: To create and orchestrate our AI agents for RAG operations\n",
    "- `python-dotenv`: For securely managing environment variables and API keys\n",
    "- `tqdm`: For displaying progress bars during data processing\n",
    "\n",
    "These libraries provide the foundation for building a semantic search engine with vector embeddings, \n",
    "database integration, and agent-based RAG capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet datasets langchain-couchbase langchain-openai crewai python-dotenv tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries\n",
    "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import getpass\n",
    "import tqdm\n",
    "from uuid import uuid4\n",
    "from datetime import timedelta\n",
    "\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.diagnostics import PingState, ServiceType\n",
    "from couchbase.exceptions import (CouchbaseException,\n",
    "                                  InternalServerFailureException)\n",
    "from couchbase.management.search import SearchIndex\n",
    "from couchbase.options import ClusterOptions\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from langchain.tools import Tool\n",
    "from langchain_couchbase.vectorstores import CouchbaseVectorStore\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from crewai import Agent, Crew, Process, Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Logging\n",
    "Logging is configured to track the progress of the script and capture any errors or warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Suppress httpx logging\n",
    "logging.getLogger('httpx').setLevel(logging.CRTITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Sensitive Informnation\n",
    "In this section, we prompt the user to input essential configuration settings needed. These settings include sensitive information like database credentials, and specific configuration names. Instead of hardcoding these details into the script, we request the user to provide them at runtime, ensuring flexibility and security.\n",
    "\n",
    "The script uses environment variables to store sensitive information, enhancing the overall security and maintainability of your code by avoiding hardcoded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or input(\"Enter your OpenAI API key: \")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "CB_HOST = os.getenv('CB_HOST') or input(\"Enter Couchbase host (default: couchbase://localhost): \") or 'couchbase://localhost'\n",
    "CB_USERNAME = os.getenv('CB_USERNAME') or input(\"Enter Couchbase username (default: Administrator): \") or 'Administrator'\n",
    "CB_PASSWORD = os.getenv('CB_PASSWORD') or getpass.getpass(\"Enter Couchbase password (default: password): \") or 'password'\n",
    "CB_BUCKET_NAME = os.getenv('CB_BUCKET_NAME') or input(\"Enter bucket name (default: vector-search-testing): \") or 'vector-search-testing'\n",
    "INDEX_NAME = os.getenv('INDEX_NAME') or input(\"Enter index name (default: vector_search_crew): \") or 'vector_search_crew'\n",
    "SCOPE_NAME = os.getenv('SCOPE_NAME') or input(\"Enter scope name (default: shared): \") or 'shared'\n",
    "COLLECTION_NAME = os.getenv('COLLECTION_NAME') or input(\"Enter collection name (default: crew): \") or 'crew'\n",
    "\n",
    "print(\"Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the Couchbase Cluster\n",
    "Connecting to a Couchbase cluster is the foundation of our project. Couchbase will serve as our primary data store, handling all the storage and retrieval operations required for our semantic search engine. By establishing this connection, we enable our application to interact with the database, allowing us to perform operations such as storing embeddings, querying data, and managing collections. This connection is the gateway through which all data will flow, so ensuring it's set up correctly is paramount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Couchbase\n",
    "try:\n",
    "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "    options = ClusterOptions(auth)\n",
    "    cluster = Cluster(CB_HOST, options)\n",
    "    cluster.wait_until_ready(timedelta(seconds=5))\n",
    "    print(\"Successfully connected to Couchbase\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to Couchbase: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying Search Service Availability\n",
    " In this section, we verify that the Couchbase Search (FTS) service is available and responding correctly. This is a crucial check because our vector search functionality depends on it. If any issues are detected with the Search service, the function will raise an exception, allowing us to catch and handle problems early before attempting vector operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_search_service(cluster):\n",
    "    \"\"\"Verify search service availability using ping\"\"\"\n",
    "    try:\n",
    "        # Get ping result\n",
    "        ping_result = cluster.ping()\n",
    "        search_available = False\n",
    "        \n",
    "        # Check if search service is responding\n",
    "        for service_type, endpoints in ping_result.endpoints.items():\n",
    "            if service_type == ServiceType.Search:\n",
    "                for endpoint in endpoints:\n",
    "                    if endpoint.state == PingState.OK:\n",
    "                        search_available = True\n",
    "                        print(f\"Search service is responding at: {endpoint.remote}\")\n",
    "                        break\n",
    "                break\n",
    "\n",
    "        if not search_available:\n",
    "            raise RuntimeError(\"Search/FTS service not found or not responding\")\n",
    "        \n",
    "        print(\"Search service check passed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Health check failed: {str(e)}\")\n",
    "        raise\n",
    "try:\n",
    "    check_search_service(cluster)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to check search service: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Collections in Couchbase\n",
    "In Couchbase, data is organized in buckets, which can be further divided into scopes and collections. Think of a collection as a table in a traditional SQL database. Before we can store any data, we need to ensure that our collections exist. If they don't, we must create them. This step is important because it prepares the database to handle the specific types of data our application will process. By setting up collections, we define the structure of our data storage, which is essential for efficient data retrieval and management.\n",
    "\n",
    "Moreover, setting up collections allows us to isolate different types of data within the same bucket, providing a more organized and scalable data structure. This is particularly useful when dealing with large datasets, as it ensures that related data is stored together, making it easier to manage and query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup collections\n",
    "try:\n",
    "    bucket = cluster.bucket(CB_BUCKET_NAME)\n",
    "    bucket_manager = bucket.collections()\n",
    "\n",
    "    # Setup main collection\n",
    "    collections = bucket_manager.get_all_scopes()\n",
    "    collection_exists = any(\n",
    "        scope.name == SCOPE_NAME and COLLECTION_NAME in [col.name for col in scope.collections]\n",
    "        for scope in collections\n",
    "    )\n",
    "\n",
    "    if not collection_exists:\n",
    "        try:\n",
    "            bucket_manager.create_collection(SCOPE_NAME, COLLECTION_NAME)\n",
    "            print(f\"Collection '{COLLECTION_NAME}' created\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create collection '{COLLECTION_NAME}': {str(e)}\")\n",
    "            raise\n",
    "    else:\n",
    "        print(f\"Collection '{COLLECTION_NAME}' already exists\")\n",
    "\n",
    "    # Create primary index\n",
    "    try:\n",
    "        cluster.query(\n",
    "            f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{CB_BUCKET_NAME}`.`{SCOPE_NAME}`.`{COLLECTION_NAME}`\"\n",
    "        ).execute()\n",
    "        print(f\"Primary index created for '{COLLECTION_NAME}'\")\n",
    "    except InternalServerFailureException as e:\n",
    "        print(f\"Failed to create primary index for '{COLLECTION_NAME}': {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    # Clear collection\n",
    "    try:\n",
    "        cluster.query(\n",
    "            f\"DELETE FROM `{CB_BUCKET_NAME}`.`{SCOPE_NAME}`.`{COLLECTION_NAME}`\"\n",
    "        ).execute()\n",
    "        print(f\"Collection '{COLLECTION_NAME}' cleared\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to clear collection '{COLLECTION_NAME}': {str(e)}\")\n",
    "        raise\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during setup: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring and Initializing Couchbase Vector Search Index for Semantic Document Retrieval\n",
    "\n",
    "Semantic search requires an efficient way to retrieve relevant documents based on a user's query. This is where the Couchbase Vector Search Index comes into play. In this step, we load the Vector Search Index definition from a JSON file, which specifies how the index should be structured. This includes the fields to be indexed, the dimensions of the vectors, and other parameters that determine how the search engine processes queries based on vector similarity.\n",
    "\n",
    "This CrewAI vector search index configuration requires specific default settings to function properly. This tutorial uses the bucket named `vector-search-testing` with the scope `shared` and collection `crew`. The configuration is set up for vectors with exactly `1536 dimensions`, using `dot product` similarity and optimized for `recall`. If you want to use a different bucket, scope, or collection, you will need to modify the index configuration accordingly.\n",
    "\n",
    "For more information on creating a vector search index, please follow the instructions at [Couchbase Vector Search Documentation](https://docs.couchbase.com/cloud/vector-search/create-vector-search-index-ui.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load index definition\n",
    "    try:\n",
    "        with open('crew_index.json', 'r') as file:\n",
    "            index_definition = json.load(file)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: crew_index.json file not found: {str(e)}\")\n",
    "        raise\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Invalid JSON in crew_index.json: {str(e)}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading index definition: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    # Setup vector search index\n",
    "    try:\n",
    "        scope_index_manager = cluster.bucket(CB_BUCKET_NAME).scope(SCOPE_NAME).search_indexes()\n",
    "\n",
    "        # Check if index exists\n",
    "        try:\n",
    "            existing_indexes = scope_index_manager.get_all_indexes()\n",
    "            index_exists = any(index.name == INDEX_NAME for index in existing_indexes)\n",
    "        except InternalServerFailureException:\n",
    "            # If no indexes exist yet, continue with creation\n",
    "            index_exists = False\n",
    "            print(\"No existing indexes found, proceeding with index creation\")\n",
    "\n",
    "        if index_exists:\n",
    "            print(f\"Index '{INDEX_NAME}' already exists\")\n",
    "        else:\n",
    "            search_index = SearchIndex.from_json(index_definition)\n",
    "            scope_index_manager.upsert_index(search_index)\n",
    "            print(f\"Index '{INDEX_NAME}' created\")\n",
    "    except InternalServerFailureException as e:\n",
    "        print(f\"Internal server error occurred while creating vector search index '{INDEX_NAME}'. This may indicate insufficient system resources or an issue with the Couchbase server configuration: {str(e)}\")\n",
    "        raise\n",
    "    except CouchbaseException as e:\n",
    "        print(f\"A Couchbase-specific error occurred while managing vector search index '{INDEX_NAME}'. Please verify your cluster connectivity and index configuration settings: {str(e)}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while managing vector search index '{INDEX_NAME}'. This requires investigation of system logs for root cause analysis: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Fatal error in vector search index setup: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up OpenAI Components\n",
    "\n",
    "This section initializes two key OpenAI components needed for our RAG system:\n",
    "\n",
    "1. OpenAI Embeddings:\n",
    "   - Uses the 'text-embedding-ada-002' model\n",
    "   - Converts text into high-dimensional vector representations (embeddings)\n",
    "   - These embeddings enable semantic search by capturing the meaning of text\n",
    "   - Required for vector similarity search in Couchbase\n",
    "\n",
    "2. ChatOpenAI Language Model:\n",
    "   - Uses the 'gpt-4o' model\n",
    "   - Temperature set to 0.0 for focused responses\n",
    "   - Higher temperatures increase creativity and variation\n",
    "   - Handles the actual text generation and responses\n",
    "   - Acts as the brain of our RAG system for processing retrieved context\n",
    "\n",
    "Both components require a valid OpenAI API key (OPENAI_API_KEY) for authentication.\n",
    "The embeddings model is optimized for creating vector representations,\n",
    "while the language model is optimized for understanding and generating human-like text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI components\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(\"OpenAI components initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Couchbase Vector Store\n",
    "A vector store is where we'll keep our embeddings. Unlike the FTS index, which is used for text-based search, the vector store is specifically designed to handle embeddings and perform similarity searches. When a user inputs a query, the search engine converts the query into an embedding and compares it against the embeddings stored in the vector store. This allows the engine to find documents that are semantically similar to the query, even if they don't contain the exact same words. By setting up the vector store in Couchbase, we create a powerful tool that enables our search engine to understand and retrieve information based on the meaning and context of the query, rather than just the specific words used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup vector store\n",
    "vector_store = CouchbaseVectorStore(\n",
    "    cluster=cluster,\n",
    "    bucket_name=CB_BUCKET_NAME,\n",
    "    scope_name=SCOPE_NAME,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embeddings,\n",
    "    index_name=INDEX_NAME,\n",
    ")\n",
    "print(\"Vector store initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the BBC News Dataset\n",
    "To build a search engine, we need data to search through. We use the BBC News dataset from RealTimeData, which provides real-world news articles. This dataset contains news articles from BBC covering various topics and time periods. Loading the dataset is a crucial step because it provides the raw material that our search engine will work with. The quality and diversity of the news articles make it an excellent choice for testing and refining our search engine, ensuring it can handle real-world news content effectively.\n",
    "\n",
    "The BBC News dataset allows us to work with authentic news articles, enabling us to build and test a search engine that can effectively process and retrieve relevant news content. The dataset is loaded using the Hugging Face datasets library, specifically accessing the \"RealTimeData/bbc_news_alltime\" dataset with the \"2024-12\" version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    news_dataset = load_dataset(\n",
    "        \"RealTimeData/bbc_news_alltime\", \"2024-12\", split=\"train\"\n",
    "    )\n",
    "    print(f\"Loaded the BBC News dataset with {len(news_dataset)} rows\")\n",
    "    logging.info(f\"Successfully loaded the BBC News dataset with {len(news_dataset)} rows.\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading the BBC News dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the Data\n",
    "We will use the content of the news articles for our RAG system.\n",
    "\n",
    "The dataset contains a few duplicate records. We are removing them to avoid duplicate results in the retrieval stage of our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles = news_dataset[\"content\"]\n",
    "unique_articles = set()\n",
    "for article in news_articles:\n",
    "    if article:\n",
    "        unique_articles.add(article)\n",
    "unique_news_articles = list(unique_articles)\n",
    "print(f\"We have {len(unique_news_articles)} unique articles in our database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data to the Vector Store\n",
    "With the Vector store set up, the next step is to populate it with data. We save the BBC articles dataset to the vector store. For each document, we will generate the embeddings for the article to use with the semantic search using LangChain. Here one of the articles is larger than the maximum tokens that we can use for our embedding model. If we want to ingest that document, we could split the document and ingest it in parts. However, since it is only a single document for simplicity, we ignore that document from the ingestion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in tqdm(unique_news_articles, desc=\"Ingesting articles\"):\n",
    "    try:\n",
    "        # Skip articles that exceed the model's token limit (50,000 characters)\n",
    "        if len(article) > 50000:\n",
    "            print(f\"Skipping article with length {len(article)} - exceeds 50,000 character limit\")\n",
    "            continue\n",
    "            \n",
    "        # Convert article into the format needed for add_texts\n",
    "        texts = [article]  # Single article as text\n",
    "        metadatas = [{}]  # Empty metadata dictionary for each article\n",
    "        uuids = [str(uuid4())]  # Generate UUID for the article\n",
    "        \n",
    "        # Use add_texts instead of add_documents\n",
    "        vector_store.add_texts(\n",
    "            texts=texts,\n",
    "            metadatas=metadatas,\n",
    "            ids=uuids\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save documents to vector store: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Vector Search Tool\n",
    "After loading our data into the vector store, we need to create a tool that can efficiently search through these vector embeddings. This involves two key components:\n",
    "\n",
    "## Vector Retriever\n",
    "The vector retriever is configured to perform similarity searches with specific parameters:\n",
    "- k=8: Returns the 8 most similar documents\n",
    "- fetch_k=20: Initially retrieves 20 candidates before filtering to the top 8\n",
    "This two-stage approach helps balance between accuracy and performance.\n",
    "\n",
    "## Search Tool\n",
    "The search tool wraps the retriever in a user-friendly interface that:\n",
    "- Accepts natural language queries\n",
    "- Handles both string and structured query inputs\n",
    "- Formats results with clear document separation\n",
    "- Includes metadata for traceability\n",
    "\n",
    "The tool is designed to integrate seamlessly with our AI agents, providing them with reliable access to our knowledge base through vector similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector retriever\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 8,\n",
    "        \"fetch_k\": 20\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create search tool using retriever\n",
    "search_tool = Tool(\n",
    "    name=\"vector_search\",\n",
    "    func=lambda query: \"\\n\\n\".join([\n",
    "        f\"Document {i+1}:\\n{'-'*40}\\n{doc.page_content}\"\n",
    "        for i, doc in enumerate(retriever.invoke(\n",
    "            query if isinstance(query, str) else str(query.get('query', ''))\n",
    "        ))\n",
    "    ]),\n",
    "    description=\"\"\"Search for relevant documents using vector similarity.\n",
    "    Input should be a simple text query string.\n",
    "    Returns a list of relevant document contents with metadata.\n",
    "    Use this tool to find detailed information about topics.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Vector search tool created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating CrewAI Agents\n",
    "\n",
    "We'll create two specialized AI agents using the CrewAI framework to handle different aspects of our information retrieval and analysis system:\n",
    "\n",
    "## Research Expert Agent\n",
    "This agent is designed to:\n",
    "- Execute semantic searches using our vector store\n",
    "- Analyze and evaluate search results \n",
    "- Identify key information and insights\n",
    "- Verify facts across multiple sources\n",
    "- Synthesize findings into comprehensive research summaries\n",
    "\n",
    "## Technical Writer Agent  \n",
    "This agent is responsible for:\n",
    "- Taking research findings and structuring them logically\n",
    "- Converting technical concepts into clear explanations\n",
    "- Ensuring proper citation and attribution\n",
    "- Maintaining engaging yet informative tone\n",
    "- Producing well-formatted final outputs\n",
    "\n",
    "The agents work together in a coordinated way:\n",
    "1. Research agent finds and analyzes relevant documents\n",
    "2. Writer agent takes those findings and crafts polished responses\n",
    "3. Both agents use a custom response template for consistent output\n",
    "\n",
    "This multi-agent approach allows us to:\n",
    "- Leverage specialized expertise for different tasks\n",
    "- Maintain high quality through separation of concerns\n",
    "- Create more comprehensive and reliable outputs\n",
    "- Scale the system's capabilities efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom response template\n",
    "response_template = \"\"\"\n",
    "Analysis Results\n",
    "===============\n",
    "{%- if .Response %}\n",
    "{{ .Response }}\n",
    "{%- endif %}\n",
    "\n",
    "Sources\n",
    "=======\n",
    "{%- for tool in .Tools %}\n",
    "* {{ tool.name }}\n",
    "{%- endfor %}\n",
    "\n",
    "Metadata\n",
    "========\n",
    "* Confidence: {{ .Confidence }}\n",
    "* Analysis Time: {{ .ExecutionTime }}\n",
    "\"\"\"\n",
    "\n",
    "# Create research agent\n",
    "researcher = Agent(\n",
    "    role='Research Expert',\n",
    "    goal='Find and analyze the most relevant documents to answer user queries accurately',\n",
    "    backstory=\"\"\"You are an expert researcher with deep knowledge in information retrieval \n",
    "    and analysis. Your expertise lies in finding, evaluating, and synthesizing information \n",
    "    from various sources. You have a keen eye for detail and can identify key insights \n",
    "    from complex documents. You always verify information across multiple sources and \n",
    "    provide comprehensive, accurate analyses.\"\"\",\n",
    "    tools=[search_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=True,\n",
    "    allow_delegation=False,\n",
    "    response_template=response_template\n",
    ")\n",
    "\n",
    "# Create writer agent\n",
    "writer = Agent(\n",
    "    role='Technical Writer',\n",
    "    goal='Generate clear, accurate, and well-structured responses based on research findings',\n",
    "    backstory=\"\"\"You are a skilled technical writer with expertise in making complex \n",
    "    information accessible and engaging. You excel at organizing information logically, \n",
    "    explaining technical concepts clearly, and creating well-structured documents. You \n",
    "    ensure all information is properly cited, accurate, and presented in a user-friendly \n",
    "    manner. You have a talent for maintaining the reader's interest while conveying \n",
    "    detailed technical information.\"\"\",\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=True,\n",
    "    allow_delegation=False,\n",
    "    response_template=response_template\n",
    ")\n",
    "\n",
    "print(\"Agents created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Search System\n",
    "\n",
    "Test the system with some example queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query, researcher, writer):\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Create tasks\n",
    "    research_task = Task(\n",
    "        description=f\"Research and analyze information relevant to: {query}\",\n",
    "        agent=researcher,\n",
    "        expected_output=\"A detailed analysis with key findings and supporting evidence\"\n",
    "    )\n",
    "    \n",
    "    writing_task = Task(\n",
    "        description=\"Create a comprehensive and well-structured response\",\n",
    "        agent=writer,\n",
    "        expected_output=\"A clear, comprehensive response that answers the query\",\n",
    "        context=[research_task]\n",
    "    )\n",
    "    \n",
    "    # Create and execute crew\n",
    "    crew = Crew(\n",
    "        agents=[researcher, writer],\n",
    "        tasks=[research_task, writing_task],\n",
    "        process=Process.sequential,\n",
    "        verbose=True,\n",
    "        cache=True,\n",
    "        planning=True\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = crew.kickoff()\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nQuery completed in {elapsed_time:.2f} seconds\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"RESPONSE\")\n",
    "        print(\"=\" * 80)\n",
    "        print(result)\n",
    "        \n",
    "        if hasattr(result, 'tasks_output'):\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"DETAILED TASK OUTPUTS\")\n",
    "            print(\"=\" * 80)\n",
    "            for task_output in result.tasks_output:\n",
    "                print(f\"\\nTask: {task_output.description[:100]}...\")\n",
    "                print(\"-\" * 40)\n",
    "                print(f\"Output: {task_output.raw}\")\n",
    "                print(\"-\" * 40)\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing crew: {str(e)}\")\n",
    "        logging.error(f\"Crew execution failed: {str(e)}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What was manchester city manager pep guardiola's reaction to the team's current form?\"\n",
    "process_query(query, researcher, writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
