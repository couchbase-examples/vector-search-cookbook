{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d610e0",
   "metadata": {},
   "source": [
    "# Agent-Based RAG with Couchbase GSI Vector Search and CrewAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3073978",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e91202c",
   "metadata": {},
   "source": [
    "In this guide, we will walk you through building a powerful semantic search engine using [Couchbase](https://www.couchbase.com) as the backend database and [CrewAI](https://github.com/crewAIInc/crewAI) for agent-based RAG operations. CrewAI allows us to create specialized agents that can work together to handle different aspects of the RAG workflow, from document retrieval to response generation. This tutorial uses Couchbase's **Global Secondary Index (GSI)** vector search capabilities, which offer high-performance vector search optimized for large-scale applications. This tutorial is designed to be beginner-friendly, with clear, step-by-step instructions that will equip you with the knowledge to create a fully functional semantic search system from scratch. For more information on vector indexes, see the [Couchbase documentation](https://docs.couchbase.com/cloud/vector-index/use-vector-indexes.html). Alternatively if you want to perform semantic search using Couchbase Search Vector Index, please take a look at [this.](https://developer.couchbase.com/tutorial-crewai-couchbase-rag-with-search-vector-index/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f3178",
   "metadata": {},
   "source": [
    "## How to Run This Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e84bba4",
   "metadata": {},
   "source": [
    "This tutorial is available as a Jupyter Notebook (.ipynb file) that you can run interactively. You can access the original notebook [here](https://github.com/couchbase-examples/vector-search-cookbook/blob/main/crewai/gsi/RAG_with_Couchbase_and_CrewAI.ipynb).\n",
    "\n",
    "You can either:\n",
    "- Download the notebook file and run it on [Google Colab](https://colab.research.google.com)\n",
    "- Run it on your system by setting up the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202801ea",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bb6aae",
   "metadata": {},
   "source": [
    "### Couchbase Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318f572",
   "metadata": {},
   "source": [
    "1. Create and Deploy Your Free Tier Operational cluster on [Capella](https://cloud.couchbase.com/sign-up)\n",
    "   - To get started with [Couchbase Capella](https://cloud.couchbase.com), create an account and use it to deploy a free tier operational cluster\n",
    "   - This account provides you with an environment where you can explore and learn about Capella\n",
    "   - To learn more, please follow the [Getting Started Guide](https://docs.couchbase.com/cloud/get-started/create-account.html)\n",
    "   - **Important**: This tutorial requires Couchbase Server **8.0+** for GSI vector search capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a6a23",
   "metadata": {},
   "source": [
    "### Couchbase Capella Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07c1ff4",
   "metadata": {},
   "source": [
    "When running Couchbase using Capella, the following prerequisites need to be met:\n",
    "- Create the database credentials to access the required bucket (Read and Write) used in the application\n",
    "- Allow access to the Cluster from the IP on which the application is running by following the [Network Security documentation](https://docs.couchbase.com/cloud/security/security.html#public-access)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223faba",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81251293",
   "metadata": {},
   "source": [
    "### Installing Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e51e49",
   "metadata": {},
   "source": [
    "We'll install the following key libraries:\n",
    "- `datasets`: For loading and managing our training data\n",
    "- `langchain-couchbase`: To integrate Couchbase with LangChain for GSI vector storage and caching\n",
    "- `langchain-openai`: For accessing OpenAI's embedding and chat models\n",
    "- `crewai`: To create and orchestrate our AI agents for RAG operations\n",
    "- `python-dotenv`: For securely managing environment variables and API keys\n",
    "\n",
    "These libraries provide the foundation for building a semantic search engine with GSI vector embeddings, database integration, and agent-based RAG capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a666ce8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet datasets==4.1.0 langchain-couchbase==0.5.0 langchain-openai==0.3.33 crewai==0.186.1 python-dotenv==1.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d980e7",
   "metadata": {},
   "source": [
    "### Import Required Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b2e73b",
   "metadata": {},
   "source": [
    "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d013a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from uuid import uuid4\n",
    "\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.diagnostics import PingState, ServiceType\n",
    "from couchbase.exceptions import (InternalServerFailureException,\n",
    "                                  QueryIndexAlreadyExistsException,\n",
    "                                  ServiceUnavailableException,\n",
    "                                  CouchbaseException)\n",
    "from couchbase.management.buckets import CreateBucketSettings\n",
    "from couchbase.options import ClusterOptions\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from crewai.tools import tool\n",
    "from langchain_couchbase.vectorstores import CouchbaseQueryVectorStore\n",
    "from langchain_couchbase.vectorstores import DistanceStrategy, IndexType\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from crewai import Agent, Crew, Process, Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d108a",
   "metadata": {},
   "source": [
    "### Configure Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65cf252",
   "metadata": {},
   "source": [
    "Logging is configured to track the progress of the script and capture any errors or warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e719ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Suppress httpx logging\n",
    "logging.getLogger('httpx').setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3690497b",
   "metadata": {},
   "source": [
    "### Load Environment Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653fc54f",
   "metadata": {},
   "source": [
    "In this section, we prompt the user to input essential configuration settings needed. These settings include sensitive information like database credentials, and specific configuration names. Instead of hardcoding these details into the script, we request the user to provide them at runtime, ensuring flexibility and security.\n",
    "\n",
    "The script uses environment variables to store sensitive information, enhancing the overall security and maintainability of your code by avoiding hardcoded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aaf9289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(\"./.env\")\n",
    "\n",
    "# Configuration\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or input(\"Enter your OpenAI API key: \")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY is not set\")\n",
    "\n",
    "CB_HOST = os.getenv('CB_HOST') or 'couchbase://localhost'\n",
    "CB_USERNAME = os.getenv('CB_USERNAME') or 'Administrator'\n",
    "CB_PASSWORD = os.getenv('CB_PASSWORD') or 'password'\n",
    "CB_BUCKET_NAME = os.getenv('CB_BUCKET_NAME') or 'vector-search-testing'\n",
    "SCOPE_NAME = os.getenv('SCOPE_NAME') or 'shared'\n",
    "COLLECTION_NAME = os.getenv('COLLECTION_NAME') or 'crew'\n",
    "\n",
    "print(\"Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa87d96",
   "metadata": {},
   "source": [
    "## Couchbase Connection Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c30a607",
   "metadata": {},
   "source": [
    "### Connect to Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996466dc",
   "metadata": {},
   "source": [
    "Connecting to a Couchbase cluster is the foundation of our project. Couchbase will serve as our primary data store, handling all the storage and retrieval operations required for our semantic search engine. By establishing this connection, we enable our application to interact with the database, allowing us to perform operations such as storing embeddings, querying data, and managing collections. This connection is the gateway through which all data will flow, so ensuring it's set up correctly is paramount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "979bd5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Couchbase\n"
     ]
    }
   ],
   "source": [
    "# Connect to Couchbase\n",
    "try:\n",
    "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "    options = ClusterOptions(auth)\n",
    "    cluster = Cluster(CB_HOST, options)\n",
    "    cluster.wait_until_ready(timedelta(seconds=5))\n",
    "    print(\"Successfully connected to Couchbase\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to Couchbase: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc61a9b",
   "metadata": {},
   "source": [
    "### Setup Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b48da",
   "metadata": {},
   "source": [
    "Create and configure Couchbase bucket, scope, and collection for storing our vector data.\n",
    "\n",
    "1. **Bucket Creation:**\n",
    "   - Checks if specified bucket exists, creates it if not\n",
    "   - Sets bucket properties like RAM quota (1024MB) and replication (disabled)\n",
    "   - Note: If you are using Capella, create a bucket manually called vector-search-testing(or any name you prefer) with the same properties.\n",
    "\n",
    "2. **Scope Management:**  \n",
    "   - Verifies if requested scope exists within bucket\n",
    "   - Creates new scope if needed (unless it's the default \"_default\" scope)\n",
    "\n",
    "3. **Collection Setup:**\n",
    "   - Checks for collection existence within scope\n",
    "   - Creates collection if it doesn't exist\n",
    "   - Waits 2 seconds for collection to be ready\n",
    "\n",
    "**Additional Tasks:**\n",
    "- Clears any existing documents for clean state\n",
    "- Implements comprehensive error handling and logging\n",
    "\n",
    "The function is called twice to set up:\n",
    "1. Main collection for vector embeddings\n",
    "2. Cache collection for storing results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b79fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 10:17:53 [INFO] Bucket 'vector-search-testing' exists.\n",
      "2025-10-06 10:17:53 [INFO] Collection 'crew' already exists. Skipping creation.\n",
      "2025-10-06 10:17:55 [INFO] All documents cleared from the collection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<couchbase.collection.Collection at 0x307407a10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup_collection(cluster, bucket_name, scope_name, collection_name):\n",
    "    try:\n",
    "        # Check if bucket exists, create if it doesn't\n",
    "        try:\n",
    "            bucket = cluster.bucket(bucket_name)\n",
    "            logging.info(f\"Bucket '{bucket_name}' exists.\")\n",
    "        except Exception as e:\n",
    "            logging.info(f\"Bucket '{bucket_name}' does not exist. Creating it...\")\n",
    "            bucket_settings = CreateBucketSettings(\n",
    "                name=bucket_name,\n",
    "                bucket_type='couchbase',\n",
    "                ram_quota_mb=1024,\n",
    "                flush_enabled=True,\n",
    "                num_replicas=0\n",
    "            )\n",
    "            cluster.buckets().create_bucket(bucket_settings)\n",
    "            time.sleep(2)  # Wait for bucket creation to complete and become available\n",
    "            bucket = cluster.bucket(bucket_name)\n",
    "            logging.info(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "\n",
    "        bucket_manager = bucket.collections()\n",
    "\n",
    "        # Check if scope exists, create if it doesn't\n",
    "        scopes = bucket_manager.get_all_scopes()\n",
    "        scope_exists = any(scope.name == scope_name for scope in scopes)\n",
    "        \n",
    "        if not scope_exists and scope_name != \"_default\":\n",
    "            logging.info(f\"Scope '{scope_name}' does not exist. Creating it...\")\n",
    "            bucket_manager.create_scope(scope_name)\n",
    "            logging.info(f\"Scope '{scope_name}' created successfully.\")\n",
    "\n",
    "        # Check if collection exists, create if it doesn't\n",
    "        collections = bucket_manager.get_all_scopes()\n",
    "        collection_exists = any(\n",
    "            scope.name == scope_name and collection_name in [col.name for col in scope.collections]\n",
    "            for scope in collections\n",
    "        )\n",
    "\n",
    "        if not collection_exists:\n",
    "            logging.info(f\"Collection '{collection_name}' does not exist. Creating it...\")\n",
    "            bucket_manager.create_collection(scope_name, collection_name)\n",
    "            logging.info(f\"Collection '{collection_name}' created successfully.\")\n",
    "        else:\n",
    "            logging.info(f\"Collection '{collection_name}' already exists. Skipping creation.\")\n",
    "\n",
    "        # Wait for collection to be ready\n",
    "        collection = bucket.scope(scope_name).collection(collection_name)\n",
    "        time.sleep(2)  # Give the collection time to be ready for queries\n",
    "\n",
    "        # Clear all documents in the collection\n",
    "        try:\n",
    "            query = f\"DELETE FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
    "            cluster.query(query).execute()\n",
    "            logging.info(\"All documents cleared from the collection.\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error while clearing documents: {str(e)}. The collection might be empty.\")\n",
    "\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error setting up collection: {str(e)}\")\n",
    "    \n",
    "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4faf3f",
   "metadata": {},
   "source": [
    "## Understanding GSI Vector Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c7d28f",
   "metadata": {},
   "source": [
    "### GSI Vector Index Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90080454",
   "metadata": {},
   "source": [
    "Semantic search with GSI requires creating a Global Secondary Index optimized for vector operations. Unlike FTS-based vector search, GSI vector indexes offer two distinct types optimized for different use cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72154198",
   "metadata": {},
   "source": [
    "#### GSI Vector Index Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55cb1f4",
   "metadata": {},
   "source": [
    "##### Hyperscale Vector Indexes (BHIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a80376e",
   "metadata": {},
   "source": [
    "- **Best for**: Pure vector searches like content discovery, recommendations, and semantic search\n",
    "- **Performance**: High performance with low memory footprint, optimized for concurrent operations\n",
    "- **Scalability**: Designed to scale to billions of vectors\n",
    "- **Use when**: You primarily perform vector-only queries without complex scalar filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbcbf25",
   "metadata": {},
   "source": [
    "##### Composite Vector Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f1bcc",
   "metadata": {},
   "source": [
    "- **Best for**: Filtered vector searches that combine vector search with scalar value filtering\n",
    "- **Performance**: Efficient pre-filtering where scalar attributes reduce the vector comparison scope\n",
    "- **Use when**: Your queries combine vector similarity with scalar filters that eliminate large portions of data\n",
    "- **Note**: Scalar filters take precedence over vector similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebefc2f",
   "metadata": {},
   "source": [
    "#### Understanding Index Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e4bff1",
   "metadata": {},
   "source": [
    "The `index_description` parameter controls how Couchbase optimizes vector storage and search through centroids and quantization:\n",
    "\n",
    "**Format**: `'IVF[<centroids>],{PQ|SQ}<settings>'`\n",
    "\n",
    "**Centroids (IVF - Inverted File):**\n",
    "- Controls how the dataset is subdivided for faster searches\n",
    "- More centroids = faster search, slower training  \n",
    "- Fewer centroids = slower search, faster training\n",
    "- If omitted (like IVF,SQ8), Couchbase auto-selects based on dataset size\n",
    "\n",
    "**Quantization Options:**\n",
    "- SQ (Scalar Quantization): SQ4, SQ6, SQ8 (4, 6, or 8 bits per dimension)\n",
    "- PQ (Product Quantization): PQ<subquantizers>x<bits> (e.g., PQ32x8)\n",
    "- Higher values = better accuracy, larger index size\n",
    "\n",
    "**Common Examples:**\n",
    "- IVF,SQ8 - Auto centroids, 8-bit scalar quantization (good default)\n",
    "- IVF1000,SQ6 - 1000 centroids, 6-bit scalar quantization  \n",
    "- IVF,PQ32x8 - Auto centroids, 32 subquantizers with 8 bits\n",
    "\n",
    "For detailed configuration options, see the [Quantization & Centroid Settings](https://docs.couchbase.com/cloud/vector-index/hyperscale-vector-index.html#algo_settings).\n",
    "\n",
    "For more information on GSI vector indexes, see [Couchbase GSI Vector Documentation](https://docs.couchbase.com/cloud/vector-index/use-vector-indexes.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88e4c207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSI vector index configuration prepared\n"
     ]
    }
   ],
   "source": [
    "# GSI Vector Index Configuration\n",
    "# Unlike FTS indexes, GSI vector indexes are created programmatically through the vector store\n",
    "# We'll configure the parameters that will be used for index creation\n",
    "\n",
    "# Vector configuration\n",
    "DISTANCE_STRATEGY = DistanceStrategy.COSINE  # Cosine similarity\n",
    "INDEX_TYPE = IndexType.BHIVE  # Using BHIVE for high-performance vector \n",
    "INDEX_DESCRIPTION = \"IVF,SQ8\"  # Auto-selected centroids with 8-bit scalar quantization\n",
    "\n",
    "# To create a Composite Index instead, use the following:\n",
    "# INDEX_TYPE = IndexType.COMPOSITE  # Combines vector search with scalar filtering\n",
    "\n",
    "print(\"GSI vector index configuration prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9d300",
   "metadata": {},
   "source": [
    "### Alternative: Composite Index Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a9d35",
   "metadata": {},
   "source": [
    "If your use case requires complex filtering with scalar attributes, you can create a **Composite index** instead by changing the configuration:\n",
    "\n",
    "```python\n",
    "# Alternative configuration for Composite index\n",
    "INDEX_TYPE = IndexType.COMPOSITE  # Instead of IndexType.BHIVE\n",
    "INDEX_DESCRIPTION = \"IVF,SQ8\"     # Same quantization settings\n",
    "DISTANCE_STRATEGY = DistanceStrategy.COSINE  # Same distance metric\n",
    "\n",
    "# The rest of the setup remains identical\n",
    "```\n",
    "\n",
    "**Use Composite indexes when:**\n",
    "- You need to filter by document metadata or attributes before vector similarity\n",
    "- Your queries combine vector search with WHERE clauses  \n",
    "- You have well-defined filtering requirements that can reduce the search space\n",
    "\n",
    "**Note**: The index creation process is identical - just change the `INDEX_TYPE`. Composite indexes enable pre-filtering with scalar attributes, making them ideal for applications requiring complex query patterns with metadata filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301cc63",
   "metadata": {},
   "source": [
    "## OpenAI Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99257fdb",
   "metadata": {},
   "source": [
    "This section initializes two key OpenAI components needed for our RAG system:\n",
    "\n",
    "1. **OpenAI Embeddings:**\n",
    "   - Uses the 'text-embedding-3-small' model\n",
    "   - Converts text into high-dimensional vector representations (embeddings)\n",
    "   - These embeddings enable semantic search by capturing the meaning of text\n",
    "   - Required for vector similarity search in Couchbase\n",
    "\n",
    "2. **ChatOpenAI Language Model:**\n",
    "   - Uses the 'gpt-4o' model\n",
    "   - Temperature set to 0.2 for balanced creativity and focus\n",
    "   - Serves as the cognitive engine for CrewAI agents\n",
    "   - Powers agent reasoning, decision-making, and task execution\n",
    "   - Enables agents to:\n",
    "     - Process and understand retrieved context from vector search\n",
    "     - Generate thoughtful responses based on that context\n",
    "     - Follow instructions defined in agent roles and goals\n",
    "     - Collaborate with other agents in the crew\n",
    "   - The relatively low temperature (0.2) ensures agents produce reliable, consistent outputs while maintaining some creative problem-solving ability\n",
    "\n",
    "Both components require a valid OpenAI API key (OPENAI_API_KEY) for authentication.\n",
    "In the CrewAI framework, the LLM acts as the \"brain\" for each agent, allowing them to interpret tasks, retrieve relevant information via the RAG system, and generate appropriate outputs based on their specialized roles and expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9e6fd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI components initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI components\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(\"OpenAI components initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902067c7",
   "metadata": {},
   "source": [
    "## Document Processing and Vector Store Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340c7ce",
   "metadata": {},
   "source": [
    "### Create Couchbase GSI Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d202628",
   "metadata": {},
   "source": [
    "Set up the GSI vector store where we'll store document embeddings for high-performance semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a877a51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 10:18:05 [INFO] GSI Vector store setup completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSI Vector store initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Setup GSI vector store with OpenAI embeddings\n",
    "try:\n",
    "    vector_store = CouchbaseQueryVectorStore(\n",
    "        cluster=cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=embeddings,\n",
    "        distance_metric=DISTANCE_STRATEGY\n",
    "    )\n",
    "    print(\"GSI Vector store initialized successfully\")\n",
    "    logging.info(\"GSI Vector store setup completed\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to initialize GSI vector store: {str(e)}\")\n",
    "    raise RuntimeError(f\"GSI Vector store initialization failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194a58c",
   "metadata": {},
   "source": [
    "### Load BBC News Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dad8d9",
   "metadata": {},
   "source": [
    "To build a search engine, we need data to search through. We use the BBC News dataset from RealTimeData, which provides real-world news articles. This dataset contains news articles from BBC covering various topics and time periods. Loading the dataset is a crucial step because it provides the raw material that our search engine will work with. The quality and diversity of the news articles make it an excellent choice for testing and refining our search engine, ensuring it can handle real-world news content effectively.\n",
    "\n",
    "The BBC News dataset allows us to work with authentic news articles, enabling us to build and test a search engine that can effectively process and retrieve relevant news content. The dataset is loaded using the Hugging Face datasets library, specifically accessing the \"RealTimeData/bbc_news_alltime\" dataset with the \"2024-12\" version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13b20d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 10:18:13 [INFO] Successfully loaded the BBC News dataset with 2687 rows.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the BBC News dataset with 2687 rows\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    news_dataset = load_dataset(\n",
    "        \"RealTimeData/bbc_news_alltime\", \"2024-12\", split=\"train\"\n",
    "    )\n",
    "    print(f\"Loaded the BBC News dataset with {len(news_dataset)} rows\")\n",
    "    logging.info(f\"Successfully loaded the BBC News dataset with {len(news_dataset)} rows.\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading the BBC News dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7592356",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad46f0",
   "metadata": {},
   "source": [
    "Remove duplicate articles for cleaner search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "496e3afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1749 unique articles in our database.\n"
     ]
    }
   ],
   "source": [
    "news_articles = news_dataset[\"content\"]\n",
    "unique_articles = set()\n",
    "for article in news_articles:\n",
    "    if article:\n",
    "        unique_articles.add(article)\n",
    "unique_news_articles = list(unique_articles)\n",
    "print(f\"We have {len(unique_news_articles)} unique articles in our database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e3853c",
   "metadata": {},
   "source": [
    "#### Save Data to Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814d5e49",
   "metadata": {},
   "source": [
    "To efficiently handle the large number of articles, we process them in batches of articles at a time. This batch processing approach helps manage memory usage and provides better control over the ingestion process.\n",
    "\n",
    "We first filter out any articles that exceed 50,000 characters to avoid potential issues with token limits. Then, using the vector store's add_texts method, we add the filtered articles to our vector database. The batch_size parameter controls how many articles are processed in each iteration.\n",
    "\n",
    "This approach offers several benefits:\n",
    "1. **Memory Efficiency**: Processing in smaller batches prevents memory overload\n",
    "2. **Error Handling**: If an error occurs, only the current batch is affected\n",
    "3. **Progress Tracking**: Easier to monitor and track the ingestion progress\n",
    "4. **Resource Management**: Better control over CPU and network resource utilization\n",
    "\n",
    "We use a conservative batch size of 50 to ensure reliable operation. The optimal batch size depends on many factors including document sizes, available system resources, network conditions, and concurrent workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "188dcccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 10:19:43 [INFO] Document ingestion completed successfully.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "\n",
    "# Automatic Batch Processing\n",
    "articles = [article for article in unique_news_articles if article and len(article) <= 50000]\n",
    "\n",
    "try:\n",
    "    vector_store.add_texts(\n",
    "        texts=articles,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    logging.info(\"Document ingestion completed successfully.\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to save documents to vector store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60ee6d",
   "metadata": {},
   "source": [
    "## Vector Search Performance Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51df07c1",
   "metadata": {},
   "source": [
    "Now let's demonstrate the performance benefits of GSI optimization by testing pure vector search performance. We'll compare three optimization levels:\n",
    "\n",
    "1. **Baseline Performance**: Vector search without GSI optimization\n",
    "2. **Vector Index-Optimized Performance**: Same search with BHIVE GSI index\n",
    "3. **Cache Benefits**: Show how caching can be applied on top of GSI for repeated queries\n",
    "\n",
    "**Important**: This testing focuses on pure vector search performance, isolating the GSI improvements from other workflow overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d72167",
   "metadata": {},
   "source": [
    "### Create Vector Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c43f62fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create GSI vector retriever optimized for high-performance searches\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}  # Return top 4 most similar documents\n",
    ")\n",
    "\n",
    "def test_vector_search_performance(query_text, label=\"Vector Search\"):\n",
    "    \"\"\"Test pure vector search performance and return timing metrics\"\"\"\n",
    "    print(f\"\\n[{label}] Testing vector search performance\")\n",
    "    print(f\"[{label}] Query: '{query_text}'\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Perform vector search using the retriever\n",
    "        docs = retriever.invoke(query_text)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        search_time = end_time - start_time\n",
    "        print(f\"[{label}] Vector search completed in {search_time:.4f} seconds\")\n",
    "        print(f\"[{label}] Found {len(docs)} relevant documents\")\n",
    "        \n",
    "        # Show a preview of the first result\n",
    "        if docs:\n",
    "            preview = docs[0].page_content[:100] + \"...\" if len(docs[0].page_content) > 100 else docs[0].page_content\n",
    "            print(f\"[{label}] Top result preview: {preview}\")\n",
    "        \n",
    "        return search_time\n",
    "    except Exception as e:\n",
    "        print(f\"[{label}] Vector search failed: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f939b9e1",
   "metadata": {},
   "source": [
    "### Test 1: Baseline Performance (No GSI Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20d10ad",
   "metadata": {},
   "source": [
    "Test pure vector search performance without GSI optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71ceaa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing baseline vector search performance without GSI optimization...\n",
      "\n",
      "[Baseline Search] Testing vector search performance\n",
      "[Baseline Search] Query: 'What are the latest developments in football transfers?'\n",
      "[Baseline Search] Vector search completed in 1.3999 seconds\n",
      "[Baseline Search] Found 4 relevant documents\n",
      "[Baseline Search] Top result preview: The latest updates and analysis from the BBC.\n",
      "\n",
      "Baseline vector search time (without GSI): 1.3999 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test baseline vector search performance without GSI index\n",
    "test_query = \"What are the latest developments in football transfers?\"\n",
    "print(\"Testing baseline vector search performance without GSI optimization...\")\n",
    "baseline_time = test_vector_search_performance(test_query, \"Baseline Search\")\n",
    "print(f\"\\nBaseline vector search time (without GSI): {baseline_time:.4f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d304e9",
   "metadata": {},
   "source": [
    "### Create BHIVE GSI Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae9cef0",
   "metadata": {},
   "source": [
    "Now let's create a BHIVE GSI vector index to enable high-performance vector searches. The index creation is done programmatically through the vector store, which will optimize the index settings based on our data and requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "389d1358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BHIVE GSI vector index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 10:20:15 [INFO] BHIVE index created with description 'IVF,SQ8'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSI Vector index created successfully\n",
      "Waiting for index to become available...\n"
     ]
    }
   ],
   "source": [
    "# Create GSI Vector Index for high-performance searches\n",
    "print(\"Creating BHIVE GSI vector index...\")\n",
    "try:\n",
    "    # Create a BHIVE index optimized for pure vector searches\n",
    "    vector_store.create_index(\n",
    "        index_type=INDEX_TYPE,  # BHIVE index type\n",
    "        index_description=INDEX_DESCRIPTION  # IVF,SQ8 for optimized performance\n",
    "    )\n",
    "    print(f\"GSI Vector index created successfully\")\n",
    "    logging.info(f\"BHIVE index created with description '{INDEX_DESCRIPTION}'\")\n",
    "    \n",
    "    # Wait a moment for index to be available\n",
    "    print(\"Waiting for index to become available...\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    # Index might already exist, which is fine\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(f\"GSI Vector index already exists, proceeding...\")\n",
    "        logging.info(f\"Index already exists\")\n",
    "    else:\n",
    "        logging.error(f\"Failed to create GSI index: {str(e)}\")\n",
    "        raise RuntimeError(f\"GSI index creation failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9e5763",
   "metadata": {},
   "source": [
    "### Test 2: Vector Index-Optimized Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8388f41b",
   "metadata": {},
   "source": [
    "Test the same vector search with BHIVE GSI optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1b89f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing vector search performance with BHIVE GSI optimization...\n",
      "\n",
      "[Vector Index-Optimized Search] Testing vector search performance\n",
      "[Vector Index-Optimized Search] Query: 'What are the latest developments in football transfers?'\n",
      "[Vector Index-Optimized Search] Vector search completed in 0.5885 seconds\n",
      "[Vector Index-Optimized Search] Found 4 relevant documents\n",
      "[Vector Index-Optimized Search] Top result preview: Four key areas for Everton's new owners to address\n",
      "\n",
      "Everton fans last saw silverware in 1995 when th...\n"
     ]
    }
   ],
   "source": [
    "# Test vector search performance with GSI index\n",
    "print(\"Testing vector search performance with BHIVE GSI optimization...\")\n",
    "gsi_search_time = test_vector_search_performance(test_query, \"Vector Index-Optimized Search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba6c37a",
   "metadata": {},
   "source": [
    "### Test 3: Cache Benefits Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc73249",
   "metadata": {},
   "source": [
    "Now let's demonstrate how caching can improve performance for repeated queries. **Note**: Caching benefits apply to both baseline and GSI-optimized searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3850c8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing cache benefits with vector search...\n",
      "First execution (cache miss):\n",
      "\n",
      "[Cache Test - First Run] Testing vector search performance\n",
      "[Cache Test - First Run] Query: 'What happened in the latest Premier League matches?'\n",
      "[Cache Test - First Run] Vector search completed in 0.6450 seconds\n",
      "[Cache Test - First Run] Found 4 relevant documents\n",
      "[Cache Test - First Run] Top result preview: Who has made Troy's Premier League team of the week?\n",
      "\n",
      "After every round of Premier League matches th...\n",
      "\n",
      "Second execution (cache hit - should be faster):\n",
      "\n",
      "[Cache Test - Second Run] Testing vector search performance\n",
      "[Cache Test - Second Run] Query: 'What happened in the latest Premier League matches?'\n",
      "[Cache Test - Second Run] Vector search completed in 0.4306 seconds\n",
      "[Cache Test - Second Run] Found 4 relevant documents\n",
      "[Cache Test - Second Run] Top result preview: Who has made Troy's Premier League team of the week?\n",
      "\n",
      "After every round of Premier League matches th...\n"
     ]
    }
   ],
   "source": [
    "# Test cache benefits with a different query to avoid interference\n",
    "cache_test_query = \"What happened in the latest Premier League matches?\"\n",
    "\n",
    "print(\"Testing cache benefits with vector search...\")\n",
    "print(\"First execution (cache miss):\")\n",
    "cache_time_1 = test_vector_search_performance(cache_test_query, \"Cache Test - First Run\")\n",
    "\n",
    "print(\"\\nSecond execution (cache hit - should be faster):\")\n",
    "cache_time_2 = test_vector_search_performance(cache_test_query, \"Cache Test - Second Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21530f7b",
   "metadata": {},
   "source": [
    "### Vector Search Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009e69c4",
   "metadata": {},
   "source": [
    "Let's analyze the vector search performance improvements across all optimization levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "388ca617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VECTOR SEARCH PERFORMANCE OPTIMIZATION SUMMARY\n",
      "================================================================================\n",
      "Phase 1 - Baseline Search (No GSI):     1.3999 seconds\n",
      "Phase 2 - Vector Index-Optimized Search:         0.5885 seconds\n",
      "Phase 3 - Cache Benefits:\n",
      "  First execution (cache miss):         0.6450 seconds\n",
      "  Second execution (cache hit):         0.4306 seconds\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "VECTOR SEARCH OPTIMIZATION IMPACT:\n",
      "--------------------------------------------------------------------------------\n",
      "GSI Index Benefit:      2.38x faster (58.0% improvement)\n",
      "Cache Benefit:          1.50x faster (33.2% improvement)\n",
      "\n",
      "Key Insights for Vector Search Performance:\n",
      "\u2022 GSI BHIVE indexes provide significant performance improvements for vector similarity search\n",
      "\u2022 Performance gains are most dramatic for complex semantic queries\n",
      "\u2022 BHIVE optimization is particularly effective for high-dimensional embeddings\n",
      "\u2022 Combined with proper quantization (SQ8), GSI delivers production-ready performance\n",
      "\u2022 These performance improvements directly benefit any application using the vector store\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VECTOR SEARCH PERFORMANCE OPTIMIZATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Phase 1 - Baseline Search (No GSI):     {baseline_time:.4f} seconds\")\n",
    "print(f\"Phase 2 - Vector Index-Optimized Search:         {gsi_search_time:.4f} seconds\")\n",
    "if cache_time_1 and cache_time_2:\n",
    "    print(f\"Phase 3 - Cache Benefits:\")\n",
    "    print(f\"  First execution (cache miss):         {cache_time_1:.4f} seconds\")\n",
    "    print(f\"  Second execution (cache hit):         {cache_time_2:.4f} seconds\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"VECTOR SEARCH OPTIMIZATION IMPACT:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# GSI improvement analysis\n",
    "if baseline_time and gsi_search_time:\n",
    "    speedup = baseline_time / gsi_search_time if gsi_search_time > 0 else float('inf')\n",
    "    time_saved = baseline_time - gsi_search_time\n",
    "    percent_improvement = (time_saved / baseline_time) * 100\n",
    "    print(f\"GSI Index Benefit:      {speedup:.2f}x faster ({percent_improvement:.1f}% improvement)\")\n",
    "\n",
    "# Cache improvement analysis\n",
    "if cache_time_1 and cache_time_2 and cache_time_2 < cache_time_1:\n",
    "    cache_speedup = cache_time_1 / cache_time_2\n",
    "    cache_improvement = ((cache_time_1 - cache_time_2) / cache_time_1) * 100\n",
    "    print(f\"Cache Benefit:          {cache_speedup:.2f}x faster ({cache_improvement:.1f}% improvement)\")\n",
    "else:\n",
    "    print(f\"Cache Benefit:          Variable (depends on query complexity and caching mechanism)\")\n",
    "\n",
    "print(f\"\\nKey Insights for Vector Search Performance:\")\n",
    "print(f\"\u2022 GSI BHIVE indexes provide significant performance improvements for vector similarity search\")\n",
    "print(f\"\u2022 Performance gains are most dramatic for complex semantic queries\")\n",
    "print(f\"\u2022 BHIVE optimization is particularly effective for high-dimensional embeddings\")\n",
    "print(f\"\u2022 Combined with proper quantization (SQ8), GSI delivers production-ready performance\")\n",
    "print(f\"\u2022 These performance improvements directly benefit any application using the vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7252b0c",
   "metadata": {},
   "source": [
    "## CrewAI Agent Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ee93f",
   "metadata": {},
   "source": [
    "### What is CrewAI?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274671b5",
   "metadata": {},
   "source": [
    "Now that we've optimized our vector search performance, let's build a sophisticated agent-based RAG system using CrewAI. CrewAI enables us to create specialized AI agents that collaborate to handle different aspects of the RAG workflow:\n",
    "\n",
    "- **Research Agent**: Finds and analyzes relevant documents using our optimized vector search\n",
    "- **Writer Agent**: Takes research findings and creates polished, structured responses\n",
    "- **Collaborative Workflow**: Agents work together, with the writer building on the researcher's findings\n",
    "\n",
    "This multi-agent approach produces higher-quality responses than single-agent systems by separating research and writing expertise, while benefiting from the GSI performance improvements we just demonstrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bda0d68",
   "metadata": {},
   "source": [
    "### Create Vector Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7b379d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GSI vector search tool using the @tool decorator\n",
    "@tool(\"gsi_vector_search\")\n",
    "def search_tool(query: str) -> str:\n",
    "    \"\"\"Search for relevant documents using Hyperscale and Composite Vector Indexes vector similarity.\n",
    "    Input should be a simple text query string.\n",
    "    Returns a list of relevant document contents from GSI vector search.\n",
    "    Use this tool to find detailed information about topics using high-performance GSI indexes.\"\"\"\n",
    "    \n",
    "    # Invoke the GSI vector retriever (now optimized with BHIVE index)\n",
    "    docs = retriever.invoke(query)\n",
    "\n",
    "    # Format the results with distance information\n",
    "    formatted_docs = \"\\n\\n\".join([\n",
    "        f\"Document {i+1}:\\n{'-'*40}\\n{doc.page_content}\"\n",
    "        for i, doc in enumerate(docs)\n",
    "    ])\n",
    "    return formatted_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a0165",
   "metadata": {},
   "source": [
    "### Create CrewAI Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73c44437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrewAI agents created successfully with optimized GSI vector search\n"
     ]
    }
   ],
   "source": [
    "# Create research agent\n",
    "researcher = Agent(\n",
    "    role='Research Expert',\n",
    "    goal='Find and analyze the most relevant documents to answer user queries accurately',\n",
    "    backstory=\"\"\"You are an expert researcher with deep knowledge in information retrieval \n",
    "    and analysis. Your expertise lies in finding, evaluating, and synthesizing information \n",
    "    from various sources. You have a keen eye for detail and can identify key insights \n",
    "    from complex documents. You always verify information across multiple sources and \n",
    "    provide comprehensive, accurate analyses.\"\"\",\n",
    "    tools=[search_tool],\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    memory=True,\n",
    "    allow_delegation=False\n",
    ")\n",
    "\n",
    "# Create writer agent\n",
    "writer = Agent(\n",
    "    role='Technical Writer',\n",
    "    goal='Generate clear, accurate, and well-structured responses based on research findings',\n",
    "    backstory=\"\"\"You are a skilled technical writer with expertise in making complex \n",
    "    information accessible and engaging. You excel at organizing information logically, \n",
    "    explaining technical concepts clearly, and creating well-structured documents. You \n",
    "    ensure all information is properly cited, accurate, and presented in a user-friendly \n",
    "    manner. You have a talent for maintaining the reader's interest while conveying \n",
    "    detailed technical information.\"\"\",\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    memory=True,\n",
    "    allow_delegation=False\n",
    ")\n",
    "\n",
    "print(\"CrewAI agents created successfully with optimized GSI vector search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63dbf3d",
   "metadata": {},
   "source": [
    "### How the Optimized RAG Workflow Works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd1697",
   "metadata": {},
   "source": [
    "The complete optimized RAG process:\n",
    "1. **User Query** \u2192 Research Agent\n",
    "2. **Vector Search** \u2192 GSI BHIVE index finds similar documents (now with proven performance improvements)\n",
    "3. **Document Analysis** \u2192 Research Agent analyzes and synthesizes findings\n",
    "4. **Response Writing** \u2192 Writer Agent creates polished, structured response\n",
    "5. **Final Output** \u2192 User receives comprehensive, well-formatted answer\n",
    "\n",
    "**Key Benefit**: The vector search performance improvements we demonstrated directly enhance the agent workflow efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca6cc10",
   "metadata": {},
   "source": [
    "## CrewAI Agent Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7d956a",
   "metadata": {},
   "source": [
    "Now let's demonstrate the complete optimized agent-based RAG system in action, benefiting from the GSI performance improvements we validated earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b1a283",
   "metadata": {},
   "source": [
    "### Demo Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2176b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_interactive_query(query, researcher, writer):\n",
    "    \"\"\"Run complete RAG workflow with CrewAI agents using optimized GSI vector search\"\"\"\n",
    "    print(f\"\\nProcessing Query: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create tasks\n",
    "    research_task = Task(\n",
    "        description=f\"Research and analyze information relevant to: {query}\",\n",
    "        agent=researcher,\n",
    "        expected_output=\"A detailed analysis with key findings\"\n",
    "    )\n",
    "    \n",
    "    writing_task = Task(\n",
    "        description=\"Create a comprehensive response\",\n",
    "        agent=writer,\n",
    "        expected_output=\"A clear, well-structured answer\",\n",
    "        context=[research_task]\n",
    "    )\n",
    "    \n",
    "    # Execute crew\n",
    "    crew = Crew(\n",
    "        agents=[researcher, writer],\n",
    "        tasks=[research_task, writing_task],\n",
    "        process=Process.sequential,\n",
    "        verbose=True,\n",
    "        cache=True,\n",
    "        planning=True\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        result = crew.kickoff()\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nCompleted in {elapsed_time:.2f} seconds\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"RESPONSE\")\n",
    "        print(\"=\" * 80)\n",
    "        print(result)\n",
    "                \n",
    "        return elapsed_time\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65e896e",
   "metadata": {},
   "source": [
    "### Run Agent-Based RAG Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d355751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable logging for cleaner output\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "# Run demo with a sample query\n",
    "demo_query = \"What are the key details about the FA Cup third round draw?\"\n",
    "final_time = process_interactive_query(demo_query, researcher, writer)\n",
    "\n",
    "if final_time:\n",
    "    print(f\"\\n\\n\u2705 CrewAI agent demo completed successfully in {final_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a24b3",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad950f",
   "metadata": {},
   "source": [
    "You have successfully built a powerful agent-based RAG system that combines Couchbase's high-performance GSI vector storage capabilities with CrewAI's multi-agent architecture. This tutorial demonstrated the complete pipeline from data ingestion to intelligent response generation, with real performance benchmarks showing the dramatic improvements GSI indexing provides."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}