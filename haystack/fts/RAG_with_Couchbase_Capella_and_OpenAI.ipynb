{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# BBC News Dataset RAG Pipeline with Couchbase and OpenAI\n",
                "\n",
                "This notebook demonstrates how to build a Retrieval Augmented Generation (RAG) system using:\n",
                "- The BBC News dataset containing real-time news articles\n",
                "- Couchbase Capella as the vector store with FTS (Full Text Search)\n",
                "- Haystack framework for the RAG pipeline\n",
                "- OpenAI for embeddings and text generation\n",
                "\n",
                "The system allows users to ask questions about current events and get AI-generated answers based on the latest news articles."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Installing Necessary Libraries\n",
                "\n",
                "To build our RAG system, we need a set of libraries. The libraries we install handle everything from connecting to databases to performing AI tasks. Each library has a specific role: Couchbase libraries manage database operations, Haystack handles AI model integrations and pipeline management, and we will use the OpenAI SDK for generating embeddings and calling OpenAI's language models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%pip install datasets haystack-ai couchbase-haystack openai pandas"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Importing Necessary Libraries\n",
                "\n",
                "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, Haystack components for RAG pipeline, embedding generation, and dataset loading."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import getpass\n",
                "import base64\n",
                "import logging\n",
                "import sys\n",
                "import time\n",
                "import pandas as pd\n",
                "from datetime import timedelta\n",
                "\n",
                "from couchbase.auth import PasswordAuthenticator\n",
                "from couchbase.cluster import Cluster\n",
                "from couchbase.exceptions import CouchbaseException\n",
                "from couchbase.options import ClusterOptions\n",
                "from datasets import load_dataset\n",
                "from haystack import Pipeline, GeneratedAnswer\n",
                "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
                "from haystack.components.preprocessors import DocumentCleaner\n",
                "from haystack.components.writers import DocumentWriter\n",
                "from haystack.components.builders.answer_builder import AnswerBuilder\n",
                "from haystack.components.builders.prompt_builder import PromptBuilder\n",
                "from haystack.components.generators import OpenAIGenerator\n",
                "from haystack.utils import Secret\n",
                "from haystack.dataclasses import Document\n",
                "\n",
                "from couchbase_haystack import (\n",
                "    CouchbaseSearchDocumentStore,\n",
                "    CouchbasePasswordAuthenticator,\n",
                "    CouchbaseClusterOptions,\n",
                "    CouchbaseSearchEmbeddingRetriever,\n",
                ")\n",
                "from couchbase.options import KnownConfigProfiles\n",
                "\n",
                "# Configure logging\n",
                "logger = logging.getLogger(__name__)\n",
                "logger.setLevel(logging.DEBUG)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Prerequisites\n",
                "\n",
                "## Create and Deploy Your Operational cluster on Capella\n",
                "\n",
                "To get started with Couchbase Capella, create an account and use it to deploy an operational cluster.\n",
                "\n",
                "To know more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html).\n",
                "\n",
                "\n",
                "### Couchbase Capella Configuration\n",
                "\n",
                "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met:\n",
                "\n",
                "* Have a multi-node Capella cluster running the Data, Query, Index, and Search services.\n",
                "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the travel-sample bucket (Read and Write) used in the application.\n",
                "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running.\n",
                "\n",
                "### OpenAI Models Setup\n",
                "\n",
                "In order to create the RAG application, we need an embedding model to ingest the documents for Vector Search and a large language model (LLM) for generating the responses based on the context. \n",
                "\n",
                "For this implementation, we'll use OpenAI's models which provide state-of-the-art performance for both embeddings and text generation:\n",
                "\n",
                "**Embedding Model**: We'll use OpenAI's `text-embedding-3-large` model, which provides high-quality embeddings with 3,072 dimensions for semantic search capabilities.\n",
                "\n",
                "**Large Language Model**: We'll use OpenAI's `gpt-4o` model for generating responses based on the retrieved context. This model offers excellent reasoning capabilities and can handle complex queries effectively.\n",
                "\n",
                "**Prerequisites for OpenAI Integration**:\n",
                "* Create an OpenAI account at [platform.openai.com](https://platform.openai.com)\n",
                "* Generate an API key from your OpenAI dashboard\n",
                "* Ensure you have sufficient credits or a valid payment method set up\n",
                "* Set up your API key as an environment variable or input it securely in the notebook\n",
                "\n",
                "For more details about OpenAI's models and pricing, please refer to the [OpenAI documentation](https://platform.openai.com/docs/models)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Configure Couchbase Credentials\n",
                "\n",
                "Enter your Couchbase and OpenAI credentials:\n",
                "\n",
                "**OPENAI_API_KEY** is your OpenAI API key which can be obtained from your OpenAI dashboard at [platform.openai.com](https://platform.openai.com/api-keys).\n",
                "\n",
                "**INDEX_NAME** is the name of the FTS search index we will use for vector search operations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CB_CONNECTION_STRING = input(\"Couchbase Cluster URL (default: localhost): \") or \"localhost\"\n",
                "CB_USERNAME = input(\"Couchbase Username (default: admin): \") or \"admin\"\n",
                "CB_PASSWORD = input(\"Couchbase password (default: Password@12345): \") or \"Password@12345\"\n",
                "CB_BUCKET_NAME = input(\"Couchbase Bucket: \")\n",
                "CB_SCOPE_NAME = input(\"Couchbase Scope: \")\n",
                "CB_COLLECTION_NAME = input(\"Couchbase Collection: \")\n",
                "CB_INDEX_NAME = input(\"Vector Search Index: \")\n",
                "OPENAI_API_KEY = input(\"OpenAI API Key: \")\n",
                "\n",
                "# Check if the variables are correctly loaded\n",
                "if not all([CB_CONNECTION_STRING, CB_USERNAME, CB_PASSWORD, CB_BUCKET_NAME, CB_SCOPE_NAME, CB_COLLECTION_NAME, CB_INDEX_NAME, CB_OPENAI_API_KEY]):\n",
                "    raise ValueError(\"All configuration variables must be provided.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from couchbase.cluster import Cluster \n",
                "from couchbase.options import ClusterOptions\n",
                "from couchbase.auth import PasswordAuthenticator\n",
                "from couchbase.management.buckets import CreateBucketSettings\n",
                "from couchbase.management.collections import CollectionSpec\n",
                "from couchbase.management.search import SearchIndex\n",
                "import json\n",
                "\n",
                "# Connect to Couchbase cluster\n",
                "cluster = Cluster(CB_CONNECTION_STRING, ClusterOptions(\n",
                "    PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)))\n",
                "\n",
                "# Create bucket if it does not exist\n",
                "bucket_manager = cluster.buckets()\n",
                "try:\n",
                "    bucket_manager.get_bucket(CB_BUCKET_NAME)\n",
                "    print(f\"Bucket '{CB_BUCKET_NAME}' already exists.\")\n",
                "except Exception as e:\n",
                "    print(f\"Bucket '{CB_BUCKET_NAME}' does not exist. Creating bucket...\")\n",
                "    bucket_settings = CreateBucketSettings(name=CB_BUCKET_NAME, ram_quota_mb=500)\n",
                "    bucket_manager.create_bucket(bucket_settings)\n",
                "    print(f\"Bucket '{CB_BUCKET_NAME}' created successfully.\")\n",
                "\n",
                "# Create scope and collection if they do not exist\n",
                "collection_manager = cluster.bucket(CB_BUCKET_NAME).collections()\n",
                "scopes = collection_manager.get_all_scopes()\n",
                "scope_exists = any(scope.name == CB_SCOPE_NAME for scope in scopes)\n",
                "\n",
                "if scope_exists:\n",
                "    print(f\"Scope '{CB_SCOPE_NAME}' already exists.\")\n",
                "else:\n",
                "    print(f\"Scope '{CB_SCOPE_NAME}' does not exist. Creating scope...\")\n",
                "    collection_manager.create_scope(CB_SCOPE_NAME)\n",
                "    print(f\"Scope '{CB_SCOPE_NAME}' created successfully.\")\n",
                "\n",
                "collections = [collection.name for scope in scopes if scope.name == CB_SCOPE_NAME for collection in scope.collections]\n",
                "collection_exists = CB_COLLECTION_NAME in collections\n",
                "\n",
                "if collection_exists:\n",
                "    print(f\"Collection '{CB_COLLECTION_NAME}' already exists in scope '{CB_SCOPE_NAME}'.\")\n",
                "else:\n",
                "    print(f\"Collection '{CB_COLLECTION_NAME}' does not exist in scope '{CB_SCOPE_NAME}'. Creating collection...\")\n",
                "    collection_manager.create_collection(collection_name=CB_COLLECTION_NAME, scope_name=CB_SCOPE_NAME)\n",
                "    print(f\"Collection '{CB_COLLECTION_NAME}' created successfully.\")\n",
                "\n",
                "# Create search index from search_index.json file at scope level\n",
                "with open('fts_index.json', 'r') as search_file:\n",
                "    search_index_definition = SearchIndex.from_json(json.load(search_file))\n",
                "    \n",
                "       # Update search index definition with user inputs\n",
                "    search_index_definition.name = CB_INDEX_NAME\n",
                "    search_index_definition.source_name = CB_BUCKET_NAME\n",
                "    \n",
                "    # Update types mapping\n",
                "    old_type_key = next(iter(search_index_definition.params['mapping']['types'].keys()))\n",
                "    type_obj = search_index_definition.params['mapping']['types'].pop(old_type_key)\n",
                "    search_index_definition.params['mapping']['types'][f\"{CB_SCOPE_NAME}.{CB_COLLECTION_NAME}\"] = type_obj\n",
                "    \n",
                "    search_index_name = search_index_definition.name\n",
                "    \n",
                "    # Get scope-level search manager\n",
                "    scope_search_manager = cluster.bucket(CB_BUCKET_NAME).scope(CB_SCOPE_NAME).search_indexes()\n",
                "    \n",
                "    try:\n",
                "        # Check if index exists at scope level\n",
                "        existing_index = scope_search_manager.get_index(search_index_name)\n",
                "        print(f\"Search index '{search_index_name}' already exists at scope level.\")\n",
                "    except Exception as e:\n",
                "        print(f\"Search index '{search_index_name}' does not exist at scope level. Creating search index from fts_index.json...\")\n",
                "        with open('fts_index.json', 'r') as search_file:\n",
                "            search_index_definition = SearchIndex.from_json(json.load(search_file))\n",
                "            scope_search_manager.upsert_index(search_index_definition)\n",
                "            print(f\"Search index '{search_index_name}' created successfully at scope level.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Load and Process Movie Dataset\n",
                "\n",
                "Load the TMDB movie dataset and prepare documents for indexing:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load TMDB dataset\n",
                "print(\"Loading TMDB dataset...\")\n",
                "dataset = load_dataset(\"AiresPucrs/tmdb-5000-movies\")\n",
                "movies_df = pd.DataFrame(dataset['train'])\n",
                "print(f\"Total movies found: {len(movies_df)}\")\n",
                "\n",
                "# Create documents from movie data\n",
                "docs_data = []\n",
                "for _, row in movies_df.iterrows():\n",
                "    if pd.isna(row['overview']):\n",
                "        continue\n",
                "        \n",
                "    try:\n",
                "        docs_data.append({\n",
                "            'id': str(row[\"id\"]),\n",
                "            'content': f\"Title: {row['title']}\\nGenres: {', '.join([genre['name'] for genre in eval(row['genres'])])}\\nOverview: {row['overview']}\",\n",
                "            'metadata': {\n",
                "                'title': row['title'],\n",
                "                'genres': row['genres'],\n",
                "                'original_language': row['original_language'],\n",
                "                'popularity': float(row['popularity']),\n",
                "                'release_date': row['release_date'],\n",
                "                'vote_average': float(row['vote_average']),\n",
                "                'vote_count': int(row['vote_count']),\n",
                "                'budget': int(row['budget']),\n",
                "                'revenue': int(row['revenue'])\n",
                "            }\n",
                "        })\n",
                "    except Exception as e:\n",
                "        logger.error(f\"Error processing movie {row['title']}: {e}\")\n",
                "\n",
                "print(f\"Created {len(docs_data)} documents with valid overviews\")\n",
                "documents = [Document(id=doc['id'], content=doc['content'], meta=doc['metadata']) \n",
                "            for doc in docs_data]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Initialize Document Store\n",
                "\n",
                "Set up the Couchbase document store for storing movie data and embeddings:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize document store\n",
                "document_store = CouchbaseSearchDocumentStore(\n",
                "    cluster_connection_string=Secret.from_token(CB_CONNECTION_STRING),\n",
                "    authenticator=CouchbasePasswordAuthenticator(\n",
                "        username=Secret.from_token(CB_USERNAME),\n",
                "        password=Secret.from_token(CB_PASSWORD)\n",
                "    ),\n",
                "    cluster_options=CouchbaseClusterOptions(\n",
                "        profile=KnownConfigProfiles.WanDevelopment,\n",
                "    ),\n",
                "    bucket=CB_BUCKET_NAME,\n",
                "    scope=CB_SCOPE_NAME,\n",
                "    collection=CB_COLLECTION_NAME,\n",
                "    vector_search_index=CB_INDEX_NAME,\n",
                ")\n",
                "\n",
                "print(\"Couchbase document store initialized successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Initialize Embedder for Document Embedding\n",
                "\n",
                "Configure the document embedder using Capella AI's endpoint and the E5 Mistral model. This component will generate embeddings for each movie overview to enable semantic search\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "embedder = OpenAIDocumentEmbedder(\n",
                "    api_key=Secret.from_token(OPENAI_API_KEY),\n",
                "    model=\"text-embedding-3-large\",\n",
                ")\n",
                "\n",
                "rag_embedder = OpenAITextEmbedder(\n",
                "    api_key=Secret.from_token(OPENAI_API_KEY),\n",
                "    model=\"text-embedding-3-large\",\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Initialize LLM Generator\n",
                "Configure the LLM generator using Capella AI's endpoint and Llama 3.1 model. This component will generate natural language responses based on the retrieved documents.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "llm = OpenAIGenerator(\n",
                "    api_key=Secret.from_token(OPENAI_API_KEY),\n",
                "    model=\"gpt-4o\",\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Create Indexing Pipeline\n",
                "Build the pipeline for processing and indexing movie documents:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create indexing pipeline\n",
                "index_pipeline = Pipeline()\n",
                "index_pipeline.add_component(\"cleaner\", DocumentCleaner())\n",
                "index_pipeline.add_component(\"embedder\", embedder)\n",
                "index_pipeline.add_component(\"writer\", DocumentWriter(document_store=document_store))\n",
                "\n",
                "# Connect indexing components\n",
                "index_pipeline.connect(\"cleaner.documents\", \"embedder.documents\")\n",
                "index_pipeline.connect(\"embedder.documents\", \"writer.documents\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Run Indexing Pipeline\n",
                "\n",
                "Execute the pipeline for processing and indexing movie documents:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run indexing pipeline\n",
                "\n",
                "if documents:\n",
                "    result = index_pipeline.run({\"cleaner\": {\"documents\": documents}})\n",
                "    print(f\"Successfully processed {len(documents)} movie overviews\")\n",
                "    print(f\"Sample document metadata: {documents[0].meta}\")\n",
                "else:\n",
                "    print(\"No documents created. Skipping indexing.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Create RAG Pipeline\n",
                "\n",
                "Set up the Retrieval Augmented Generation pipeline for answering questions about movies:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define RAG prompt template\n",
                "prompt_template = \"\"\"\n",
                "Given these documents, answer the question.\\nDocuments:\n",
                "{% for doc in documents %}\n",
                "    {{ doc.content }}\n",
                "{% endfor %}\n",
                "\n",
                "\\nQuestion: {{question}}\n",
                "\\nAnswer:\n",
                "\"\"\"\n",
                "\n",
                "# Create RAG pipeline\n",
                "rag_pipeline = Pipeline()\n",
                "\n",
                "# Add components\n",
                "rag_pipeline.add_component(\n",
                "    \"query_embedder\",\n",
                "    rag_embedder,\n",
                ")\n",
                "rag_pipeline.add_component(\"retriever\", CouchbaseSearchEmbeddingRetriever(document_store=document_store))\n",
                "rag_pipeline.add_component(\"prompt_builder\", PromptBuilder(template=prompt_template))\n",
                "rag_pipeline.add_component(\"llm\",llm)\n",
                "rag_pipeline.add_component(\"answer_builder\", AnswerBuilder())\n",
                "\n",
                "# Connect RAG components\n",
                "rag_pipeline.connect(\"query_embedder\", \"retriever.query_embedding\")\n",
                "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
                "rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.prompt\")\n",
                "rag_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
                "rag_pipeline.connect(\"llm.meta\", \"answer_builder.meta\")\n",
                "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")\n",
                "\n",
                "print(\"RAG pipeline created successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Ask Questions About Movies\n",
                "\n",
                "Use the RAG pipeline to ask questions about movies and get AI-generated answers:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example question\n",
                "question = \"Who does Savva want to save from the vicious hyenas?\"\n",
                "\n",
                "# Run the RAG pipeline\n",
                "result = rag_pipeline.run(\n",
                "    {\n",
                "        \"query_embedder\": {\"text\": question},\n",
                "        \"retriever\": {\"top_k\": 5},\n",
                "        \"prompt_builder\": {\"question\": question},\n",
                "        \"answer_builder\": {\"query\": question},\n",
                "    },\n",
                "    include_outputs_from={\"retriever\", \"query_embedder\"}\n",
                ")\n",
                "\n",
                "# Get the generated answer\n",
                "answer: GeneratedAnswer = result[\"answer_builder\"][\"answers\"][0]\n",
                "\n",
                "# Print retrieved documents\n",
                "print(\"=== Retrieved Documents ===\")\n",
                "retrieved_docs = result[\"retriever\"][\"documents\"]\n",
                "for idx, doc in enumerate(retrieved_docs, start=1):\n",
                "    print(f\"Id: {doc.id} Title: {doc.meta['title']}\")\n",
                "\n",
                "# Print final results\n",
                "print(\"\\n=== Final Answer ===\")\n",
                "print(f\"Question: {answer.query}\")\n",
                "print(f\"Answer: {answer.data}\")\n",
                "print(\"\\nSources:\")\n",
                "for doc in answer.documents:\n",
                "    print(f\"-> {doc.meta['title']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Conclusion\n",
                "\n",
                "In this tutorial, we built a Retrieval-Augmented Generation (RAG) system using Couchbase Capella, OpenAI, and Haystack with the BBC News dataset. This demonstrates how to combine vector search capabilities with large language models to answer questions about current events using real-time information.\n",
                "\n",
                "The key components include:\n",
                "- **Couchbase Capella** for vector storage and FTS-based retrieval\n",
                "- **Haystack** for pipeline orchestration and component management  \n",
                "- **OpenAI** for embeddings (`text-embedding-3-large`) and text generation (`gpt-4o`)\n",
                "\n",
                "This approach enables AI applications to access and reason over current information that extends beyond the LLM's training data, making responses more accurate and relevant for real-world use cases."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "haystack",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
