{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this guide, we will walk you through building a Retrieval Augmented Generation (RAG) application using Couchbase Capella as the database, [gpt-4o](https://platform.openai.com/docs/models/gpt-4o) model as the large language model provided by OpenAI. We will use the [text-embedding-3-large](https://platform.openai.com/docs/guides/embeddings/embedding-models) model for generating embeddings.\n",
    "\n",
    "This notebook demonstrates how to build a RAG system using:\n",
    "- The [BBC News dataset](https://huggingface.co/datasets/RealTimeData/bbc_news_alltime) containing news articles\n",
    "- Couchbase Capella as the vector store with GSI (Global Secondary Index) for vector search\n",
    "- Haystack framework for the RAG pipeline\n",
    "- OpenAI for embeddings and text generation\n",
    "\n",
    "We leverage Couchbase's Global Secondary Index (GSI) vector search capabilities to create and manage vector indexes, enabling efficient semantic search capabilities. GSI provides high-performance vector search with support for both Hyperscale Vector Indexes and Composite Vector Indexes, designed to scale to billions of vectors with low memory footprint and optimized concurrent operations.\n",
    "\n",
    "Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval. This tutorial will equip you with the knowledge to create a fully functional RAG system using OpenAI Services and Haystack with Couchbase's advanced GSI vector search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you start\n",
    "\n",
    "## Create and Deploy Your Operational cluster on Capella\n",
    "\n",
    "To get started with Couchbase Capella, create an account and use it to deploy an operational cluster.\n",
    "\n",
    "To know more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Couchbase Capella Configuration\n",
    "\n",
    "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met:\n",
    "\n",
    "* Have a multi-node Capella cluster running the Data, Query, Index, and Search services.\n",
    "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the bucket (Read and Write) used in the application.\n",
    "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running.\n",
    "\n",
    "### OpenAI Models Setup\n",
    "\n",
    "In order to create the RAG application, we need an embedding model to ingest the documents for Vector Search and a large language model (LLM) for generating the responses based on the context. \n",
    "\n",
    "For this implementation, we'll use OpenAI's models which provide state-of-the-art performance for both embeddings and text generation:\n",
    "\n",
    "**Embedding Model**: We'll use OpenAI's `text-embedding-3-large` model, which provides high-quality embeddings with 3,072 dimensions for semantic search capabilities.\n",
    "\n",
    "**Large Language Model**: We'll use OpenAI's `gpt-4o` model for generating responses based on the retrieved context. This model offers excellent reasoning capabilities and can handle complex queries effectively.\n",
    "\n",
    "**Prerequisites for OpenAI Integration**:\n",
    "* Create an OpenAI account at [platform.openai.com](https://platform.openai.com)\n",
    "* Generate an API key from your OpenAI dashboard\n",
    "* Ensure you have sufficient credits or a valid payment method set up\n",
    "* Set up your API key as an environment variable or input it securely in the notebook\n",
    "\n",
    "For more details about OpenAI's models and pricing, please refer to the [OpenAI documentation](https://platform.openai.com/docs/models).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Necessary Libraries\n",
    "To build our RAG system, we need a set of libraries. The libraries we install handle everything from connecting to databases to performing AI tasks. Each library has a specific role: Couchbase libraries manage database operations, Haystack handles AI model integrations and pipeline management, and we will use the OpenAI SDK for generating embeddings and calling OpenAI's language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries\n",
    "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, Haystack components for RAG pipeline, embedding generation, and dataset loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import base64\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.exceptions import CouchbaseException\n",
    "from couchbase.options import ClusterOptions, KnownConfigProfiles, QueryOptions\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from haystack import Pipeline, Document, GeneratedAnswer\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder, OpenAITextEmbedder\n",
    "from haystack.components.builders.answer_builder import AnswerBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.utils import Secret\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from couchbase_haystack import (\n",
    "    CouchbaseQueryDocumentStore, \n",
    "    CouchbaseQueryEmbeddingRetriever,\n",
    "    QueryVectorSearchType, \n",
    "    QueryVectorSearchSimilarity,\n",
    "    CouchbasePasswordAuthenticator,\n",
    "    CouchbaseClusterOptions\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Sensitive Information\n",
    "In this section, we prompt the user to input essential configuration settings needed. These settings include sensitive information like database credentials, collection names, and API keys. Instead of hardcoding these details into the script, we request the user to provide them at runtime, ensuring flexibility and security.\n",
    "\n",
    "The script also validates that all required inputs are provided, raising an error if any crucial information is missing. This approach ensures that your integration is both secure and correctly configured without hardcoding sensitive information, enhancing the overall security and maintainability of your code.\n",
    "\n",
    "**OPENAI_API_KEY** is your OpenAI API key which can be obtained from your OpenAI dashboard at [platform.openai.com](https://platform.openai.com/api-keys).\n",
    "\n",
    "**INDEX_NAME** is the name of the GSI vector index we will create for vector search operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_CONNECTION_STRING = input(\"Couchbase Cluster URL (default: localhost): \") or \"couchbase://localhost\"\n",
    "CB_USERNAME = input(\"Couchbase Username (default: admin): \") or \"admin\"\n",
    "CB_PASSWORD = input(\"Couchbase password (default: Password@12345): \") or \"Password@12345\"\n",
    "CB_BUCKET_NAME = input(\"Couchbase Bucket: \")\n",
    "SCOPE_NAME = input(\"Couchbase Scope: \")\n",
    "COLLECTION_NAME = input(\"Couchbase Collection: \")\n",
    "INDEX_NAME = input(\"Vector Search Index: \")\n",
    "OPENAI_API_KEY = input(\"OpenAI API Key: \")\n",
    "\n",
    "# Check if the variables are correctly loaded\n",
    "if not all([CB_CONNECTION_STRING, CB_USERNAME, CB_PASSWORD, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME, INDEX_NAME, OPENAI_API_KEY]):\n",
    "    raise ValueError(\"All configuration variables must be provided.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Logging\n",
    "Logging is essential for tracking the execution of our script and debugging any issues that may arise. We set up a logger that will display information about the script's progress, including timestamps and log levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to Couchbase Capella\n",
    "The next step is to establish a connection to our Couchbase Capella cluster. This connection will allow us to interact with the database, store and retrieve documents, and perform vector searches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize the Couchbase Cluster\n",
    "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "    options = ClusterOptions(auth)\n",
    "    options.apply_profile(KnownConfigProfiles.WanDevelopment)\n",
    "    \n",
    "    # Connect to the cluster\n",
    "    cluster = Cluster(CB_CONNECTION_STRING, options)\n",
    "    \n",
    "    # Wait for the cluster to be ready\n",
    "    cluster.wait_until_ready(timedelta(seconds=5))\n",
    "    logging.info(\"Successfully connected to the Couchbase cluster\")\n",
    "except CouchbaseException as e:\n",
    "    raise RuntimeError(f\"Failed to connect to Couchbase: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Bucket, Scope, and Collection\n",
    "Before we can store our data, we need to ensure that the appropriate bucket, scope, and collection exist in our Couchbase cluster. The code below checks if these components exist and creates them if they don't, providing a foundation for storing our vector embeddings and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from couchbase.management.buckets import CreateBucketSettings\n",
    "import json\n",
    "\n",
    "# Create bucket if it does not exist\n",
    "bucket_manager = cluster.buckets()\n",
    "try:\n",
    "    bucket_manager.get_bucket(CB_BUCKET_NAME)\n",
    "    print(f\"Bucket '{CB_BUCKET_NAME}' already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Bucket '{CB_BUCKET_NAME}' does not exist. Creating bucket...\")\n",
    "    bucket_settings = CreateBucketSettings(name=CB_BUCKET_NAME, ram_quota_mb=500)\n",
    "    bucket_manager.create_bucket(bucket_settings)\n",
    "    print(f\"Bucket '{CB_BUCKET_NAME}' created successfully.\")\n",
    "\n",
    "# Create scope and collection if they do not exist\n",
    "collection_manager = cluster.bucket(CB_BUCKET_NAME).collections()\n",
    "scopes = collection_manager.get_all_scopes()\n",
    "scope_exists = any(scope.name == SCOPE_NAME for scope in scopes)\n",
    "\n",
    "if scope_exists:\n",
    "    print(f\"Scope '{SCOPE_NAME}' already exists.\")\n",
    "else:\n",
    "    print(f\"Scope '{SCOPE_NAME}' does not exist. Creating scope...\")\n",
    "    collection_manager.create_scope(SCOPE_NAME)\n",
    "    print(f\"Scope '{SCOPE_NAME}' created successfully.\")\n",
    "\n",
    "collections = [collection.name for scope in scopes if scope.name == SCOPE_NAME for collection in scope.collections]\n",
    "collection_exists = COLLECTION_NAME in collections\n",
    "\n",
    "if collection_exists:\n",
    "    print(f\"Collection '{COLLECTION_NAME}' already exists in scope '{SCOPE_NAME}'.\")\n",
    "else:\n",
    "    print(f\"Collection '{COLLECTION_NAME}' does not exist in scope '{SCOPE_NAME}'. Creating collection...\")\n",
    "    collection_manager.create_collection(collection_name=COLLECTION_NAME, scope_name=SCOPE_NAME)\n",
    "    print(f\"Collection '{COLLECTION_NAME}' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the BBC News Dataset\n",
    "To build a RAG engine, we need data to search through. We use the [BBC Realtime News dataset](https://huggingface.co/datasets/RealTimeData/bbc_news_alltime), a dataset with up-to-date BBC news articles grouped by month. This dataset contains articles that were created after the LLM was trained. It will showcase the use of RAG to augment the LLM. \n",
    "\n",
    "The BBC News dataset's varied content allows us to simulate real-world scenarios where users ask complex questions, enabling us to fine-tune our RAG's ability to understand and respond to various types of queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    news_dataset = load_dataset('RealTimeData/bbc_news_alltime', '2024-12', split=\"train\")\n",
    "    print(f\"Loaded the BBC News dataset with {len(news_dataset)} rows\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading TREC dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first two examples from the dataset\n",
    "print(\"Dataset columns:\", news_dataset.column_names)\n",
    "print(\"\\nFirst two examples:\")\n",
    "print(news_dataset[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for RAG\n",
    "\n",
    "We need to extract the context passages from the dataset to use as our knowledge base for the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "news_articles = news_dataset\n",
    "unique_articles = {}\n",
    "\n",
    "for article in news_articles:\n",
    "    content = article.get(\"content\")\n",
    "    if content:\n",
    "        content_hash = hashlib.md5(content.encode()).hexdigest()  # Generate hash of content\n",
    "        if content_hash not in unique_articles:\n",
    "            unique_articles[content_hash] = article  # Store full article\n",
    "\n",
    "unique_news_articles = list(unique_articles.values())  # Convert back to list\n",
    "\n",
    "print(f\"We have {len(unique_news_articles)} unique articles in our database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Embeddings using OpenAI\n",
    "Embeddings are numerical representations of text that capture semantic meaning. Unlike keyword-based search, embeddings enable semantic search to understand context and retrieve documents that are conceptually similar even without exact keyword matches. We'll use OpenAI's `text-embedding-3-large` model to create high-quality embeddings with 3,072 dimensions. This model transforms our text data into vector representations that can be efficiently searched using Haystack's OpenAI document embedder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Set up the document embedder for processing documents\n",
    "    document_embedder = OpenAIDocumentEmbedder(\n",
    "        api_key=Secret.from_token(OPENAI_API_KEY),\n",
    "        model=\"text-embedding-3-large\"\n",
    "    )\n",
    "    \n",
    "    # Set up the text embedder for query processing\n",
    "    rag_embedder = OpenAITextEmbedder(\n",
    "        api_key=Secret.from_token(OPENAI_API_KEY),\n",
    "        model=\"text-embedding-3-large\"\n",
    "    )\n",
    "    \n",
    "    print(\"Successfully created embedding models\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating embedding models: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Embeddings Model\n",
    "We can test the text embeddings model by generating an embedding for a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = rag_embedder.run(text=\"this is a test sentence\")\n",
    "test_embedding = test_result[\"embedding\"]\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Couchbase GSI Document Store\n",
    "The Couchbase GSI document store is set up to store the documents from the dataset using Couchbase's Global Secondary Index vector search capabilities. This document store is optimized for high-performance vector similarity search operations and can scale to billions of vectors using Haystack's Couchbase integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create the Couchbase GSI document store\n",
    "    document_store = CouchbaseQueryDocumentStore(\n",
    "        cluster_connection_string=Secret.from_token(CB_CONNECTION_STRING),\n",
    "        authenticator=CouchbasePasswordAuthenticator(\n",
    "            username=Secret.from_token(CB_USERNAME),\n",
    "            password=Secret.from_token(CB_PASSWORD)\n",
    "        ),\n",
    "        cluster_options=CouchbaseClusterOptions(\n",
    "            profile=KnownConfigProfiles.WanDevelopment,\n",
    "        ),\n",
    "        bucket=CB_BUCKET_NAME,\n",
    "        scope=SCOPE_NAME,\n",
    "        collection=COLLECTION_NAME,\n",
    "        search_type=QueryVectorSearchType.ANN,\n",
    "        similarity=QueryVectorSearchSimilarity.L2\n",
    "    )\n",
    "    print(\"Successfully created GSI document store\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to create GSI document store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Haystack Documents\n",
    "In this section, we'll process our news articles and create Haystack Document objects.\n",
    "Each Document is created with specific metadata that will be used for retrieval and generation.\n",
    "We'll observe examples of the document content to understand how the documents are structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haystack_documents = []\n",
    "# Process and store documents\n",
    "for article in unique_news_articles:  # Process all unique articles\n",
    "    try:\n",
    "        document = Document(\n",
    "            content=article[\"content\"],\n",
    "            meta={\n",
    "                \"title\": article[\"title\"],\n",
    "                \"description\": article[\"description\"],\n",
    "                \"published_date\": article[\"published_date\"],\n",
    "                \"link\": article[\"link\"],\n",
    "            }\n",
    "        )\n",
    "        haystack_documents.append(document)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create document: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Observing an example of the document content\n",
    "print(\"Document content preview:\")\n",
    "print(f\"Content: {haystack_documents[0].content[:200]}...\")\n",
    "print(f\"Metadata: {haystack_documents[0].meta}\")\n",
    "\n",
    "print(f\"Created {len(haystack_documents)} documents\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Running the Indexing Pipeline\n",
    "\n",
    "In this section, we'll create an indexing pipeline to process our documents. The pipeline will:\n",
    "\n",
    "1. Split the documents into smaller chunks using the DocumentSplitter\n",
    "2. Generate embeddings for each chunk using our document embedder\n",
    "3. Store these chunks with their embeddings in our Couchbase document store\n",
    "\n",
    "This process transforms our raw documents into a searchable knowledge base that can be queried semantically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Process documents: split into chunks, generate embeddings, and store in document store\n",
    "# Create indexing pipeline\n",
    "indexing_pipeline = Pipeline()\n",
    "indexing_pipeline.add_component(\"cleaner\", DocumentCleaner())\n",
    "indexing_pipeline.add_component(\"embedder\", document_embedder)\n",
    "indexing_pipeline.add_component(\"writer\", DocumentWriter(document_store=document_store))\n",
    "\n",
    "indexing_pipeline.connect(\"cleaner.documents\", \"embedder.documents\")\n",
    "indexing_pipeline.connect(\"embedder.documents\", \"writer.documents\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Indexing Pipeline\n",
    "\n",
    "Execute the pipeline for processing and indexing BCC news documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the indexing pipeline\n",
    "if haystack_documents:\n",
    "    result = indexing_pipeline.run({\"cleaner\": {\"documents\": haystack_documents}})\n",
    "    print(f\"Indexed {len(result['writer']['documents_written'])} document chunks\")\n",
    "else:\n",
    "    print(\"No documents created. Skipping indexing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using OpenAI's Large Language Model (LLM)\n",
    "Large language models are AI systems that are trained to understand and generate human language. We'll be using OpenAI's `gpt-4o` model to process user queries and generate meaningful responses based on the retrieved context from our Couchbase document store. This model is a key component of our RAG system, allowing it to go beyond simple keyword matching and truly understand the intent behind a query. By integrating OpenAI's LLM, we equip our RAG system with the ability to interpret complex queries, understand the nuances of language, and provide more accurate and contextually relevant responses.\n",
    "\n",
    "The language model's ability to understand context and generate coherent responses is what makes our RAG system truly intelligent. It can not only find the right information but also present it in a way that is useful and understandable to the user.\n",
    "\n",
    "The LLM is configured using Haystack's OpenAI generator component with your OpenAI API key for seamless integration with their services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Set up the LLM generator\n",
    "    generator = OpenAIGenerator(\n",
    "        api_key=Secret.from_token(OPENAI_API_KEY),\n",
    "        model=\"gpt-4o\"\n",
    "    )\n",
    "    logging.info(\"Successfully created the OpenAI generator\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating OpenAI generator: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the RAG Pipeline\n",
    "\n",
    "In this section, we'll create a RAG pipeline using Haystack components. This pipeline serves as the foundation for our RAG system, enabling semantic search capabilities and efficient retrieval of relevant information.\n",
    "\n",
    "The RAG pipeline provides a complete workflow that allows us to:\n",
    "1. Perform semantic searches based on user queries\n",
    "2. Retrieve the most relevant documents or chunks\n",
    "3. Generate contextually appropriate responses using our LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RAG prompt template\n",
    "prompt_template = \"\"\"\n",
    "Given these documents, answer the question.\\nDocuments:\n",
    "{% for doc in documents %}\n",
    "    {{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "\\nQuestion: {{question}}\n",
    "\\nAnswer:\n",
    "\"\"\"\n",
    "\n",
    "# Create the RAG pipeline\n",
    "rag_pipeline = Pipeline()\n",
    "\n",
    "# Add components to the pipeline\n",
    "rag_pipeline.add_component(\n",
    "    \"query_embedder\",\n",
    "    rag_embedder,\n",
    ")\n",
    "rag_pipeline.add_component(\"retriever\", CouchbaseQueryEmbeddingRetriever(document_store=document_store))\n",
    "rag_pipeline.add_component(\"prompt_builder\", PromptBuilder(template=prompt_template))\n",
    "rag_pipeline.add_component(\"llm\",generator)\n",
    "rag_pipeline.add_component(\"answer_builder\", AnswerBuilder())\n",
    "\n",
    "# Connect RAG components\n",
    "rag_pipeline.connect(\"query_embedder\", \"retriever.query_embedding\")\n",
    "rag_pipeline.connect(\"retriever.documents\", \"prompt_builder.documents\")\n",
    "rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.prompt\")\n",
    "rag_pipeline.connect(\"llm.replies\", \"answer_builder.replies\")\n",
    "rag_pipeline.connect(\"llm.meta\", \"answer_builder.meta\")\n",
    "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")\n",
    "\n",
    "print(\"Successfully created RAG pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) with Couchbase and Haystack\n",
    "\n",
    "Let's test our RAG system by performing a semantic search on a sample query. In this example, we'll use a question about Pep Guardiola's reaction to Manchester City's recent form. The RAG system will:\n",
    "\n",
    "1. Process the natural language query\n",
    "2. Search through our document store for relevant information\n",
    "3. Retrieve the most semantically similar documents\n",
    "4. Generate a comprehensive response using the LLM\n",
    "\n",
    "This demonstrates how our system combines the power of vector search with language model capabilities to provide accurate, contextual answers based on the information in our database.\n",
    "\n",
    "**Note:** By default, without any GSI vector index, Couchbase uses linear brute force search which compares the query vector against every document in the collection. This works for small datasets but can become slow as the dataset grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample query from the dataset\n",
    "\n",
    "query = \"Who will Daniel Dubois fight in Saudi Arabia on 22 February?\"\n",
    "\n",
    "try:\n",
    "    # Perform the semantic search using the RAG pipeline\n",
    "    start_time = time.time()\n",
    "    result = rag_pipeline.run({\n",
    "        \"query_embedder\": {\"text\": query},\n",
    "        \"retriever\": {\"top_k\": 5},\n",
    "        \"prompt_builder\": {\"question\": query},\n",
    "        \"answer_builder\": {\"query\": query},\n",
    "        },\n",
    "     include_outputs_from={\"retriever\", \"query_embedder\"}\n",
    "    )\n",
    "    search_elapsed_time = time.time() - start_time\n",
    "    # Get the generated answer\n",
    "    answer: GeneratedAnswer = result[\"answer_builder\"][\"answers\"][0]\n",
    "\n",
    "    # Print retrieved documents\n",
    "    print(\"=== Retrieved Documents ===\")\n",
    "    retrieved_docs = result[\"retriever\"][\"documents\"]\n",
    "    for idx, doc in enumerate(retrieved_docs, start=1):\n",
    "        print(f\"Id: {doc.id} Title: {doc.meta['title']}\")\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\n=== Final Answer ===\")\n",
    "    print(f\"Question: {answer.query}\")\n",
    "    print(f\"Answer: {answer.data}\")\n",
    "    print(\"\\nSources:\")\n",
    "    for doc in answer.documents:\n",
    "        print(f\"-> {doc.meta['title']}\")\n",
    "    # Display search results\n",
    "    print(f\"\\nOptimized GSI Vector Search Results (completed in {search_elapsed_time:.2f} seconds):\")\n",
    "    #print(result[\"generator\"][\"replies\"][0])\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error performing RAG search: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create GSI Vector Index (Optimized Search)\n",
    "\n",
    "While the above RAG system works effectively, we can significantly improve query performance by leveraging Couchbase's advanced GSI vector search capabilities.\n",
    "\n",
    "Couchbase offers three types of vector indexes, but for GSI-based vector search we focus on two main types:\n",
    "\n",
    "In this section, we'll set up the Couchbase vector store using GSI (Global Secondary Index) for high-performance vector search.\n",
    "\n",
    "GSI vector search supports two main index types:\n",
    "\n",
    "## Hyperscale Vector Indexes\n",
    "- Specifically designed for vector searches\n",
    "- Perform vector similarity and semantic searches faster than the other types of indexes\n",
    "- Designed to scale to billions of vectors\n",
    "- Most of the index resides in a highly optimized format on disk\n",
    "- High accuracy even for vectors with a large number of dimensions\n",
    "- Supports concurrent searches and inserts for datasets that are constantly changing\n",
    "\n",
    "Use this type of index when you want to primarily query vector values with a low memory footprint. In general, Hyperscale Vector indexes are the best choice for most applications that use vector searches.\n",
    "\n",
    "## Composite Vector Indexes\n",
    "- Combines a standard Global Secondary index (GSI) with a single vector column\n",
    "- Designed for searches using a single vector value along with standard scalar values that filter out large portions of the dataset. The scalar attributes in a query reduce the number of vectors the Couchbase Server has to compare when performing a vector search to find similar vectors.\n",
    "- Consume a moderate amount of memory and can index billions of documents.\n",
    "- Work well for cases where your queries are highly selective — returning a small number of results from a large dataset\n",
    "\n",
    "Use Composite Vector indexes when you want to perform searches of documents using both scalars and a vector where the scalar values filter out large portions of the dataset.\n",
    "\n",
    "For more details, see the [Couchbase Vector Index documentation](https://docs.couchbase.com/server/current/vector-index/use-vector-indexes.html).\n",
    "\n",
    "## Understanding Index Configuration (Couchbase 8.0 Feature)\n",
    "\n",
    "The index_description parameter controls how Couchbase optimizes vector storage and search performance through centroids and quantization:\n",
    "\n",
    "Format: `'IVF[<centroids>],{PQ|SQ}<settings>'`\n",
    "\n",
    "**Centroids (IVF - Inverted File):**\n",
    "- Controls how the dataset is subdivided for faster searches\n",
    "- More centroids = faster search, slower training  \n",
    "- Fewer centroids = slower search, faster training\n",
    "- If omitted (like IVF,SQ8), Couchbase auto-selects based on dataset size\n",
    "\n",
    "**Quantization Options:**\n",
    "- SQ (Scalar Quantization): SQ4, SQ6, SQ8 (4, 6, or 8 bits per dimension)\n",
    "- PQ (Product Quantization): PQ<subquantizers>x<bits> (e.g., PQ32x8)\n",
    "- Higher values = better accuracy, larger index size\n",
    "\n",
    "**Common Examples:**\n",
    "- IVF,SQ8 - Auto centroids, 8-bit scalar quantization (good default)\n",
    "- IVF1000,SQ6 - 1000 centroids, 6-bit scalar quantization  \n",
    "- IVF,PQ32x8 - Auto centroids, 32 subquantizers with 8 bits\n",
    "\n",
    "For detailed configuration options, see the [Quantization & Centroid Settings](https://docs.couchbase.com/server/current/vector-index/hyperscale-vector-index.html#algo_settings).\n",
    "\n",
    "In the code below, we demonstrate creating a BHIVE index for optimal performance. This method takes an index type (BHIVE or COMPOSITE) and description parameter for optimization settings. Alternatively, GSI indexes can be created manually from the Couchbase UI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BHIVE (Hyperscale Vector Index) for optimized vector search\n",
    "try:\n",
    "    bhive_index_name = f\"{INDEX_NAME}_bhive\"\n",
    "\n",
    "    # Use the cluster connection to create the BHIVE index\n",
    "    scope = cluster.bucket(CB_BUCKET_NAME).scope(SCOPE_NAME)\n",
    "    \n",
    "    options = {\n",
    "        \"dimension\": 3072,  # text-embedding-3-large dimension\n",
    "        \"description\": \"IVF1024,PQ32x8\",\n",
    "        \"similarity\": \"L2\",\n",
    "    }\n",
    "    \n",
    "    scope.query(\n",
    "        f\"\"\"\n",
    "        CREATE INDEX {bhive_index_name}\n",
    "        ON {COLLECTION_NAME} (embedding VECTOR)\n",
    "        USING GSI WITH {json.dumps(options)}\n",
    "        \"\"\",\n",
    "    QueryOptions(\n",
    "        timeout=timedelta(seconds=300)\n",
    "    )).execute()\n",
    "    print(f\"Successfully created BHIVE index: {bhive_index_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"BHIVE index may already exist or error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Optimized GSI Vector Search\n",
    "\n",
    "The example below shows running the same RAG query, but now using the BHIVE GSI index we created above. You'll notice improved performance as the index efficiently retrieves data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the optimized GSI vector search with BHIVE index\n",
    "query = \"Who will Daniel Dubois fight in Saudi Arabia on 22 February?\"\n",
    "\n",
    "try:\n",
    "    # The RAG pipeline will automatically use the optimized GSI index\n",
    "    # Perform the semantic search with GSI optimization\n",
    "    start_time = time.time()\n",
    "    result = rag_pipeline.run({\n",
    "        \"query_embedder\": {\"text\": query},\n",
    "        \"retriever\": {\"top_k\": 4},\n",
    "        \"prompt_builder\": {\"question\": query},\n",
    "        \"answer_builder\": {\"query\": query},\n",
    "        },\n",
    "     include_outputs_from={\"retriever\", \"query_embedder\"}\n",
    "    )\n",
    "    search_elapsed_time = time.time() - start_time\n",
    "    # Get the generated answer\n",
    "    answer: GeneratedAnswer = result[\"answer_builder\"][\"answers\"][0]\n",
    "\n",
    "    # Print retrieved documents\n",
    "    print(\"=== Retrieved Documents ===\")\n",
    "    retrieved_docs = result[\"retriever\"][\"documents\"]\n",
    "    for idx, doc in enumerate(retrieved_docs, start=0):\n",
    "        print(f\"Id: {doc.id} Title: {doc.meta['title']}\")\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\n=== Final Answer ===\")\n",
    "    print(f\"Question: {answer.query}\")\n",
    "    print(f\"Answer: {answer.data}\")\n",
    "    print(\"\\nSources:\")\n",
    "    for doc in answer.documents:\n",
    "        print(f\"-> {doc.meta['title']}\")\n",
    "    # Display search results\n",
    "    print(f\"\\nOptimized GSI Vector Search Results (completed in {search_elapsed_time:.2f} seconds):\")\n",
    "    #print(result[\"generator\"][\"replies\"][0])\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error performing optimized semantic search: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this tutorial, we've built a Retrieval Augmented Generation (RAG) system using Couchbase Capella's GSI vector search, OpenAI, and Haystack. We used the BBC News dataset, which contains real-time news articles, to demonstrate how RAG can be used to answer questions about current events and provide up-to-date information that extends beyond the LLM's training data.\n",
    "\n",
    "The key components of our RAG system include:\n",
    "\n",
    "1. **Couchbase Capella GSI Vector Search** as the high-performance vector database for storing and retrieving document embeddings\n",
    "2. **Haystack** as the framework for building modular RAG pipelines with flexible component connections\n",
    "3. **OpenAI Services** for generating embeddings (`text-embedding-3-large`) and LLM responses (`gpt-4o`)\n",
    "4. **GSI Vector Indexes** (BHIVE/Composite) for optimized vector search performance\n",
    "\n",
    "This approach allows us to enhance the capabilities of large language models by grounding their responses in specific, up-to-date information from our knowledge base, while leveraging Couchbase's advanced GSI vector search for optimal performance and scalability. Haystack's modular pipeline approach provides flexibility and extensibility for building complex RAG applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
