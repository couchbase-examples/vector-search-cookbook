{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3459e9c0",
   "metadata": {},
   "source": [
    "# Semantic Search with Couchbase GSI Vector Indexes and Jina AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0c7eae",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569c4838",
   "metadata": {},
   "source": [
    "This tutorial demonstrates building a high-performance semantic search engine using Couchbase's GSI (Global Secondary Index) vector search and Jina AI for embeddings and language models. We'll show measurable performance improvements with GSI optimization and implement a complete RAG (Retrieval-Augmented Generation) system. Alternatively if you want to perform semantic search using the FTS, please take a look at [this.](https://developer.couchbase.com/tutorial-jina-couchbase-rag-with-fts)\n",
    "\n",
    "**Key Features:**\n",
    "- High-performance GSI vector search with BHIVE indexing\n",
    "- Jina AI embeddings and language models\n",
    "- Performance benchmarks showing GSI benefits\n",
    "- Complete RAG workflow with caching optimization\n",
    "\n",
    "**Requirements:** Couchbase Server 8.0+ or Capella with Query Service enabled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c99a0cf",
   "metadata": {},
   "source": [
    "## How to Run This Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f64ee4",
   "metadata": {},
   "source": [
    "This tutorial is available as a Jupyter Notebook that you can run interactively on [Google Colab](https://colab.research.google.com/) or locally by setting up the Python environment. You can access the original notebook [here](https://github.com/couchbase-examples/vector-search-cookbook/blob/main/jinaai/gsi/RAG_with_Couchbase_and_Jina_AI.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865125e5",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca8959",
   "metadata": {},
   "source": [
    "### System Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ae0e50",
   "metadata": {},
   "source": [
    "- **Couchbase Server 8.0+** or Couchbase Capella\n",
    "- **Query Service enabled** (required for GSI Vector Indexes)\n",
    "- **Jina AI API credentials** ([Get them here](https://jina.ai/))\n",
    "- **JinaChat API credentials** ([Get them here](https://chat.jina.ai/api))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa0e83",
   "metadata": {},
   "source": [
    "### Couchbase Capella Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a91cf7",
   "metadata": {},
   "source": [
    "1. **Create Account:** Deploy a [free tier cluster](https://cloud.couchbase.com/sign-up)\n",
    "2. **Configure Access:** Set up database credentials and network security  \n",
    "3. **Enable Query Service:** Required for GSI vector search functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429522e4",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb33ec",
   "metadata": {},
   "source": [
    "### Install Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317fdebc",
   "metadata": {},
   "source": [
    "Install the necessary packages for Couchbase GSI vector search, Jina AI integration, and LangChain RAG capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ec9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Jina doesnt support openai other than 0.27\n",
    "%pip install --quiet datasets==3.6.0 langchain-couchbase==0.5.0 langchain-community==0.3.24 openai==0.27 python-dotenv==1.1.0 ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ffe2f",
   "metadata": {},
   "source": [
    "### Import Required Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac90d29",
   "metadata": {},
   "source": [
    "Import libraries for Couchbase GSI vector search, Jina AI models, and LangChain components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72845965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.exceptions import (CouchbaseException)\n",
    "from couchbase.management.buckets import CreateBucketSettings\n",
    "from couchbase.options import ClusterOptions\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.chat_models import JinaChat\n",
    "from langchain_community.embeddings import JinaEmbeddings\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_couchbase.cache import CouchbaseCache\n",
    "from langchain_couchbase.vectorstores import CouchbaseQueryVectorStore\n",
    "from langchain_couchbase.vectorstores import DistanceStrategy\n",
    "from langchain_couchbase.vectorstores import IndexType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e11f3",
   "metadata": {},
   "source": [
    "### Configure Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4fab47",
   "metadata": {},
   "source": [
    "Set up logging to track progress and capture any errors during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452f2ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',force=True)\n",
    "\n",
    "# Suppress all logs from specific loggers\n",
    "logging.getLogger('openai').setLevel(logging.WARNING)\n",
    "logging.getLogger('httpx').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda451b3",
   "metadata": {},
   "source": [
    "### Environment Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc32e2a",
   "metadata": {},
   "source": [
    "Load environment variables for secure access to Jina AI and Couchbase services. Create a `.env` file with your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f03167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"./.env\") \n",
    "\n",
    "JINA_API_KEY = os.getenv(\"JINA_API_KEY\")\n",
    "JINACHAT_API_KEY = os.getenv(\"JINACHAT_API_KEY\")\n",
    "\n",
    "CB_HOST = os.getenv(\"CB_HOST\") or 'couchbase://localhost'\n",
    "CB_USERNAME = os.getenv(\"CB_USERNAME\") or 'Administrator'\n",
    "CB_PASSWORD = os.getenv(\"CB_PASSWORD\") or 'password'\n",
    "CB_BUCKET_NAME = os.getenv(\"CB_BUCKET_NAME\") or 'vector-search-testing'\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\") or 'vector_search_jina'\n",
    "\n",
    "SCOPE_NAME = os.getenv(\"SCOPE_NAME\") or 'shared'\n",
    "COLLECTION_NAME = os.getenv(\"COLLECTION_NAME\") or 'jina'\n",
    "CACHE_COLLECTION = os.getenv(\"CACHE_COLLECTION\") or 'cache'\n",
    "\n",
    "# Check if the variables are correctly loaded\n",
    "if not JINA_API_KEY:\n",
    "    raise ValueError(\"JINA_API_KEY environment variable is not set\")\n",
    "if not JINACHAT_API_KEY:\n",
    "    raise ValueError(\"JINACHAT_API_KEY environment variable is not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8cf779",
   "metadata": {},
   "source": [
    "## Couchbase Connection Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe5cc9",
   "metadata": {},
   "source": [
    "### Connect to Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ffa96d",
   "metadata": {},
   "source": [
    "Establish connection to Couchbase cluster for vector storage and retrieval operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba814468",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 11:18:34,736 - INFO - Successfully connected to Couchbase\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "    options = ClusterOptions(auth)\n",
    "    cluster = Cluster(CB_HOST, options)\n",
    "    cluster.wait_until_ready(timedelta(seconds=5))\n",
    "    logging.info(\"Successfully connected to Couchbase\")\n",
    "except Exception as e:\n",
    "    raise ConnectionError(f\"Failed to connect to Couchbase: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68554e69",
   "metadata": {},
   "source": [
    "### Setup Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6465c41",
   "metadata": {},
   "source": [
    "The setup_collection() function handles creating and configuring the hierarchical data organization in Couchbase:\n",
    "\n",
    "1. Bucket Creation:\n",
    "   - Checks if specified bucket exists, creates it if not\n",
    "   - Sets bucket properties like RAM quota (1024MB) and replication (disabled)\n",
    "   - Note: You will not be able to create a bucket on Capella\n",
    "\n",
    "2. Scope Management:  \n",
    "   - Verifies if requested scope exists within bucket\n",
    "   - Creates new scope if needed (unless it's the default \"_default\" scope)\n",
    "\n",
    "3. Collection Setup:\n",
    "   - Checks for collection existence within scope\n",
    "   - Creates collection if it doesn't exist\n",
    "   - Waits 2 seconds for collection to be ready\n",
    "\n",
    "Additional Tasks:\n",
    "- Clears any existing documents for clean state\n",
    "\n",
    "The function is called twice to set up:\n",
    "1. Main collection for vector embeddings\n",
    "2. Cache collection for storing results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83ccb1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 11:18:36,208 - INFO - Bucket 'vector-search-testing' exists.\n",
      "2025-10-08 11:18:36,219 - INFO - Collection 'jina' already exists. Skipping creation.\n",
      "2025-10-08 11:18:38,322 - INFO - All documents cleared from the collection.\n",
      "2025-10-08 11:18:38,322 - INFO - Bucket 'vector-search-testing' exists.\n",
      "2025-10-08 11:18:38,327 - INFO - Collection 'jina_cache' already exists. Skipping creation.\n",
      "2025-10-08 11:18:40,480 - INFO - All documents cleared from the collection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<couchbase.collection.Collection at 0x127cdee90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup_collection(cluster, bucket_name, scope_name, collection_name):\n",
    "    try:\n",
    "        # Check if bucket exists, create if it doesn't\n",
    "        try:\n",
    "            bucket = cluster.bucket(bucket_name)\n",
    "            logging.info(f\"Bucket '{bucket_name}' exists.\")\n",
    "        except Exception as e:\n",
    "            logging.info(f\"Bucket '{bucket_name}' does not exist. Creating it...\")\n",
    "            bucket_settings = CreateBucketSettings(\n",
    "                name=bucket_name,\n",
    "                bucket_type='couchbase',\n",
    "                ram_quota_mb=1024,\n",
    "                flush_enabled=True,\n",
    "                num_replicas=0\n",
    "            )\n",
    "            cluster.buckets().create_bucket(bucket_settings)\n",
    "            time.sleep(2)  # Wait for bucket creation to complete and become available\n",
    "            bucket = cluster.bucket(bucket_name)\n",
    "            logging.info(f\"Bucket '{bucket_name}' created successfully.\")\n",
    "\n",
    "        bucket_manager = bucket.collections()\n",
    "\n",
    "        # Check if scope exists, create if it doesn't\n",
    "        scopes = bucket_manager.get_all_scopes()\n",
    "        scope_exists = any(scope.name == scope_name for scope in scopes)\n",
    "        \n",
    "        if not scope_exists and scope_name != \"_default\":\n",
    "            logging.info(f\"Scope '{scope_name}' does not exist. Creating it...\")\n",
    "            bucket_manager.create_scope(scope_name)\n",
    "            logging.info(f\"Scope '{scope_name}' created successfully.\")\n",
    "\n",
    "        # Check if collection exists, create if it doesn't\n",
    "        collections = bucket_manager.get_all_scopes()\n",
    "        collection_exists = any(\n",
    "            scope.name == scope_name and collection_name in [col.name for col in scope.collections]\n",
    "            for scope in collections\n",
    "        )\n",
    "\n",
    "        if not collection_exists:\n",
    "            logging.info(f\"Collection '{collection_name}' does not exist. Creating it...\")\n",
    "            bucket_manager.create_collection(scope_name, collection_name)\n",
    "            logging.info(f\"Collection '{collection_name}' created successfully.\")\n",
    "        else:\n",
    "            logging.info(f\"Collection '{collection_name}' already exists. Skipping creation.\")\n",
    "\n",
    "        # Wait for collection to be ready\n",
    "        collection = bucket.scope(scope_name).collection(collection_name)\n",
    "        time.sleep(2)  # Give the collection time to be ready for queries\n",
    "\n",
    "        # Clear all documents in the collection\n",
    "        try:\n",
    "            query = f\"DELETE FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
    "            cluster.query(query).execute()\n",
    "            logging.info(\"All documents cleared from the collection.\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error while clearing documents: {str(e)}. The collection might be empty.\")\n",
    "\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error setting up collection: {str(e)}\")\n",
    "    \n",
    "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)\n",
    "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, CACHE_COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6740dc",
   "metadata": {},
   "source": [
    "## Document Processing and Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ce47f",
   "metadata": {},
   "source": [
    "### Create Jina Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ff2f94",
   "metadata": {},
   "source": [
    "Set up Jina AI embeddings to convert text into high-dimensional vectors that capture semantic meaning for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed50574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 11:18:56,191 - INFO - Successfully created JinaEmbeddings\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    embeddings = JinaEmbeddings(\n",
    "        jina_api_key=JINA_API_KEY, model_name=\"jina-embeddings-v3\"\n",
    "    )\n",
    "    logging.info(\"Successfully created JinaEmbeddings\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating JinaEmbeddings: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61549cc",
   "metadata": {},
   "source": [
    "### Create GSI Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac616279",
   "metadata": {},
   "source": [
    "Set up the GSI vector store for high-performance vector storage and similarity search using Couchbase's Query Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b16d7be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 11:18:57,341 - INFO - Successfully created GSI vector store\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vector_store = CouchbaseQueryVectorStore(\n",
    "        cluster=cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=embeddings,\n",
    "        distance_metric=DistanceStrategy.COSINE\n",
    "    )\n",
    "    logging.info(\"Successfully created GSI vector store\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to create GSI vector store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080a40dd",
   "metadata": {},
   "source": [
    "### Index Creation Timing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ab4b1b",
   "metadata": {},
   "source": [
    "**Important**: GSI Vector Indexes must be created AFTER uploading vector data. The index creation process analyzes existing vectors to optimize search performance through clustering and quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e134d",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2846784f",
   "metadata": {},
   "source": [
    "Load the BBC News dataset for real-world testing data with authentic news articles covering various topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45f85e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 11:19:03,903 - INFO - Successfully loaded the BBC News dataset with 2687 rows.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the BBC News dataset with 2687 rows\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    news_dataset = load_dataset(\n",
    "        \"RealTimeData/bbc_news_alltime\", \"2024-12\", split=\"train\"\n",
    "    )\n",
    "    print(f\"Loaded the BBC News dataset with {len(news_dataset)} rows\")\n",
    "    logging.info(f\"Successfully loaded the BBC News dataset with {len(news_dataset)} rows.\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading the BBC News dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b521a3b5",
   "metadata": {},
   "source": [
    "#### Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d030e9",
   "metadata": {},
   "source": [
    "Remove duplicate articles to ensure clean search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df70cb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1749 unique articles in our database.\n"
     ]
    }
   ],
   "source": [
    "news_articles = news_dataset[\"content\"]\n",
    "unique_articles = set()\n",
    "for article in news_articles:\n",
    "    if article:\n",
    "        unique_articles.add(article)\n",
    "unique_news_articles = list(unique_articles)\n",
    "print(f\"We have {len(unique_news_articles)} unique articles in our database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee290eb0",
   "metadata": {},
   "source": [
    "#### Store Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4fc61b",
   "metadata": {},
   "source": [
    "Process articles in batches and store them in the vector database with embeddings. We'll use 60% of the dataset for faster processing while maintaining good search quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f3d18b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 11:20:18,363 - INFO - Document ingestion completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Calculate 60% of the dataset size and round to nearest integer\n",
    "dataset_size = len(unique_news_articles)\n",
    "subset_size = round(dataset_size * 0.6)\n",
    "\n",
    "# Filter articles by length and create subset\n",
    "filtered_articles = [article for article in unique_news_articles[:subset_size] \n",
    "                    if article and len(article) <= 50000]\n",
    "\n",
    "# Process in batches\n",
    "batch_size = 50\n",
    "\n",
    "try:\n",
    "    vector_store.add_texts(\n",
    "        texts=filtered_articles,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    logging.info(\"Document ingestion completed successfully\")\n",
    "    \n",
    "except CouchbaseException as e:\n",
    "    logging.error(f\"Couchbase error during ingestion: {str(e)}\")\n",
    "    raise RuntimeError(f\"Error performing document ingestion: {str(e)}\")\n",
    "except Exception as e:\n",
    "    if \"Payment Required\" in str(e):\n",
    "        logging.error(\"Payment required for Jina AI API. Please check your subscription status and API key.\")\n",
    "        print(\"To resolve this error:\")\n",
    "        print(\"1. Visit 'https://jina.ai/reader/#pricing' to review subscription options\")\n",
    "        print(\"2. Ensure your API key is valid and has sufficient credits\") \n",
    "        print(\"3. Consider upgrading your subscription plan if needed\")\n",
    "    else:\n",
    "        logging.error(f\"Unexpected error during ingestion: {str(e)}\")\n",
    "        raise RuntimeError(f\"Failed to save documents to vector store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93f69b5",
   "metadata": {},
   "source": [
    "## Vector Search Performance Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba3085",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Now let's demonstrate the performance benefits of GSI optimization by testing pure vector search performance. We'll compare three optimization levels:\n",
    "\n",
    "1. **Baseline Performance**: Vector search without GSI optimization\n",
    "2. **Vector Search-Optimized Performance**: Same search with BHIVE GSI index\n",
    "3. **Cache Benefits**: Show how caching can be applied on top of GSI for repeated queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200aa57",
   "metadata": {},
   "source": [
    "### GSI Vector Index Types Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd16ef6c",
   "metadata": {},
   "source": [
    "Before we start testing, let's understand the index types available:\n",
    "\n",
    "**Hyperscale Vector Indexes (BHIVE):**\n",
    "- **Best for**: Pure vector searches - content discovery, recommendations, semantic search\n",
    "- **Performance**: High performance with low memory footprint, designed to scale to billions of vectors\n",
    "- **Optimization**: Optimized for concurrent operations, supports simultaneous searches and inserts\n",
    "- **Use when**: You primarily perform vector-only queries without complex scalar filtering\n",
    "- **Ideal for**: Large-scale semantic search, recommendation systems, content discovery\n",
    "\n",
    "**Composite Vector Indexes:**\n",
    "- **Best for**: Filtered vector searches that combine vector search with scalar value filtering\n",
    "- **Performance**: Efficient pre-filtering where scalar attributes reduce the vector comparison scope\n",
    "- **Use when**: Your queries combine vector similarity with scalar filters that eliminate large portions of data\n",
    "- **Ideal for**: Compliance-based filtering, user-specific searches, time-bounded queries\n",
    "- **Note**: Scalar filters take precedence over vector similarity\n",
    "\n",
    "**Choosing the Right Index Type:**\n",
    "- Start with Hyperscale Vector Index for pure vector searches and large datasets\n",
    "- Use Composite Vector Index when scalar filters significantly reduce your search space\n",
    "- Consider your dataset size: Hyperscale scales to billions, Composite works well for tens of millions to billions\n",
    "\n",
    "For this tutorial, we'll use **BHIVE** as it's optimized for pure semantic search scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82799ae",
   "metadata": {},
   "source": [
    "### Index Configuration Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c28359a",
   "metadata": {},
   "source": [
    "The `index_description` parameter controls how Couchbase optimizes vector storage and search performance through centroids and quantization:\n",
    "\n",
    "**Format**: `'IVF[<centroids>],{PQ|SQ}<settings>'`\n",
    "\n",
    "#### **IVF (Inverted File Index) - Centroids Configuration**\n",
    "- **Purpose**: Controls how the dataset is subdivided into clusters for faster searches\n",
    "- **Trade-offs**: More centroids = faster searches but slower training time\n",
    "- **Auto-selection**: If omitted (e.g., `IVF,SQ8`), Couchbase automatically selects the optimal number based on dataset size\n",
    "- **Manual setting**: Specify exact count (e.g., `IVF1000,SQ8` for 1000 centroids)\n",
    "\n",
    "#### **Quantization Options - Vector Compression**\n",
    "\n",
    "**SQ (Scalar Quantization)**\n",
    "- **Purpose**: Compresses vectors by reducing precision of individual components\n",
    "- **Settings**: `SQ4`, `SQ6`, `SQ8` (4-bit, 6-bit, 8-bit precision)\n",
    "- **Trade-off**: Lower bits = more compression but less precision\n",
    "- **Best for**: General-purpose applications where some precision loss is acceptable\n",
    "\n",
    "**PQ (Product Quantization)**\n",
    "- **Purpose**: Advanced compression using subquantizers for better precision\n",
    "- **Format**: `PQ<subquantizers>x<bits>` (e.g., `PQ32x8` = 32 subquantizers of 8 bits each)\n",
    "- **Trade-off**: More complex but often better precision than SQ at similar compression ratios\n",
    "- **Best for**: Applications requiring high precision with significant compression\n",
    "\n",
    "#### **Common Configuration Examples**\n",
    "\n",
    "```\n",
    "IVF,SQ8          # Auto-selected centroids with 8-bit scalar quantization (recommended default)\n",
    "IVF1000,SQ6      # 1000 centroids with 6-bit scalar quantization (higher compression)\n",
    "IVF,PQ32x8       # Auto-selected centroids with 32 subquantizers of 8 bits each\n",
    "IVF500,PQ16x4    # 500 centroids with 16 subquantizers of 4 bits each (high compression)\n",
    "```\n",
    "\n",
    "#### **Performance Considerations**\n",
    "\n",
    "**Distance Interpretation**: In GSI vector search, lower distance values indicate higher similarity, while higher distance values indicate lower similarity.\n",
    "\n",
    "**Scalability**: BHIVE indexes can scale to billions of vectors with optimized concurrent operations, making them suitable for large-scale production deployments.\n",
    "\n",
    "For detailed configuration options, see the [Quantization & Centroid Settings](https://docs.couchbase.com/cloud/vector-index/hyperscale-vector-index.html#algo_settings).\n",
    "\n",
    "For more information on GSI vector indexes, see [Couchbase GSI Vector Documentation](https://docs.couchbase.com/cloud/vector-index/use-vector-indexes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e409d9a",
   "metadata": {},
   "source": [
    "### Vector Search Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e693479",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_vector_search_performance(vector_store, query, label=\"Vector Search\"):\n",
    "    \"\"\"Test pure vector search performance and return timing metrics\"\"\"\n",
    "    print(f\"\\n[{label}] Testing vector search performance\")\n",
    "    print(f\"[{label}] Query: '{query}'\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        results = vector_store.similarity_search_with_score(query, k=3)\n",
    "        end_time = time.time()\n",
    "        search_time = end_time - start_time\n",
    "        \n",
    "        print(f\"[{label}] Vector search completed in {search_time:.4f} seconds\")\n",
    "        print(f\"[{label}] Found {len(results)} documents\")\n",
    "        \n",
    "        if results:\n",
    "            doc, distance = results[0]\n",
    "            print(f\"[{label}] Top result distance: {distance:.6f} (lower = more similar)\")\n",
    "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
    "            print(f\"[{label}] Top result preview: {preview}\")\n",
    "        \n",
    "        return search_time\n",
    "    except Exception as e:\n",
    "        print(f\"[{label}] Vector search failed: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc751f3c",
   "metadata": {},
   "source": [
    "### Test 1: Baseline Performance (No GSI Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e875b60",
   "metadata": {},
   "source": [
    "Test pure vector search performance without GSI optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e02fd5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing baseline vector search performance without GSI optimization...\n",
      "\n",
      "[Baseline Search] Testing vector search performance\n",
      "[Baseline Search] Query: 'What was manchester city manager pep guardiola's reaction to the team's current form?'\n",
      "[Baseline Search] Vector search completed in 0.8305 seconds\n",
      "[Baseline Search] Found 3 documents\n",
      "[Baseline Search] Top result distance: 0.457932 (lower = more similar)\n",
      "[Baseline Search] Top result preview: 'Promised change, but Juventus are back in crisis'\n",
      "\n",
      "\"We have entirely changed the way we think about...\n",
      "\n",
      "Baseline vector search time (without GSI): 0.8305 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test baseline vector search performance without GSI index\n",
    "test_query = \"What was manchester city manager pep guardiola's reaction to the team's current form?\"\n",
    "print(\"Testing baseline vector search performance without GSI optimization...\")\n",
    "baseline_time = test_vector_search_performance(vector_store, test_query, \"Baseline Search\")\n",
    "print(f\"\\nBaseline vector search time (without GSI): {baseline_time:.4f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc252f",
   "metadata": {},
   "source": [
    "### Create BHIVE GSI Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f3cb6",
   "metadata": {},
   "source": [
    "Now let's create a BHIVE GSI vector index to enable high-performance vector searches. The index creation is done programmatically through the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f67fd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BHIVE GSI vector index...\n",
      "GSI Vector index created successfully\n",
      "Waiting for index to become available...\n"
     ]
    }
   ],
   "source": [
    "# Create GSI Vector Index for high-performance searches\n",
    "print(\"Creating BHIVE GSI vector index...\")\n",
    "try:\n",
    "    vector_store.create_index(\n",
    "        index_type=IndexType.BHIVE, # Use IndexType.COMPOSITE for Composite index\n",
    "        index_description=\"IVF,SQ8\"\n",
    "    )\n",
    "    print(\"GSI Vector index created successfully\")\n",
    "    \n",
    "    # Wait for index to become available\n",
    "    print(\"Waiting for index to become available...\")\n",
    "    time.sleep(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(\"GSI Vector index already exists, proceeding...\")\n",
    "    else:\n",
    "        print(f\"Error creating GSI index: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896a83b",
   "metadata": {},
   "source": [
    "### Alternative: Composite Index Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e992ab",
   "metadata": {},
   "source": [
    "If your use case requires complex filtering with scalar attributes, you can create a **Composite index** instead by changing the configuration above:\n",
    "\n",
    "```python\n",
    "# Alternative: Create a Composite index for filtered searches\n",
    "vector_store.create_index(\n",
    "    index_type=IndexType.COMPOSITE,  # Instead of IndexType.BHIVE\n",
    "    index_description=\"IVF,SQ8\"      # Same quantization settings\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e24394",
   "metadata": {},
   "source": [
    "### Test 2: Vector Search-Optimized Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8032cb",
   "metadata": {},
   "source": [
    "Test the same vector search with BHIVE GSI optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39f734a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing vector search performance with BHIVE GSI optimization...\n",
      "\n",
      "[Vector Search-Optimized Search] Testing vector search performance\n",
      "[Vector Search-Optimized Search] Query: 'What happened in the latest Premier League matches?'\n",
      "[Vector Search-Optimized Search] Vector search completed in 0.6452 seconds\n",
      "[Vector Search-Optimized Search] Found 3 documents\n",
      "[Vector Search-Optimized Search] Top result distance: 0.394714 (lower = more similar)\n",
      "[Vector Search-Optimized Search] Top result preview: The latest updates and analysis from the BBC.\n"
     ]
    }
   ],
   "source": [
    "# Test vector search performance with GSI index\n",
    "gsi_test_query = \"What happened in the latest Premier League matches?\"\n",
    "print(\"Testing vector search performance with BHIVE GSI optimization...\")\n",
    "gsi_time = test_vector_search_performance(vector_store, gsi_test_query, \"Vector Search-Optimized Search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5db689",
   "metadata": {},
   "source": [
    "### Test 3: Cache Benefits Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5878b5fe",
   "metadata": {},
   "source": [
    "Now let's demonstrate how caching can improve performance for repeated queries. **Note**: Caching benefits apply to both baseline and GSI-optimized searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23748700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Couchbase cache for improved performance on repeated queries...\n",
      "\u2713 Couchbase cache enabled!\n"
     ]
    }
   ],
   "source": [
    "# Set up Couchbase cache (can be applied to any search approach)\n",
    "print(\"Setting up Couchbase cache for improved performance on repeated queries...\")\n",
    "cache = CouchbaseCache(\n",
    "    cluster=cluster,\n",
    "    bucket_name=CB_BUCKET_NAME,\n",
    "    scope_name=SCOPE_NAME,\n",
    "    collection_name=COLLECTION_NAME,\n",
    ")\n",
    "set_llm_cache(cache)\n",
    "print(\"\u2713 Couchbase cache enabled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03d05d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing cache benefits with vector search...\n",
      "First execution (cache miss):\n",
      "\n",
      "[Cache Test - First Run] Testing vector search performance\n",
      "[Cache Test - First Run] Query: 'What are the latest football transfer developments?'\n",
      "[Cache Test - First Run] Vector search completed in 0.9695 seconds\n",
      "[Cache Test - First Run] Found 3 documents\n",
      "[Cache Test - First Run] Top result distance: 0.394020 (lower = more similar)\n",
      "[Cache Test - First Run] Top result preview: The latest updates and analysis from the BBC.\n",
      "\n",
      "Second execution (cache hit - should be faster):\n",
      "\n",
      "[Cache Test - Second Run] Testing vector search performance\n",
      "[Cache Test - Second Run] Query: 'What are the latest football transfer developments?'\n",
      "[Cache Test - Second Run] Vector search completed in 0.5252 seconds\n",
      "[Cache Test - Second Run] Found 3 documents\n",
      "[Cache Test - Second Run] Top result distance: 0.394020 (lower = more similar)\n",
      "[Cache Test - Second Run] Top result preview: The latest updates and analysis from the BBC.\n"
     ]
    }
   ],
   "source": [
    "# Test cache benefits with a different query to avoid interference\n",
    "cache_test_query = \"What are the latest football transfer developments?\"\n",
    "\n",
    "print(\"Testing cache benefits with vector search...\")\n",
    "print(\"First execution (cache miss):\")\n",
    "cache_time_1 = test_vector_search_performance(vector_store, cache_test_query, \"Cache Test - First Run\")\n",
    "\n",
    "print(\"\\nSecond execution (cache hit - should be faster):\")\n",
    "cache_time_2 = test_vector_search_performance(vector_store, cache_test_query, \"Cache Test - Second Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50d311",
   "metadata": {},
   "source": [
    "### Vector Search Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2aca1b",
   "metadata": {},
   "source": [
    "Let's analyze the vector search performance improvements across all optimization levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e87794a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VECTOR SEARCH PERFORMANCE OPTIMIZATION SUMMARY\n",
      "================================================================================\n",
      "Phase 1 - Baseline Search (No GSI):     0.8305 seconds\n",
      "Phase 2 - Vector Search-Optimized Search:         0.6452 seconds\n",
      "Phase 3 - Cache Benefits:\n",
      "  First execution (cache miss):         0.9695 seconds\n",
      "  Second execution (cache hit):         0.5252 seconds\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "VECTOR SEARCH OPTIMIZATION IMPACT:\n",
      "--------------------------------------------------------------------------------\n",
      "GSI Index Benefit:      1.29x faster (22.3% improvement)\n",
      "Cache Benefit:          1.85x faster (45.8% improvement)\n",
      "\n",
      "Key Insights for Vector Search Performance:\n",
      "\u2022 GSI BHIVE indexes provide significant performance improvements for vector similarity search\n",
      "\u2022 Performance gains are most dramatic for complex semantic queries\n",
      "\u2022 BHIVE optimization is particularly effective for high-dimensional embeddings\n",
      "\u2022 Combined with proper quantization (SQ8), GSI delivers production-ready performance\n",
      "\u2022 These performance improvements directly benefit any application using the vector store\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VECTOR SEARCH PERFORMANCE OPTIMIZATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Phase 1 - Baseline Search (No GSI):     {baseline_time:.4f} seconds\")\n",
    "print(f\"Phase 2 - Vector Search-Optimized Search:         {gsi_time:.4f} seconds\")\n",
    "if cache_time_1 and cache_time_2:\n",
    "    print(f\"Phase 3 - Cache Benefits:\")\n",
    "    print(f\"  First execution (cache miss):         {cache_time_1:.4f} seconds\")\n",
    "    print(f\"  Second execution (cache hit):         {cache_time_2:.4f} seconds\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"VECTOR SEARCH OPTIMIZATION IMPACT:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# GSI improvement analysis\n",
    "if baseline_time and gsi_time:\n",
    "    speedup = baseline_time / gsi_time if gsi_time > 0 else float('inf')\n",
    "    time_saved = baseline_time - gsi_time\n",
    "    percent_improvement = (time_saved / baseline_time) * 100\n",
    "    print(f\"GSI Index Benefit:      {speedup:.2f}x faster ({percent_improvement:.1f}% improvement)\")\n",
    "\n",
    "# Cache improvement analysis\n",
    "if cache_time_1 and cache_time_2 and cache_time_2 < cache_time_1:\n",
    "    cache_speedup = cache_time_1 / cache_time_2\n",
    "    cache_improvement = ((cache_time_1 - cache_time_2) / cache_time_1) * 100\n",
    "    print(f\"Cache Benefit:          {cache_speedup:.2f}x faster ({cache_improvement:.1f}% improvement)\")\n",
    "else:\n",
    "    print(f\"Cache Benefit:          Variable (depends on query complexity and caching mechanism)\")\n",
    "\n",
    "print(f\"\\nKey Insights for Vector Search Performance:\")\n",
    "print(f\"\u2022 GSI BHIVE indexes provide significant performance improvements for vector similarity search\")\n",
    "print(f\"\u2022 Performance gains are most dramatic for complex semantic queries\")\n",
    "print(f\"\u2022 BHIVE optimization is particularly effective for high-dimensional embeddings\")\n",
    "print(f\"\u2022 Combined with proper quantization (SQ8), GSI delivers production-ready performance\")\n",
    "print(f\"\u2022 These performance improvements directly benefit any application using the vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcc39f3",
   "metadata": {},
   "source": [
    "## Jina AI RAG Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae31907",
   "metadata": {},
   "source": [
    "### What is RAG (Retrieval-Augmented Generation)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d50d7a",
   "metadata": {},
   "source": [
    "Now that we've optimized our vector search performance, let's demonstrate how to build a complete RAG system using Jina AI. RAG combines the power of our GSI-optimized semantic search with language model generation:\n",
    "\n",
    "1. **Query Processing**: User question is converted to vector embedding using Jina AI\n",
    "2. **Document Retrieval**: GSI BHIVE index finds most relevant documents (now with proven performance improvements)\n",
    "3. **Context Assembly**: Retrieved documents provide factual context for the language model\n",
    "4. **Response Generation**: Jina's language model generates intelligent answers grounded in the retrieved data\n",
    "\n",
    "This demo shows how the vector search performance improvements we validated directly enhance the RAG workflow efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c68d0",
   "metadata": {},
   "source": [
    "### Create Jina Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e582ba",
   "metadata": {},
   "source": [
    "Initialize Jina's chat model for generating intelligent responses based on our GSI-optimized retrieval system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9477c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 11:24:30,099 - INFO - Successfully created JinaChat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Jina AI language model for RAG demo...\n",
      "\u2713 JinaChat language model created successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up Jina AI language model for RAG demo...\")\n",
    "try:\n",
    "    llm = JinaChat(temperature=0.1, jinachat_api_key=JINACHAT_API_KEY)\n",
    "    print(\"\u2713 JinaChat language model created successfully\")\n",
    "    logging.info(\"Successfully created JinaChat\")\n",
    "except Exception as e:\n",
    "    print(f\"\u2717 Error creating JinaChat: {str(e)}\")\n",
    "    print(\"Please check your JINACHAT_API_KEY and network connection.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bdd6d1",
   "metadata": {},
   "source": [
    "### Build Optimized RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2f73b9",
   "metadata": {},
   "source": [
    "Create the complete RAG pipeline that integrates our GSI-optimized vector search with Jina's language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44af7a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized RAG pipeline created successfully\n",
      "Components: GSI BHIVE Vector Search \u2192 Context Assembly \u2192 Jina Language Model \u2192 Response\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Create RAG prompt template for structured responses\n",
    "    template = \"\"\"You are a helpful assistant that answers questions based on the provided context. \n",
    "    If you cannot answer based on the context provided, respond with a generic answer. \n",
    "    Answer the question as truthfully as possible using the context below:\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # Build the RAG chain: Vector Search-Optimized Retrieval \u2192 Context \u2192 Generation \u2192 Output\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": vector_store.as_retriever(search_kwargs={\"k\": 2}), \n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    print(\"Optimized RAG pipeline created successfully\")\n",
    "    print(\"Components: GSI BHIVE Vector Search \u2192 Context Assembly \u2192 Jina Language Model \u2192 Response\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating RAG pipeline: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a335ef",
   "metadata": {},
   "source": [
    "### RAG Demo with Optimized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25dccc",
   "metadata": {},
   "source": [
    "Test the complete RAG system leveraging our GSI performance optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "915cd261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG System with Vector Search-Optimized Vector Search\n",
      "============================================================\n",
      "User Query: What are the new eligibility rules for transgender women competing in leading women's golf tours, and what prompted these changes?\n",
      "\n",
      "Processing with optimized pipeline...\n",
      "1. Converting query to vector embedding with Jina AI\n",
      "2. Searching GSI BHIVE index for relevant documents (optimized)\n",
      "3. Assembling context from retrieved documents\n",
      "4. Generating intelligent response with JinaChat\n",
      "\n",
      "RAG Response (completed in 4.25 seconds):\n",
      "------------------------------------------------------------\n",
      "The new eligibility rules for transgender women competing in leading women's golf tours starting from 2025 prevent transgender women who have gone through male puberty from participating. Female players protesting led to these changes, as they called for policies to prevent those recorded as male at birth from competing in women's events.\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing RAG System with Vector Search-Optimized Vector Search\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Test with a specific query\n",
    "    sample_query = \"What are the new eligibility rules for transgender women competing in leading women's golf tours, and what prompted these changes?\"\n",
    "    print(f\"User Query: {sample_query}\")\n",
    "    print(\"\\nProcessing with optimized pipeline...\")\n",
    "    print(\"1. Converting query to vector embedding with Jina AI\")\n",
    "    print(\"2. Searching GSI BHIVE index for relevant documents (optimized)\")\n",
    "    print(\"3. Assembling context from retrieved documents\")\n",
    "    print(\"4. Generating intelligent response with JinaChat\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    rag_response = rag_chain.invoke(sample_query)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"\\nRAG Response (completed in {end_time - start_time:.2f} seconds):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(rag_response)\n",
    "    \n",
    "except Exception as e:\n",
    "    if \"Payment Required\" in str(e):\n",
    "        print(\"\\nPayment required for Jina AI API.\")\n",
    "        print(\"To resolve:\")\n",
    "        print(\"\u2022 Visit https://jina.ai/reader/#pricing for subscription options\")\n",
    "        print(\"\u2022 Ensure your API key is valid and has sufficient credits\")\n",
    "    else:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b326eb1f",
   "metadata": {},
   "source": [
    "### Multiple Query RAG Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c3168",
   "metadata": {},
   "source": [
    "Test the RAG system with various queries to demonstrate the benefits of our optimized vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd59a9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Optimized RAG System with Multiple Queries\n",
      "=======================================================\n",
      "\n",
      "--- RAG Query 1 ---\n",
      "Question: What happened in the car incident on Shaftesbury Avenue in London?\n",
      "Response (completed in 3.32 seconds): ### Answer:\n",
      "A 31-year-old man was arrested on suspicion of attempted murder after driving a car on the wrong side of the road in Shaftesbury Avenue, London, injuring four pedestrians. The incident was treated as an isolated incident and was not terror-related.\n",
      "\n",
      "--- RAG Query 2 ---\n",
      "Question: What did King Charles talk about in his recent Christmas speech?\n",
      "Response (completed in 0.74 seconds): ### King Charles's Recent Christmas Speech Highlights:\n",
      "\n",
      "- Visited a Christmas market at Battersea Power Station.\n",
      "- Met with Apple chief Tim Cook at Apple's UK headquarters.\n",
      "- Interacted with carol singers, Christmas shoppers, and stallholders.\n",
      "- Explored the power station and visited stalls at the Curated Makers Market.\n",
      "\n",
      "\u2705 RAG demo completed successfully!\n",
      "\u2705 The system leverages GSI BHIVE optimization for fast document retrieval!\n",
      "\u2705 Jina AI provides high-quality embeddings and intelligent response generation!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting Optimized RAG System with Multiple Queries\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "try:\n",
    "    test_queries = [\n",
    "        \"What happened in the car incident on Shaftesbury Avenue in London?\",\n",
    "        \"What did King Charles talk about in his recent Christmas speech?\",\n",
    "    ]\n",
    "\n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n--- RAG Query {i} ---\")\n",
    "        print(f\"Question: {query}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = rag_chain.invoke(query)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"Response (completed in {end_time - start_time:.2f} seconds): {response}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    if \"Payment Required\" in str(e):\n",
    "        print(\"Payment required for Jina AI API.\")\n",
    "    else:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "print(f\"\\n\u2705 RAG demo completed successfully!\")\n",
    "print(\"\u2705 The system leverages GSI BHIVE optimization for fast document retrieval!\")\n",
    "print(\"\u2705 Jina AI provides high-quality embeddings and intelligent response generation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627d6056",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd98a38f",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "source": [
    "You've successfully built a high-performance semantic search engine combining:\n",
    "- **Couchbase GSI BHIVE indexes** for optimized vector search\n",
    "- **Jina AI embeddings and language models** for intelligent processing\n",
    "- **Complete RAG pipeline** with caching optimization"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}