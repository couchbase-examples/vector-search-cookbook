{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this guide, we will walk you through building a powerful semantic search engine using Couchbase as the backend database and [Amazon Bedrock](https://aws.amazon.com/bedrock/) as both the embedding and language model provider. Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval. This tutorial is designed to be beginner-friendly, with clear, step-by-step instructions that will equip you with the knowledge to create a fully functional semantic search system from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run this tutorial\n",
    "\n",
    "This tutorial is available as a Jupyter Notebook (`.ipynb` file) that you can run interactively. You can access the original notebook [here](https://github.com/couchbase-examples/vector-search-cookbook/blob/main/awsbedrock/RAG_with_Couchbase_and_Bedrock.ipynb).\n",
    "\n",
    "You can either download the notebook file and run it on [Google Colab](https://colab.research.google.com/) or run it on your system by setting up the Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you start\n",
    "\n",
    "## Get Credentials for AWS Bedrock\n",
    "* Please follow the [instructions](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html) to set up AWS Bedrock and generate credentials.\n",
    "* Ensure you have the necessary IAM permissions to access Bedrock services.\n",
    "\n",
    "## Create and Deploy Your Free Tier Operational cluster on Capella\n",
    "\n",
    "To get started with Couchbase Capella, create an account and use it to deploy a forever free tier operational cluster. This account provides you with an environment where you can explore and learn about Capella with no time constraint.\n",
    "\n",
    "To know more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html).\n",
    "\n",
    "### Couchbase Capella Configuration\n",
    "\n",
    "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met.\n",
    "\n",
    "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the travel-sample bucket (Read and Write) used in the application.\n",
    "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Stage: Installing Necessary Libraries\n",
    "\n",
    "To build our semantic search engine, we need a robust set of tools. The libraries we install handle everything from connecting to databases to performing complex machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet datasets langchain-couchbase langchain-aws boto3 python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries\n",
    "\n",
    "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import time\n",
    "import getpass\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.exceptions import CouchbaseException, InternalServerFailureException, QueryIndexAlreadyExistsException\n",
    "from couchbase.management.search import SearchIndex\n",
    "from couchbase.options import ClusterOptions\n",
    "from datasets import load_dataset\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_couchbase.cache import CouchbaseCache\n",
    "from langchain_couchbase.vectorstores import CouchbaseVectorStore\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Logging\n",
    "\n",
    "Logging is configured to track the progress of the script and capture any errors or warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Sensitive Informnation\n",
    "In this section, we prompt the user to input essential configuration settings needed. These settings include sensitive information like AWS credentials, database credentials, and specific configuration names. Instead of hardcoding these details into the script, we request the user to provide them at runtime, ensuring flexibility and security.\n",
    "\n",
    "The script also validates that all required inputs are provided, raising an error if any crucial information is missing. This approach ensures that your integration is both secure and correctly configured without hardcoding sensitive information, enhancing the overall security and maintainability of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# AWS Credentials\n",
    "AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID') or input('Enter your AWS Access Key ID: ')\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY') or getpass.getpass('Enter your AWS Secret Access Key: ')\n",
    "AWS_REGION = os.getenv('AWS_REGION') or input('Enter your AWS region (default: us-east-1): ') or 'us-east-1'\n",
    "\n",
    "# Couchbase Settings\n",
    "CB_HOST = os.getenv('CB_HOST') or input('Enter your Couchbase host (default: couchbase://localhost): ') or 'couchbase://localhost'\n",
    "CB_USERNAME = os.getenv('CB_USERNAME') or input('Enter your Couchbase username (default: Administrator): ') or 'Administrator'\n",
    "CB_PASSWORD = os.getenv('CB_PASSWORD') or getpass.getpass('Enter your Couchbase password (default: password): ') or 'password'\n",
    "CB_BUCKET_NAME = os.getenv('CB_BUCKET_NAME') or input('Enter your Couchbase bucket name (default: vector-search-testing): ') or 'vector-search-testing'\n",
    "INDEX_NAME = os.getenv('INDEX_NAME') or input('Enter your index name (default: vector_search_bedrock): ') or 'vector_search_bedrock'\n",
    "SCOPE_NAME = os.getenv('SCOPE_NAME') or input('Enter your scope name (default: shared): ') or 'shared'\n",
    "COLLECTION_NAME = os.getenv('COLLECTION_NAME') or input('Enter your collection name (default: bedrock): ') or 'bedrock'\n",
    "CACHE_COLLECTION = os.getenv('CACHE_COLLECTION') or input('Enter your cache collection name (default: cache): ') or 'cache'\n",
    "\n",
    "# Check if required credentials are set\n",
    "for cred_name, cred_value in {\n",
    "    'AWS_ACCESS_KEY_ID': AWS_ACCESS_KEY_ID,\n",
    "    'AWS_SECRET_ACCESS_KEY': AWS_SECRET_ACCESS_KEY, \n",
    "    'CB_HOST': CB_HOST,\n",
    "    'CB_USERNAME': CB_USERNAME,\n",
    "    'CB_PASSWORD': CB_PASSWORD,\n",
    "    'CB_BUCKET_NAME': CB_BUCKET_NAME\n",
    "}.items():\n",
    "    if not cred_value:\n",
    "        raise ValueError(f\"{cred_name} is not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the Couchbase Cluster\n",
    "Connecting to a Couchbase cluster is the foundation of our project. Couchbase will serve as our primary data store, handling all the storage and retrieval operations required for our semantic search engine. By establishing this connection, we enable our application to interact with the database, allowing us to perform operations such as storing embeddings, querying data, and managing collections. This connection is the gateway through which all data will flow, so ensuring it's set up correctly is paramount.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 03:27:02,702 - INFO - Successfully connected to Couchbase\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "    options = ClusterOptions(auth)\n",
    "    cluster = Cluster(CB_HOST, options)\n",
    "    cluster.wait_until_ready(timedelta(seconds=5))\n",
    "    logging.info(\"Successfully connected to Couchbase\")\n",
    "except Exception as e:\n",
    "    raise ConnectionError(f\"Failed to connect to Couchbase: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Collections in Couchbase\n",
    "In Couchbase, data is organized in buckets, which can be further divided into scopes and collections. Think of a collection as a table in a traditional SQL database. Before we can store any data, we need to ensure that our collections exist. If they don't, we must create them. This step is important because it prepares the database to handle the specific types of data our application will process. By setting up collections, we define the structure of our data storage, which is essential for efficient data retrieval and management.\n",
    "\n",
    "Moreover, setting up collections allows us to isolate different types of data within the same bucket, providing a more organized and scalable data structure. This is particularly useful when dealing with large datasets, as it ensures that related data is stored together, making it easier to manage and query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 03:27:05,221 - INFO - Collection 'bedrock' already exists. Skipping creation.\n",
      "2025-01-13 03:27:06,388 - INFO - Primary index present or created successfully.\n",
      "2025-01-13 03:27:07,485 - INFO - All documents cleared from the collection.\n",
      "2025-01-13 03:27:08,669 - INFO - Collection 'cache' already exists. Skipping creation.\n",
      "2025-01-13 03:27:09,906 - INFO - Primary index present or created successfully.\n",
      "2025-01-13 03:27:10,204 - INFO - All documents cleared from the collection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<couchbase.collection.Collection at 0x1767ec090>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup_collection(cluster, bucket_name, scope_name, collection_name):\n",
    "    try:\n",
    "        bucket = cluster.bucket(bucket_name)\n",
    "        bucket_manager = bucket.collections()\n",
    "\n",
    "        # Check if collection exists, create if it doesn't\n",
    "        collections = bucket_manager.get_all_scopes()\n",
    "        collection_exists = any(\n",
    "            scope.name == scope_name and collection_name in [col.name for col in scope.collections]\n",
    "            for scope in collections\n",
    "        )\n",
    "\n",
    "        if not collection_exists:\n",
    "            logging.info(f\"Collection '{collection_name}' does not exist. Creating it...\")\n",
    "            bucket_manager.create_collection(scope_name, collection_name)\n",
    "            logging.info(f\"Collection '{collection_name}' created successfully.\")\n",
    "        else:\n",
    "            logging.info(f\"Collection '{collection_name}' already exists. Skipping creation.\")\n",
    "\n",
    "        collection = bucket.scope(scope_name).collection(collection_name)\n",
    "\n",
    "        # Ensure primary index exists\n",
    "        try:\n",
    "            cluster.query(f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{bucket_name}`.`{scope_name}`.`{collection_name}`\").execute()\n",
    "            logging.info(\"Primary index present or created successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error creating primary index: {str(e)}\")\n",
    "\n",
    "        # Clear all documents in the collection\n",
    "        try:\n",
    "            query = f\"DELETE FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
    "            cluster.query(query).execute()\n",
    "            logging.info(\"All documents cleared from the collection.\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error while clearing documents: {str(e)}. The collection might be empty.\")\n",
    "\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error setting up collection: {str(e)}\")\n",
    "\n",
    "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)\n",
    "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, CACHE_COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Couchbase Vector Search Index\n",
    "\n",
    "Semantic search requires an efficient way to retrieve relevant documents based on a user's query. This is where the Couchbase **Vector Search Index** comes into play. In this step, we load the Vector Search Index definition from a JSON file, which specifies how the index should be structured. This includes the fields to be indexed, the dimensions of the vectors, and other parameters that determine how the search engine processes queries based on vector similarity.\n",
    "\n",
    "This AWS Bedrock vector search index configuration requires specific default settings to function properly. This tutorial uses the bucket named `vector-search-testing` with the scope `shared` and collection `bedrock`. The configuration is set up for vectors with exactly 1024 dimensions, using dot product similarity and optimized for recall. If you want to use a different bucket, scope, or collection, you will need to modify the index configuration accordingly.\n",
    "\n",
    "For more information on creating a vector search index, please follow the [instructions](https://docs.couchbase.com/cloud/vector-search/create-vector-search-index-ui.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('aws_index.json', 'r') as file:\n",
    "        index_definition = json.load(file)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading index definition: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating or Updating Search Indexes\n",
    "\n",
    "With the index definition loaded, the next step is to create or update the **Vector Search Index** in Couchbase. This step is crucial because it optimizes our database for vector similarity search operations, allowing us to perform searches based on the semantic content of documents rather than just keywords. By creating or updating a Vector Search Index, we enable our search engine to handle complex queries that involve finding semantically similar documents using vector embeddings, which is essential for a robust semantic search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 03:27:11,478 - INFO - Index 'vector_search_bedrock' found\n",
      "2025-01-13 03:27:12,323 - INFO - Index 'vector_search_bedrock' already exists. Skipping creation/update.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    scope_index_manager = cluster.bucket(CB_BUCKET_NAME).scope(SCOPE_NAME).search_indexes()\n",
    "\n",
    "    # Check if index already exists\n",
    "    existing_indexes = scope_index_manager.get_all_indexes()\n",
    "    index_name = index_definition[\"name\"]\n",
    "\n",
    "    if index_name in [index.name for index in existing_indexes]:\n",
    "        logging.info(f\"Index '{index_name}' found\")\n",
    "    else:\n",
    "        logging.info(f\"Creating new index '{index_name}'...\")\n",
    "\n",
    "    # Create SearchIndex object from JSON definition\n",
    "    search_index = SearchIndex.from_json(index_definition)\n",
    "\n",
    "    # Upsert the index (create if not exists, update if exists)\n",
    "    scope_index_manager.upsert_index(search_index)\n",
    "    logging.info(f\"Index '{index_name}' successfully created/updated.\")\n",
    "\n",
    "except QueryIndexAlreadyExistsException:\n",
    "    logging.info(f\"Index '{index_name}' already exists. Skipping creation/update.\")\n",
    "\n",
    "except InternalServerFailureException as e:\n",
    "    error_message = str(e)\n",
    "    logging.error(f\"InternalServerFailureException raised: {error_message}\")\n",
    "\n",
    "    try:\n",
    "        # Accessing the response_body attribute from the context\n",
    "        error_context = e.context\n",
    "        response_body = error_context.response_body\n",
    "        if response_body:\n",
    "            error_details = json.loads(response_body)\n",
    "            error_message = error_details.get('error', '')\n",
    "\n",
    "            if \"collection: 'bedrock' doesn't belong to scope: 'shared'\" in error_message:\n",
    "                raise ValueError(\"Collection 'bedrock' does not belong to scope 'shared'. Please check the collection and scope names.\")\n",
    "\n",
    "    except ValueError as ve:\n",
    "        logging.error(str(ve))\n",
    "        raise\n",
    "\n",
    "    except Exception as json_error:\n",
    "        logging.error(f\"Failed to parse the error message: {json_error}\")\n",
    "        raise RuntimeError(f\"Internal server error while creating/updating search index: {error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Amazon Bedrock Client and Embeddings\n",
    "\n",
    "Embeddings are at the heart of semantic search. They are numerical representations of text that capture the semantic meaning of the words and phrases. We'll use Amazon Bedrock's Titan embedding model for embeddings.\n",
    "\n",
    "## Using Amazon Bedrock's Titan Model\n",
    "\n",
    "Language models are AI systems that are trained to understand and generate human language. We'll be using Amazon Bedrock's Titan model to process user queries and generate meaningful responses. The Titan model family includes both embedding models for converting text into vector representations and text generation models for producing human-like responses.\n",
    "\n",
    "Key features of Amazon Bedrock's Titan models:\n",
    "- Titan Embeddings model for embedding vector generation\n",
    "- Titan Text model for natural language understanding and generation\n",
    "- Seamless integration with AWS infrastructure\n",
    "- Enterprise-grade security and scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 03:27:12,555 - INFO - Successfully created Bedrock embeddings client\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bedrock_client = boto3.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name=AWS_REGION,\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
    "    )\n",
    "    \n",
    "    embeddings = BedrockEmbeddings(\n",
    "        client=bedrock_client,\n",
    "        model_id=\"amazon.titan-embed-text-v2:0\"\n",
    "    )\n",
    "    logging.info(\"Successfully created Bedrock embeddings client\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating Bedrock embeddings client: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Setting Up the Couchbase Vector Store\n",
    "A vector store is where we'll keep our embeddings. Unlike the FTS index, which is used for text-based search, the vector store is specifically designed to handle embeddings and perform similarity searches. When a user inputs a query, the search engine converts the query into an embedding and compares it against the embeddings stored in the vector store. This allows the engine to find documents that are semantically similar to the query, even if they don't contain the exact same words. By setting up the vector store in Couchbase, we create a powerful tool that enables our search engine to understand and retrieve information based on the meaning and context of the query, rather than just the specific words used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 03:27:16,604 - INFO - Successfully created vector store\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vector_store = CouchbaseVectorStore(\n",
    "        cluster=cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=embeddings,\n",
    "        index_name=INDEX_NAME,\n",
    "    )\n",
    "    logging.info(\"Successfully created vector store\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to create vector store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the BBC News Dataset\n",
    "To build a search engine, we need data to search through. We use the BBC News dataset from RealTimeData, which provides real-world news articles. This dataset contains news articles from BBC covering various topics and time periods. Loading the dataset is a crucial step because it provides the raw material that our search engine will work with. The quality and diversity of the news articles make it an excellent choice for testing and refining our search engine, ensuring it can handle real-world news content effectively.\n",
    "\n",
    "The BBC News dataset allows us to work with authentic news articles, enabling us to build and test a search engine that can effectively process and retrieve relevant news content. The dataset is loaded using the Hugging Face datasets library, specifically accessing the \"RealTimeData/bbc_news_alltime\" dataset with the \"2024-12\" version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 03:27:22,973 - INFO - Successfully loaded the BBC News dataset with 2687 rows.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the BBC News dataset with 2687 rows\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    news_dataset = load_dataset(\n",
    "        \"RealTimeData/bbc_news_alltime\", \"2024-12\", split=\"train\"\n",
    "    )\n",
    "    print(f\"Loaded the BBC News dataset with {len(news_dataset)} rows\")\n",
    "    logging.info(f\"Successfully loaded the BBC News dataset with {len(news_dataset)} rows.\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading the BBC News dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Processing and Loading Data in Batches\n",
    " The code below processes the BBC News dataset in batches to efficiently load it into our vector store. Here's a detailed breakdown:\n",
    "\n",
    " Batch Processing (50 articles per batch):\n",
    " 1. Takes a portion of the total dataset for processing since loading all articles at once may consume excessive memory\n",
    " 2. Tracks seen article links in a set to prevent duplicates\n",
    " 3. For each batch:\n",
    "    - Creates metadata dictionaries with article details (title, date, section, link)\n",
    "    - Filters out invalid content:\n",
    "      * Removes empty or whitespace-only articles\n",
    "      * Skips duplicate articles based on URL\n",
    "    - Generates unique IDs like \"article_1\", \"article_2\" etc.\n",
    "    - Adds valid articles to vector store with their metadata\n",
    "\n",
    " Key Features:\n",
    " - Duplicate Prevention: Uses seen_links set to ensure each article is only processed once\n",
    " - Content Validation: Checks for empty/whitespace content before processing\n",
    " - Progress Tracking: Shows completion percentage during processing\n",
    " - Memory Efficiency: Processes in small batches to avoid memory issues\n",
    " - Error Handling: Wraps operations in try-except for graceful error handling\n",
    "\n",
    " The code uses vector_store.add_texts() to store both the article content and metadata,\n",
    " enabling semantic search capabilities while preserving article metadata for results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100% (1881/1881)\n",
      "Completed loading all documents to vector store!\n",
      "Total unique articles processed: 1281\n"
     ]
    }
   ],
   "source": [
    "# Suppress all logs from specific loggers\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "logging.getLogger('openai').setLevel(logging.WARNING)\n",
    "logging.getLogger('httpx').setLevel(logging.WARNING)\n",
    "\n",
    "# Process in batches with metadata\n",
    "try:\n",
    "    batch_size = 50\n",
    "    total_rows = round(len(news_dataset) * 0.7) # Process 70% of the dataset\n",
    "    seen_links = set()  # Track seen article links to avoid duplicates\n",
    "    \n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        batch = news_dataset[i:min(i + batch_size, total_rows)]\n",
    "        \n",
    "        # Create metadata list\n",
    "        metadatas = [\n",
    "            {\n",
    "                'title': title,\n",
    "                'published_date': pub_date,\n",
    "                'section': section,\n",
    "                'link': link\n",
    "            }\n",
    "            for title, pub_date, section, link in zip(\n",
    "                batch['title'], \n",
    "                batch['published_date'], \n",
    "                batch['section'], \n",
    "                batch['link']\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Filter out empty content and duplicates\n",
    "        valid_indices = []\n",
    "        valid_texts = []\n",
    "        valid_metadatas = []\n",
    "        \n",
    "        for idx, (content, metadata) in enumerate(zip(batch['content'], metadatas)):\n",
    "            if content and len(content.strip()) > 0 and metadata['link'] not in seen_links:\n",
    "                valid_indices.append(idx)\n",
    "                valid_texts.append(content)\n",
    "                valid_metadatas.append(metadata)\n",
    "                seen_links.add(metadata['link'])\n",
    "        \n",
    "        if valid_texts:\n",
    "            # Add texts with metadata\n",
    "            vector_store.add_texts(\n",
    "                texts=valid_texts,\n",
    "                metadatas=valid_metadatas,\n",
    "                ids=[f\"article_{i+j}\" for j in valid_indices],\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "        \n",
    "        # Clean progress output\n",
    "        print(f\"Progress: {(i + len(batch['content'])) * 100 // total_rows}% ({i + len(batch['content'])}/{total_rows})\", end='\\r')\n",
    "        \n",
    "    print(\"\\nCompleted loading all documents to vector store!\")\n",
    "    print(f\"Total unique articles processed: {len(seen_links)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to save documents to vector store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up a Couchbase Cache\n",
    "To further optimize our system, we set up a Couchbase-based cache. A cache is a temporary storage layer that holds data that is frequently accessed, speeding up operations by reducing the need to repeatedly retrieve the same information from the database. In our setup, the cache will help us accelerate repetitive tasks, such as looking up similar documents. By implementing a cache, we enhance the overall performance of our search engine, ensuring that it can handle high query volumes and deliver results quickly.\n",
    "\n",
    "Caching is particularly valuable in scenarios where users may submit similar queries multiple times or where certain pieces of information are frequently requested. By storing these in a cache, we can significantly reduce the time it takes to respond to these queries, improving the user experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cache = CouchbaseCache(\n",
    "        cluster=cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=CACHE_COLLECTION,\n",
    "    )\n",
    "    logging.info(\"Successfully created cache\")\n",
    "    set_llm_cache(cache)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to create cache: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Amazon Bedrock's Titan Text Express v1 Model\n",
    "\n",
    "Amazon Bedrock's Titan Text Express v1 is a state-of-the-art foundation model designed for fast and efficient text generation tasks. This model excels at:\n",
    "\n",
    "- Text generation and completion\n",
    "- Question answering \n",
    "- Summarization\n",
    "- Content rewriting\n",
    "- Analysis and extraction\n",
    "\n",
    "Key features of Titan Text Express v1:\n",
    "\n",
    "- Optimized for low-latency responses while maintaining high quality output\n",
    "- Supports up to 8K tokens context window\n",
    "- Built-in content filtering and safety controls\n",
    "- Cost-effective compared to larger models\n",
    "- Seamlessly integrates with AWS services\n",
    "\n",
    "The model uses a temperature parameter (0-1) to control randomness in responses:\n",
    "- Lower values (e.g. 0) produce more focused, deterministic outputs\n",
    "- Higher values introduce more creativity and variation\n",
    "\n",
    "We'll be using this model through Amazon Bedrock's API to process user queries and generate contextually relevant responses based on our vector database content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    llm = ChatBedrock(\n",
    "        client=bedrock_client,\n",
    "        model_id=\"amazon.titan-text-express-v1\",\n",
    "        model_kwargs={\"temperature\": 0}\n",
    "    )\n",
    "    logging.info(\"Successfully created Bedrock LLM client\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error creating Bedrock LLM client: {str(e)}. Please check your AWS credentials and Bedrock access.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Semantic Search\n",
    "Semantic search in Couchbase involves converting queries and documents into vector representations using an embeddings model. These vectors capture the semantic meaning of the text and are stored directly in Couchbase. When a query is made, Couchbase performs a similarity search by comparing the query vector against the stored document vectors. The similarity metric used for this comparison is configurable, allowing flexibility in how the relevance of documents is determined. Common metrics include cosine similarity, Euclidean distance, or dot product, but other metrics can be implemented based on specific use cases. Different embedding models like BERT, Word2Vec, or GloVe can also be used depending on the application's needs, with the vectors generated by these models stored and searched within Couchbase itself.\n",
    "\n",
    "In the provided code, the search process begins by recording the start time, followed by executing the similarity_search_with_score method of the CouchbaseVectorStore. This method searches Couchbase for the most relevant documents based on the vector similarity to the query. The search results include the document content and a similarity score that reflects how closely each document aligns with the query in the defined semantic space. The time taken to perform this search is then calculated and logged, and the results are displayed, showing the most relevant documents along with their similarity scores. This approach leverages Couchbase as both a storage and retrieval engine for vector data, enabling efficient and scalable semantic searches. The integration of vector storage and search capabilities within Couchbase allows for sophisticated semantic search operations without relying on external services for vector storage or comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic Search Results (completed in 1.96 seconds):\n",
      "--------------------------------------------------------------------------------\n",
      "Score: 0.4616, Text: Former Formula 1 boss Bernie Ecclestone is to sell his collection of race cars driven by motorsport legends including Michael Schumacher, Niki Lauda and Nelson Piquet.\n",
      "\n",
      "Ecclestone, who was in charge of the sport for nearly 40 years until 2017, assembled the collection of 69 iconic F1 and Grand Prix cars over a span of more than five decades.\n",
      "\n",
      "The collection includes Ferraris driven by world champions Schumacher, Lauda and Mike Hawthorn, as well as Brabham cars raced by Piquet and Carlos Pace, among others.\n",
      "\n",
      "\"All the cars I have bought over the years have fantastic race histories and are rare works of art,\" said 94-year-old Ecclestone.\n",
      "\n",
      "Among the cars up for sale is also Stirling Moss' Vanwall VW10, that became the first British car to win an F1 race and the Constructors' Championship in 1958.\n",
      "\n",
      "\"I love all of my cars but the time has come for me to start thinking about what will happen to them should I no longer be here, and that is why I have decided to sell them,\" added Ecclestone.\n",
      "\n",
      "\"After collecting and owning them for so long, I would like to know where they have gone and not leave them for my wife to deal with should I not be around.\"\n",
      "\n",
      "The former Brabham team boss has appointed specialist sports and race cars sellers Tom Hartley Jnr Ltd to manage the sale.\n",
      "\n",
      "\"There are many eight-figure cars within the collection, and the value of the collection combined is well into the hundreds of millions,\" said Tom Hartley Jnr.\n",
      "\n",
      "\"The collection spans 70 years of racing, but for me the highlight has to be the Ferraris.\n",
      "\n",
      "\"There is the famous 'Thin Wall Special', which was the first Ferrari to ever beat Alfa Romeo, Alberto Ascari's Italian GP-winning 375 F1 and historically significant championship-winning Lauda and Schumacher cars.\"\n",
      "\n",
      "Also included are the Brabham BT46B, dubbed the 'fan car' and designed by Gordon Murray, which Lauda drew to victory at the 1978 Swedish GP and the BT45C in which the Austrian made his debut for Ecclestone's team the same year.\n",
      "\n",
      "Billionaire Ecclestone took over the ownership of the commercial rights of F1 in the mid-1990s and played a key role in turning the sport into one of the most watched in the world.\n",
      "--------------------------------------------------------------------------------\n",
      "Score: 0.2004, Text: England take control of Test after SA collapse\n",
      "\n",
      "Lauren Filer troubled South Africa with the short ball and finished with 2-53\n",
      "\n",
      "One-off Test (day two of four)\n",
      "\n",
      "England took control of the one-off Test against South Africa as the hosts lost their last seven first-innings wickets for 44 runs in a dramatic evening session. The Proteas' top order battled to reach 237-3 in Bloemfontein, before a delay for lightning after tea coincided with a change of course in the game. England's seamers pounced with the new ball under darkening skies to dismiss South Africa for 281 and take a 114-run lead. That advantage was extended to 145 as Tammy Beaumont and Heather Knight steered England to 31-1 at the close, after first-innings centurion Maia Bouchier fell for a duck. South Africa earlier appeared on course to reach parity through Marizanne Kapp, who made a fluent 57 before she was bowled by debutant Ryana MacDonald-Gay which sparked the collapse as the delay came just five balls later. MacDonald-Gay struck again shortly after the resumption with Nadine de Klerk caught behind for a duck, Lauren Filer ended Sune Luus' resistance for a gritty 56 and Chloe Tryon tamely chipped Lauren Bell to mid-on for 20. Bell finished with 4-49 after mopping up the tail, with the collapse distracting from the disciplined efforts of South Africa's top order throughout the morning and afternoon sessions. Opener Anneke Bosch fell in the second over of the day, but a stand of 92 between Laura Wolvaardt and Annerie Dercksen helped the hosts rebuild until the latter was caught at slip off Filer from the final ball before lunch. The omission of the decision review system (DRS) for the match had a huge impact on the home side as captain Wolvaardt was visibly frustrated to be given out lbw to Sophie Ecclestone for 65, suggesting she had hit the ball. A chanceless partnership of 99 between Kapp and Luus followed but, once broken, a fragile middle to lower order was exposed and gave England hope of a first Test win since 2014.\n",
      "\n",
      "There was some concern that Kate Cross' omission from England's XI because of a back spasm could leave the seam attack short of control, but Bell and Filer stepped up with impressive maturity despite South Africa's gutsy partnerships for the second and fourth wickets. At times, Filer sacrificed accuracy as a result of her raw pace, but that point of difference proved invaluable on a docile pitch. The impressive Dercksen withstood the pressure after England's early breakthrough and dug in alongside her captain, and though both batters were tested by Filer's short ball in her fiery opening spell, they were mostly untroubled. But Knight threw Filer the ball for a short burst before lunch and it produced the desired result, with Dercksen fending off a short ball to slip where Ecclestone took a cracking one-handed rebound catch. While Luus and Kapp's partnership after lunch was solid, South Africa are not blessed with the same batting depth as England and it always felt as if one wicket would change the direction of the Test. MacDonald-Gay took out Kapp's off stump with a beauty as reward for her disciplined spell and then the lower order was blown away by Bell, with the tail backing away as England moved into the ascendancy.\n",
      "\n",
      "Sophie Ecclestone bowled nine maidens in her 25-over spell\n",
      "\n",
      "While the seamers played starring roles, wicketkeeper Amy Jones and left-arm spinner Ecclestone also proved crucial. They ensured that South Africa's top order, despite batting well, were unable to be ruthless in the manner of Bouchier and Nat Sciver-Brunt for England on day one. Ecclestone may have got a touch fortunate with the wicket of Wolvaardt if she did get an edge, but that moment followed a spell of remarkable consistency that gave the batter so few options to score and put the pressure back on. Wolvaardt seemed happy to defend Ecclestone and score from the other end, but she could only defy the spinner's greatness for so long, getting stuck on the crease by one defensive prod too many. The result was a furious Wolvaardt thumping her helmet with her bat as she marched off in disbelief, and while that was Ecclestone's only scalp, she twirled away for 25 overs and conceded just 40, which allowed captain Knight to rotate her seamers regularly and ensure they were all well rested between spells. Meanwhile, Jones' ability to stand up to the medium-pacers set up the prized wicket of Kapp, who also found herself stuck in the crease and unable to manipulate the ball as effectively as she had against Bell and Filer. Kapp became frustrated and lost her off stump, while De Klerk also fell victim to the same trap as she got an edge through to Jones for a sharp catch. Luus had previously dropped anchor as Kapp was more aggressive, but following the wickets she looked uncertain about whether to continue with the approach or try to increase her strike rate when batting with the tail, eventually falling to the new ball by edging Filer behind after a 148-ball stay. The pitch is showing signs of deteriorating and Beaumont and Knight had to negotiate some uneven bounce at the end of the day. England's healthy lead puts them in a commanding position, though a forecast for wet weather on day four could force them to move the game along at a quicker rate.\n",
      "• None Notifications, social media and more with BBC Sport\n",
      "--------------------------------------------------------------------------------\n",
      "Score: 0.1733, Text: Episode 7: Getting away with it\n",
      "\n",
      "Creswell is managing the narrative – his future freedom depends on it.\n",
      "--------------------------------------------------------------------------------\n",
      "Score: 0.1508, Text: Brian Spencer was arrested after police recognised him from the footage\n",
      "\n",
      "A man who featured in a viral video of him being hit in the crotch and head by bricks during the Southport riot has been jailed. Brian Spencer was first hit in the head by a brick thrown by a fellow rioter as he goaded a line of riot police on 30 July. As he held his head, he was then hit by a second brick directly in the crotch, causing him to stagger in the street. The 40-year-old, of Lytham Road, Southport, pleaded guilty at Liverpool Crown to violent disorder and was jailed for two years and six months.\n",
      "\n",
      "The riot broke out the day after three girls were killed in a knife attack in Southport. Merseyside Police said his injuries were captured on mobile phone footage which was shared widely on social media after \"some wayward missiles\" hit him during the \"appalling scenes\" in Southport.\n",
      "\n",
      "A police spokesperson said Spencer \"could be seen acting in an aggressive manner\" as part of a large group of people who were standing in front of police officers and throwing bricks. Spencer was also seen \"punching a police vehicle several times and picking up and throwing wheelie bins\" at officers, police said. Merseyside Police said its officers were later called to hospital after injured Spencer racially abused another patient while he was receiving treatment for his head injury. A police spokesperson said the \"officers recognised him from the viral social media footage\" and he was arrested. Det Insp Paula Jones said there were \"despicable scenes as bricks, bins and other missiles were thrown\" at officers during the riot. She added: \"Spencer was involved in the violence and will now spend a significant amount of time in prison.\"\n",
      "--------------------------------------------------------------------------------\n",
      "Score: 0.1489, Text: Nine people were arrested at Anfield on Sunday, police said.\n",
      "\n",
      "Nine men have been arrested following reports of tragedy chanting at Sunday's Premier League match between Liverpool and Manchester City. Merseyside Police said nine men, aged between 19 and 57 and variously from Greater Manchester, Cheshire and Lancashire, had been detained at Anfield Stadium. One of the suspects, a 28-year-old man, was also arrested on suspicion of assault, police said. Another man, a 19-year-old from Medlar-with-Wesham in Lancashire, has been charged with a public order offence, namely tragedy chanting, under the 1991 Football Offences Act.\n",
      "\n",
      "He will face Sefton Magistrates' Court next month. Liverpool won the match 2-0 and are nine points clear at the top of the table.\n",
      "--------------------------------------------------------------------------------\n",
      "Score: 0.1463, Text: Triple killer given whole life sentence for neighbour's murder\n",
      "\n",
      "Brian Whitelock will never be considered for parole after murdering Wendy Buckney\n",
      "\n",
      "The family of a man murdered 24 years ago have said \"catastrophic failings\" of the probation service enabled his killer to strike again. Brian Whitelock, 57, has been given a whole life sentence, meaning he will never be considered for parole, for the drug-fuelled and \"brutal\" murder of \"bubbly and caring\" retired riding instructor, Wendy Buckney. He had moved into a flat across the road from the 71-year-old in Clydach, Swansea, after serving 18 years in prison for murder and manslaughter, including killing his own brother. It is understood a multi-agency serious case review will now take place to discover if authorities missed opportunities to protect the public from the double killer.\n",
      "\n",
      "Wendy Buckney knew about her neighbour Brian Whitelock's past and wanted to give him a second chance. But were opportunities missed to protect the public from this triple-killer?\n",
      "\n",
      "The Probation Service said it was also conducting its own review. Whole life orders are reserved for the most severe cases. Nurse Lucy Letby, April Jones killer Mark Bridger and Wayne Couzens, who murdered Sarah Everard while he was a police officer, were all given whole life orders. Sentencing Whitelock at Swansea Crown Court, Judge Mr Justice Griffiths called the murder in August 2022 a \"frenzy of violence\". \"[Wendy] knew what you were doing to her. Her injuries included injuries to her hands while she tried to defend herself, but of course she was no match for you,\" he said. \"You described it as torture, and that's what it was.\" He added: \"You described her as like a second mother to you. She deserved nothing but gratitude, but you brutally murdered her anyway and you have never, ever suggested a motive.\" Wendy's family were joined in court for the sentencing by the families of Whitelock's previous victims, Nicky Morgan and Glen Whitelock. Nine of the 12 jurors from his trial also returned to see him sentenced. In a victim impact statement read in court, Wendy Buckney's sister Ann said Wendy knew about Whitelock's past but had given him odd jobs to do around her flat because she felt he deserved a \"second chance\". Applause broke out in the court room when the whole life sentence was handed down.\n",
      "\n",
      "Warning: This article contains details that some people might find distressing.\n",
      "\n",
      "Ann said her family cannot stop thinking about the day Wendy was killed and whether she was frightened, or \"begging him to stop\". She said: \"He has not just taken one life. He has taken ours too.\" The scale of Wendy's injuries meant they were not able to visit the mortuary to say goodbye. Huw Rogers of the Crown Prosecution Service called the attack \"sustained and brutal\", adding that Wendy's family showed \"remarkable fortitude\" during the trial.\n",
      "\n",
      "Wendy Buckney was popular in the local community as the founder of an affordable riding school\n",
      "\n",
      "Det Ch Insp Matthew Davies of South Wales Police said the crime scene was \"one of the worst\" he had investigated. As well as the extensive injuries to Wendy's body, he said the flat was \"devastated\". A blood-covered sofa was standing on its end and a shelving unit had been ripped apart. The broken shelves matched bruises found on Wendy's body.\n",
      "\n",
      "Wendy was naked, and Whitelock's DNA was found on her underwear. It was the prosecution's case that he had also sexually assaulted her. He admitted to Wendy's murder at the scene of his crime but in court, where he represented himself, he claimed manslaughter on the grounds of diminished responsibility, saying his actions were caused by a head injury. But the prosecution rejected that claim, saying the combined effects of alcohol and a \"fixation\" with diazepam, which he was buying off the streets, had sent him into a violent rage. The jury at Swansea Crown Court took just six minutes to find him guilty of murder. During the court case, it emerged it was not the first time he had killed.\n",
      "\n",
      "Whitelock had already spent 18 years in prison for murdering 27-year-old Nicky Morgan in October 2000, after drinking and taking diazepam. He then set fire to Nicky's body, starting a blaze that killed his own brother, Glen, who was sleeping upstairs. He was given a life sentence for the killings, but was released on licence in 2019 and went to live in Clydach, near Swansea. He had to follow strict conditions, including drug testing and there were restrictions on where he could travel to.\n",
      "\n",
      "Nicky Morgan's family told BBC Wales they repeatedly warned the Probation Service that he was travelling to areas he was banned from and buying drugs. Whitelock was eventually recalled to prison in December 2020 for attacking a shop worker. Nicky's family then wrote to the Parole Board, urging them to deny his re-release and warning he would commit \"another horrific offence\".\n",
      "\n",
      "Nicky Morgan was 27 when he was murdered by Brian Whitelock\n",
      "\n",
      "In a decision document seen by BBC Wales, the Parole Board said it had identified \"protective factors\" that would reduce the risk of him reoffending, including \"his ability to reflect on situations and victim empathy\". Nicky Morgan's sister Melanie Huxley said: \"From the day he was released he was using drugs, drinking, riding around Swansea - all against his conditions. \"It could have been prevented if they had listened.\"\n",
      "\n",
      "In the days leading up to Wendy Buckney's murder, neighbours became increasingly concerned about Whitelock's behaviour and reported it to police. Videos filmed by one neighbour showed him leaning out of his flat window with blood on his face and slurring his words in what the judge called, \"a very advanced state of intoxication\". DCI Davies said officers attended but found no crimes had been committed and gave suitable advice.\n",
      "\n",
      "The Parole Board said its review of the case had been completed and would not be published, but that it took such cases \"extremely seriously\" and was \"committed to learning lessons\". The Ministry of Justice, which runs the Probation Service, said a \"serious further offence\" review was underway, the findings of which will be shared with Wendy's family. A spokesperson added: \"This was an appalling crime and our thoughts remain with the family and friends of Wendy Buckney.\" After the sentencing, Nicky and Glen's families urged any review to look at mistakes made while Whitelock was out on licence. They added: \"We firmly believe he should have been recalled long before. \"Somebody should be held accountable as if he was in prison for breaching his conditions, Wendy could still be alive.\"\n",
      "--------------------------------------------------------------------------------\n",
      "Score: 0.1451, Text: Best albums of 2024: Charli XCX, Beyonce, The Cure and more\n",
      "\n",
      "Charli XCX scored her second UK number 1 album with Brat, which was released in June to rave reviews\n",
      "\n",
      "When Charli XCX recorded her sixth album, Brat, she thought her prickly, abrasive dance anthems were \"not going to appeal to a lot of people\". In the end, the record topped the charts and became a cultural phenomenon. It was nominated for seven Grammys, referenced in the US presidential election, turned into a paint swatch, and named \"word of the year\" by Collins Dictionary. Now the album has been named the best new release of 2024 in a \"poll of polls\" compiled by BBC News. In multiple end of year lists, critics called Brat \"brilliant from start to finish, external\" and \"pop music for the future, external\", praising the way its \"painfully relatable, external\" lyrics captured Charli's insecurities, anxieties and obsessions. In the star's own words, the record is \"chaos and emotional turmoil set to a club soundtrack\". \"The louder you play it, the more honest it gets,\" said the Los Angeles Times, external. The BBC's poll is a \"super-ranking\" compiled from 30 year-end lists published by the world's most influential music magazines - including the NME, Rolling Stone, Spain's Mondo Sonoro and France's Les Inrockuptibles. Records were assigned points based on their position in each list - with the number one album getting 20 points, the number two album receiving 19 points, and so on. Brat was the runaway winner with a score of 486 points, nearly twice as many as the number two album, Beyoncé's Cowboy Carter. In total, the critics named 184 records among their favourites, from the The Cure's long-awaited comeback, Songs Of A Lost World, to the kaleidoscopic rap of Doechii's Alligator Bites Never Heal.\n",
      "\n",
      "Charli was born Emma Aitchison in Essex, UK, and has been chipping away at the coalface of pop for more than a decade. At the start of her career, she scored hits with shiny pop anthems such as Fancy, I Love It and Boom Clap - but over the years, her music has become more volatile and aggressive. Underground anthems like Vroom, Vroom and Track 10 turned her into a cult star but, as she confessed on Brat: \"I've started thinking again about whether I deserve commercial success\". With that in mind, she entered 2024 with a new sense of purpose. \"Before we'd even done much writing, she had a masterplan of all the stuff she wanted to write about, and all the things she wanted to say,\" producer AG Cook tells the BBC. \"She had a real vision for the album, right from the start.\" \"Even the name Brat was in play for about two years,\" adds co-producer Finn Keane.\n",
      "\n",
      "Released in June, Brat became the soundtrack to the summer; and Charli extended her success with a remix album that rewrote many of the songs and added an array of guest stars, from Billie Eilish and Robyn to The 1975 and Lorde. The remix project was \"really, off-the-cuff and last minute\", says Cook, \"but that's been part of the fun of Brat\". \"Charli is just incredibly quick and open to ideas,\" adds Keane. \"You can give her kind of any kind of crazy track, and she'll instantly be able to come up with something super hooky, with a twist that's very memorable and elaborate. Billboard: \"Charli XCX pulled off one of the most exciting and culturally significant album launches in modern memory... And best of all? It was all on Charli's own terms. Drawing inspiration primarily from club culture and hyperpop, Charli pulled once-niche spaces in music into the mainstream.\" The Forty Five: \"In making a club record to ignite the underground, she's reached the world's biggest stages. Musically, Charli is at her peak.\"\n",
      "\n",
      "Frequently mis-labelled as a country album, Beyoncé's Cowboy Carter is so much more. A racial reckoning with the black roots of American folk music, its 27 tracks embrace everything from line-dancing to psychedelic rock, with guest appearances from Dolly Parton, Willie Nelson and Post Malone. The Times: \"The pop hoedown single Texas Hold 'Em remains the best piece, but the acoustic guitar-driven sexy ode Bodyguard is another highlight. Will this finally win Beyoncé her best album Grammy?\" NME: \"A masterclass in creativity from an artist who never forgets her roots.\"\n",
      "\n",
      "The fourth album by Dublin's Fontaines DC saw the quintet take their scratchy, sinister sound and run it through a technicolor filter. The results include everything from stadium-sized sing-alongs (Favourite) to panic-inducing punk anthems (Starburster). Allmusic: \"When all is said and done, they remain fantastic songwriters, able to convey a variety of emotions without relying on the trappings of punk. The corners may have been sanded off, but it has only revealed new and interesting textures underneath.\" Mojo magazine: \"Fontaines D.C. are now, in terms of risk-taking potential, the Arctic Monkeys' closest rivals.\"\n",
      "\n",
      "The title says it all. None of the songs on Billie Eilish's exquisite third album are content to sit still, moving from hushed intimacy to emotional volatility as the singer navigates the murky waters of her early 20s. The Telegraph: \"Eilish has made something rich, strange, smart, sad and wise enough to stand comparison with Joni Mitchell's Blue. A heartbreak masterpiece for her generation, and for the ages.\" The Guardian: \"An album that keeps wrongfooting the listener, Hit Me Hard and Soft is clearly intended as something to gradually unpick: A bold move in a pop world where audiences are usually depicted as suffering from an attention deficit that requires instant gratification.\"\n",
      "\n",
      "Billed by one publication as the \"poet laureate of indie rock, external\", MJ Lenderman's breakthrough album is tender, melancholy and wryly funny, populated by a cast of flawed, disappointed and disappointing characters he observed around his hometown of Asheville, North Carolina. New York Times: \"An ace guitarist with a keen ear for jangly tones, he lends even his most pathetic characters a bit of warm-blooded humanity.\" The Line Of Best Fit: \"How he gets you to care about nobodies from nowhere and their very strange plights is in part to do with his knack for universal empathy, but more importantly, the fact that he sings everything like he was just robbed at gunpoint by his 8th grade bully who he later watched win the lottery. You feel bad for things you don't necessarily even understand.\"\n",
      "\n",
      "Sixteen years in the making, The Cure's 14th studio album didn't disappoint. Written during a period where frontman Robert Smith lost his mother, father and brother, it is simultaneously dark and fragile. Speaking to the BBC, Smith said making the record had been \"hugely cathartic\" in escaping the \"doom and gloom\" he felt. Time magazine: \"It's no exaggeration that this is an album haunted by death, so it's almost ironic that, musically speaking, there hasn't been this much life in The Cure for decades.\" Pitchfork: \"It feels like a record whose time is right, delivering a concentrated dose of The Cure and cutting the fat that dogged their later albums.\"\n",
      "\n",
      "A sprawling, two-hour opus of dreamy pop and psychedelia, this is one of the year's most mysterious records. You can't buy the CD or vinyl, and it's not available on Spotify or Apple Music. At the time of writing, it's only available as a continuous, ad-free stream on YouTube, external, or as a download from Bandcamp., external But the seventh album by Cyndi Lee (the drag alter-ego of rock musician Patrick Flegel) is definitely worth your seeking out - like the lost transmissions of a ghostly 1960s pirate radio station. Uncut: \"Cindy Lee has managed to buck just about every trend, convention and expectation of what releasing music in the digital age is supposed to look and like. And, even more crucially, it sounds just as refreshing.\" Stereogum: \"Diamond Jubilee is two hours of unrushed wandering through a lo-fi escape, catchy to the point of sticky, tarnishing in its abrasiveness yet sun-baked to perfection.\"\n",
      "\n",
      "On her sixth album as Waxahatchee, singer-songwriter Katie Crutchfield tackles everything from anxiety and self-doubt, to her ongoing struggle with sobriety, with piercing insight and a laid-back country-rock feel. Pitchfork: \"Her mind is alive and humming, and her language leaps out at you with its hunger.\" Consequence of Sound: \"Crutchfield is still growing, both personally and artistically, and we're just glad she's invited us along for the ride.\"\n",
      "\n",
      "After landing the decisive blow in his rap beef with Drake, Compton rapper Kendrick Lamar took a victory lap on his surprise sixth album, GNX. Razor sharp and rhythmically complex, it's both a poison pen letter to his detractors, and a love letter to Los Angeles' hip-hop culture. LA Times: \"Lamar is worked up about liars, about folks doling out backhanded compliments, about other rappers with \"old-ass flows\" wasting space with empty rhymes. Indeed, what seems to make him angriest is the idea that a person could triumph in hip-hop by taking hip-hop less seriously than he does.\" Complex: \"Even cooler is how much space Kendrick gives to underground rappers from the LA scene—figures who are talented but raw, and would likely struggle to gain national recognition without a boost.\"\n",
      "\n",
      "Six albums into her career, former Disney star Sabrina Carpenter landed on a winning formula - one that puts aside the cookie-cutter pop of her teen years, and zeroes in on her sly humour as a USP. Fleet of foot and packed with memorable one-liners, it produced three number one singles in the UK, including song of the year contender Espresso. Esquire: \"The range, humour, and sophistication of these 12 songs were a revelation.\"\n",
      "\n",
      "=14) Jessica Pratt - Here In The Pitch 16) Vampire Weekend - Only God Was Above Us 22) English Teacher - This Could Be Texas 23) The Last Dinner Party - Prelude To Ecstasy\n",
      "\n",
      "The chart was compiled from 30 \"best of\" lists in the following publications: Billboard, Complex, Consequence Of Sound, Daily Mail, Dazed Magazine, Double J, Esquire, Entertainment Weekly, The Forty Five, Gorilla Vs Bear, The Guardian, The Independent, LA Times, Les Inrocks, Line Of Best Fit, Mojo, Mondo Sonoro, NME, New York Times, Paste, People, Pitchfork, Pop Matters, The Skinny, Rolling Stone, Stereogum, The Sunday Times, The Telegraph, Time Magazine and Uncut.\n",
      "--------------------------------------------------------------------------------\n",
      "Score: 0.1451, Text: Jude Bellingham is one of six contenders for the 2024 BBC Sports Personality of the Year award.\n",
      "\n",
      "Here BBC Sport takes a look back at the Real Madrid and England footballer's year in five photos.\n",
      "--------------------------------------------------------------------------------\n",
      "Score: 0.1432, Text: Why has Elon Musk been pushing for a government shutdown? Why has Elon Musk been pushing for a government shutdown?\n",
      "--------------------------------------------------------------------------------\n",
      "Score: 0.1343, Text: This video can not be played To play this video you need to enable JavaScript in your browser.\n",
      "\n",
      "King Charles got into the festive spirit with an enthusiastically received visit to a Christmas market in Battersea Power Station in south-west London. A community choir was singing the carol We Three Kings as he arrived - and they had at least one real-life King there in person to listen. \"You must be frozen. You'll need a stiff drink,\" the King said to the singers, outside in the December drizzle. The King also met Apple chief Tim Cook at the computer firm's UK headquarters in the huge former power station building, before singer Raye gave them a seasonal serenade of \"Holy Night\".\n",
      "\n",
      "The King visited Apple's UK headquarters in the power station\n",
      "\n",
      "The King met Apple boss Tim Cook and singer Raye on a visit to Apple HQ\n",
      "\n",
      "Much of the power station is now a glitzy mall and Christmas shoppers got more than they bargained for as the King appeared, walking past the window of Starbucks. There can be something wistful about even the most cheerful carols - and it's been a difficult year for the King, with his health problems. But he looked cheerful here, talking to some rather gobsmacked stallholders under the Christmas lights. He always seems energised by the crowds, joking and chatting. When he was being steered on to the next part of the trip, he still dived across for an impromptu meeting with shoppers, shaking hands and facing the wall of mobile phones. The trip to the power station seemed to recharge him. He was introduced to 90-year-old Rita Kelly, who had worked here in the 1950s when the turbines were pumping out power for the capital, and who said she was \"honoured\" to talk about her memories with the King. \"Working here was a very happy time,\" she said. Although she said the King seemed to know about her \"mischievous\" side, when as a youngster she'd misguidedly tried to go upside one of the huge chimneys. Not even Father Christmas would have risked that one.\n",
      "\n",
      "The King met Rita Kelly, a retired typist who worked in the power station in the 1950s\n",
      "\n",
      "The King went over to speak to some surprised Christmas shoppers\n",
      "\n",
      "The King visited the Curated Makers Market and saw the stalls for small traders and craftspeople. It was founded by Megan Jones, who has been supported by the King's Trust, formerly known as the Prince's Trust. \"People tend to go to big shops, but here they can talk to people behind the brand,\" said Soophia Foroughi, who chatted to the King about her handmade jewellery, under her label Ava and Azar. Natasha Kutrovatz, who sells her own jewellery, was delighted by the King's interest in people making a living from handmade crafts. \"As a parent, it's a much more flexible way of working,\" she said. The 1930s power station once produced a fifth of London's electricity, including supplying landmarks such as Buckingham Palace and the Houses of Parliament. The link to Buckingham Palace had been discreetly labelled as \"Carnaby Street\" in the control room, in case anyone broke in and literally wanted to switch off the Palace lights, said a spokeswoman for the power station. With the crowds and cameras for the King's visit, it was probably louder inside the halls than when it was generating power. It's enough to shake Santa's workshop, although in a designer outlet like this, the resident Santa is living in a \"yurt\". This huge cathedral of the industrial age remains an impressive sight - and the King's mother, when she was still Princess Elizabeth, had come here in 1949 to see the power station operating. The power station, built with six million bricks, is now filled with shops and places to eat and drink and the King looked up inside this cavernous post-industrial landmark. He waved back to some rather taken-aback shoppers, looking down from the layers of walkways above. The boiler house is so big that St Paul's Cathedral could fit inside. The King is known to be interested in architecture and design - and this building, with its four chimneys looking like an upside down coffee table, was designed by Sir Giles Gilbert Scott, also responsible for the classic red phone box and Liverpool Anglican Cathedral.\n",
      "\n",
      "The singer Raye performed at the end of the King's visit to Battersea Power Station\n",
      "\n",
      "There's a strong sense of design for another business based in the power station - the technology firm, Apple, which the King visited after the Christmas market. Some might see parallels between Apple computers and the monarchy - a bit expensive but with many dedicated fans who think it's worth it. The King went into the Apple headquarters to meet chief executive Mr Cook and was shown an example of the firm's artificial technology, which can turn a few squiggles into a fully-fledged illustration. The computer company has worked with the King's Trust to help provide young people with digital skills and the King unveiled a plaque. Cook praised the King's \"lifelong commitment to philanthropy and the betterment of humanity\". There's a message about changed times in how it's Apple that now occupies these old buildings. This power station once used a million tonnes of coal a year, dug up in coalfields in Wales and northern England. It is tech-firms like Apple and designer shops that now fill the huge halls. And given how much the King has been photographed by mobile phones, there can few people in the country who have seen as many iPhones close up. At the end of the visit, drawings by children at a nearby primary school were projected on to the giant chimneys, climbing up into the evening sky. As the King left there was more music, with a performance by singer-songwriter Raye. After all the Christmas trees and conversations, maybe it was time for his own stiff drink.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Bernie Ecclestone selling?\"\n",
    "\n",
    "try:\n",
    "    # Perform the semantic search\n",
    "    start_time = time.time()\n",
    "    search_results = vector_store.similarity_search_with_score(query, k=10)\n",
    "    search_elapsed_time = time.time() - start_time\n",
    "\n",
    "    logging.info(f\"Semantic search completed in {search_elapsed_time:.2f} seconds\")\n",
    "\n",
    "    # Display search results\n",
    "    print(f\"\\nSemantic Search Results (completed in {search_elapsed_time:.2f} seconds):\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for doc, score in search_results:\n",
    "        print(f\"Score: {score:.4f}, Text: {doc.page_content}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "except CouchbaseException as e:\n",
    "    raise RuntimeError(f\"Error performing semantic search: {str(e)}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Unexpected error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) with Couchbase and LangChain\n",
    "Couchbase and LangChain can be seamlessly integrated to create RAG (Retrieval-Augmented Generation) chains, enhancing the process of generating contextually relevant responses. In this setup, Couchbase serves as the vector store, where embeddings of documents are stored. When a query is made, LangChain retrieves the most relevant documents from Couchbase by comparing the query’s embedding with the stored document embeddings. These documents, which provide contextual information, are then passed to a generative language model within LangChain.\n",
    "\n",
    "The language model, equipped with the context from the retrieved documents, generates a response that is both informed and contextually accurate. This integration allows the RAG chain to leverage Couchbase’s efficient storage and retrieval capabilities, while LangChain handles the generation of responses based on the context provided by the retrieved documents. Together, they create a powerful system that can deliver highly relevant and accurate answers by combining the strengths of both retrieval and generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever from vector store\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\"k\": 10}\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create RAG prompt template\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that answers questions based on the provided context.\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# Create RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "logging.info(\"Successfully created RAG chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Response:  Ecclestone, who was in charge of the sport for nearly 40 years until 2017, assembled the collection of 69 iconic F1 and Grand Prix cars over a span of more than five decades.\n",
      "RAG response generated in 7.86 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "rag_response = rag_chain.invoke(query)\n",
    "rag_elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"RAG Response: {rag_response}\")\n",
    "print(f\"RAG response generated in {rag_elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Couchbase as a caching mechanism\n",
    "Couchbase can be effectively used as a caching mechanism for RAG (Retrieval-Augmented Generation) responses by storing and retrieving precomputed results for specific queries. This approach enhances the system's efficiency and speed, particularly when dealing with repeated or similar queries. When a query is first processed, the RAG chain retrieves relevant documents, generates a response using the language model, and then stores this response in Couchbase, with the query serving as the key.\n",
    "\n",
    "For subsequent requests with the same query, the system checks Couchbase first. If a cached response is found, it is retrieved directly from Couchbase, bypassing the need to re-run the entire RAG process. This significantly reduces response time because the computationally expensive steps of document retrieval and response generation are skipped. Couchbase's role in this setup is to provide a fast and scalable storage solution for caching these responses, ensuring that frequently asked queries can be answered more quickly and efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1: What happened in the match between Fullham and Liverpool?\n",
      "Response:  Liverpool won the match 2-0 and are nine points clear at the top of the table.\n",
      "Time taken: 5.16 seconds\n",
      "\n",
      "Query 2: What is Bernie Ecclestone selling?\n",
      "Response:  Ecclestone, who was in charge of the sport for nearly 40 years until 2017, assembled the collection of 69 iconic F1 and Grand Prix cars over a span of more than five decades.\n",
      "Time taken: 2.23 seconds\n",
      "\n",
      "Query 3: What happened in the match between Fullham and Liverpool?\n",
      "Response:  Liverpool won the match 2-0 and are nine points clear at the top of the table.\n",
      "Time taken: 2.08 seconds\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    queries = [\n",
    "        \"What happened in the match between Fullham and Liverpool?\",\n",
    "        \"What is Bernie Ecclestone selling?\",\n",
    "        \"What happened in the match between Fullham and Liverpool?\", # Repeated query\n",
    "    ]\n",
    "\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\nQuery {i}: {query}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = rag_chain.invoke(query)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error generating RAG response: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
