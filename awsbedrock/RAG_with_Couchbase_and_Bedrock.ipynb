{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this guide, we will walk you through building a powerful semantic search engine using Couchbase as the backend database and [Amazon Bedrock](https://aws.amazon.com/bedrock/) as both the embedding and language model provider. Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval. This tutorial is designed to be beginner-friendly, with clear, step-by-step instructions that will equip you with the knowledge to create a fully functional semantic search system from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run this tutorial\n",
    "\n",
    "This tutorial is available as a Jupyter Notebook (`.ipynb` file) that you can run interactively. You can access the original notebook [here](https://github.com/couchbase-examples/vector-search-cookbook/blob/main/awsbedrock/RAG_with_Couchbase_and_Bedrock.ipynb).\n",
    "\n",
    "You can either download the notebook file and run it on [Google Colab](https://colab.research.google.com/) or run it on your system by setting up the Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before you start\n",
    "\n",
    "## Get Credentials for AWS Bedrock\n",
    "* Please follow the [instructions](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html) to set up AWS Bedrock and generate credentials.\n",
    "* Ensure you have the necessary IAM permissions to access Bedrock services.\n",
    "\n",
    "## Create and Deploy Your Free Tier Operational cluster on Capella\n",
    "\n",
    "To get started with Couchbase Capella, create an account and use it to deploy a forever free tier operational cluster. This account provides you with an environment where you can explore and learn about Capella with no time constraint.\n",
    "\n",
    "To know more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html).\n",
    "\n",
    "### Couchbase Capella Configuration\n",
    "\n",
    "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met.\n",
    "\n",
    "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the travel-sample bucket (Read and Write) used in the application.\n",
    "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Stage: Installing Necessary Libraries\n",
    "\n",
    "To build our semantic search engine, we need a robust set of tools. The libraries we install handle everything from connecting to databases to performing complex machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet datasets langchain-couchbase langchain-aws boto3 python-dotenv kagglehub pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries\n",
    "\n",
    "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "import boto3\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.exceptions import (CouchbaseException,\n",
    "                                  InternalServerFailureException,\n",
    "                                  QueryIndexAlreadyExistsException)\n",
    "from couchbase.management.search import SearchIndex\n",
    "from couchbase.options import ClusterOptions\n",
    "from dotenv import load_dotenv\n",
    "from langchain_aws import BedrockEmbeddings, ChatBedrock\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_couchbase.cache import CouchbaseCache\n",
    "from langchain_couchbase.vectorstores import CouchbaseVectorStore\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Logging\n",
    "\n",
    "Logging is configured to track the progress of the script and capture any errors or warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Sensitive Informnation\n",
    "In this section, we prompt the user to input essential configuration settings needed. These settings include sensitive information like AWS credentials, database credentials, and specific configuration names. Instead of hardcoding these details into the script, we request the user to provide them at runtime, ensuring flexibility and security.\n",
    "\n",
    "The script also validates that all required inputs are provided, raising an error if any crucial information is missing. This approach ensures that your integration is both secure and correctly configured without hardcoding sensitive information, enhancing the overall security and maintainability of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "# Load environment variables from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "# AWS Credentials\n",
    "AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID') or input('Enter your AWS Access Key ID: ')\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY') or getpass.getpass('Enter your AWS Secret Access Key: ')\n",
    "AWS_REGION = os.getenv('AWS_REGION') or input('Enter your AWS region (default: us-east-1): ') or 'us-east-1'\n",
    "\n",
    "# Couchbase Settings\n",
    "CB_HOST = os.getenv('CB_HOST') or input('Enter your Couchbase host (default: couchbase://localhost): ') or 'couchbase://localhost'\n",
    "CB_USERNAME = os.getenv('CB_USERNAME') or input('Enter your Couchbase username (default: Administrator): ') or 'Administrator'\n",
    "CB_PASSWORD = os.getenv('CB_PASSWORD') or getpass.getpass('Enter your Couchbase password (default: password): ') or 'password'\n",
    "CB_BUCKET_NAME = os.getenv('CB_BUCKET_NAME') or input('Enter your Couchbase bucket name (default: vector-search-testing): ') or 'vector-search-testing'\n",
    "INDEX_NAME = os.getenv('INDEX_NAME') or input('Enter your index name (default: vector_search_bedrock): ') or 'vector_search_bedrock'\n",
    "SCOPE_NAME = os.getenv('SCOPE_NAME') or input('Enter your scope name (default: shared): ') or 'shared'\n",
    "COLLECTION_NAME = os.getenv('COLLECTION_NAME') or input('Enter your collection name (default: bedrock): ') or 'bedrock'\n",
    "CACHE_COLLECTION = os.getenv('CACHE_COLLECTION') or input('Enter your cache collection name (default: cache): ') or 'cache'\n",
    "\n",
    "# Check if required credentials are set\n",
    "for cred_name, cred_value in {\n",
    "    'AWS_ACCESS_KEY_ID': AWS_ACCESS_KEY_ID,\n",
    "    'AWS_SECRET_ACCESS_KEY': AWS_SECRET_ACCESS_KEY, \n",
    "    'CB_HOST': CB_HOST,\n",
    "    'CB_USERNAME': CB_USERNAME,\n",
    "    'CB_PASSWORD': CB_PASSWORD,\n",
    "    'CB_BUCKET_NAME': CB_BUCKET_NAME\n",
    "}.items():\n",
    "    if not cred_value:\n",
    "        raise ValueError(f\"{cred_name} is not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the Couchbase Cluster\n",
    "Connecting to a Couchbase cluster is the foundation of our project. Couchbase will serve as our primary data store, handling all the storage and retrieval operations required for our semantic search engine. By establishing this connection, we enable our application to interact with the database, allowing us to perform operations such as storing embeddings, querying data, and managing collections. This connection is the gateway through which all data will flow, so ensuring it's set up correctly is paramount.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 15:50:26,753 - INFO - Successfully connected to Couchbase\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "    options = ClusterOptions(auth)\n",
    "    cluster = Cluster(CB_HOST, options)\n",
    "    cluster.wait_until_ready(timedelta(seconds=5))\n",
    "    logging.info(\"Successfully connected to Couchbase\")\n",
    "except Exception as e:\n",
    "    raise ConnectionError(f\"Failed to connect to Couchbase: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Collections in Couchbase\n",
    "In Couchbase, data is organized in buckets, which can be further divided into scopes and collections. Think of a collection as a table in a traditional SQL database. Before we can store any data, we need to ensure that our collections exist. If they don't, we must create them. This step is important because it prepares the database to handle the specific types of data our application will process. By setting up collections, we define the structure of our data storage, which is essential for efficient data retrieval and management.\n",
    "\n",
    "Moreover, setting up collections allows us to isolate different types of data within the same bucket, providing a more organized and scalable data structure. This is particularly useful when dealing with large datasets, as it ensures that related data is stored together, making it easier to manage and query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 15:50:29,258 - INFO - Collection 'bedrock' already exists. Skipping creation.\n",
      "2025-01-08 15:50:30,510 - INFO - Primary index present or created successfully.\n",
      "2025-01-08 15:50:31,705 - INFO - All documents cleared from the collection.\n",
      "2025-01-08 15:50:33,039 - INFO - Collection 'cache' already exists. Skipping creation.\n",
      "2025-01-08 15:50:34,072 - INFO - Primary index present or created successfully.\n",
      "2025-01-08 15:50:34,322 - INFO - All documents cleared from the collection.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<couchbase.collection.Collection at 0x176fb9ed0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup_collection(cluster, bucket_name, scope_name, collection_name):\n",
    "    try:\n",
    "        bucket = cluster.bucket(bucket_name)\n",
    "        bucket_manager = bucket.collections()\n",
    "\n",
    "        # Check if collection exists, create if it doesn't\n",
    "        collections = bucket_manager.get_all_scopes()\n",
    "        collection_exists = any(\n",
    "            scope.name == scope_name and collection_name in [col.name for col in scope.collections]\n",
    "            for scope in collections\n",
    "        )\n",
    "\n",
    "        if not collection_exists:\n",
    "            logging.info(f\"Collection '{collection_name}' does not exist. Creating it...\")\n",
    "            bucket_manager.create_collection(scope_name, collection_name)\n",
    "            logging.info(f\"Collection '{collection_name}' created successfully.\")\n",
    "        else:\n",
    "            logging.info(f\"Collection '{collection_name}' already exists. Skipping creation.\")\n",
    "\n",
    "        collection = bucket.scope(scope_name).collection(collection_name)\n",
    "\n",
    "        # Ensure primary index exists\n",
    "        try:\n",
    "            cluster.query(f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{bucket_name}`.`{scope_name}`.`{collection_name}`\").execute()\n",
    "            logging.info(\"Primary index present or created successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error creating primary index: {str(e)}\")\n",
    "\n",
    "        # Clear all documents in the collection\n",
    "        try:\n",
    "            query = f\"DELETE FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
    "            cluster.query(query).execute()\n",
    "            logging.info(\"All documents cleared from the collection.\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error while clearing documents: {str(e)}. The collection might be empty.\")\n",
    "\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error setting up collection: {str(e)}\")\n",
    "\n",
    "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)\n",
    "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, CACHE_COLLECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Couchbase Vector Search Index\n",
    "\n",
    "Semantic search requires an efficient way to retrieve relevant documents based on a user's query. This is where the Couchbase **Vector Search Index** comes into play. In this step, we load the Vector Search Index definition from a JSON file, which specifies how the index should be structured. This includes the fields to be indexed, the dimensions of the vectors, and other parameters that determine how the search engine processes queries based on vector similarity.\n",
    "\n",
    "This AWS Bedrock vector search index configuration requires specific default settings to function properly. This tutorial uses the bucket named `vector-search-testing` with the scope `shared` and collection `bedrock`. The configuration is set up for vectors with exactly 1024 dimensions, using dot product similarity and optimized for recall. If you want to use a different bucket, scope, or collection, you will need to modify the index configuration accordingly.\n",
    "\n",
    "For more information on creating a vector search index, please follow the [instructions](https://docs.couchbase.com/cloud/vector-search/create-vector-search-index-ui.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('aws_index.json', 'r') as file:\n",
    "        index_definition = json.load(file)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading index definition: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating or Updating Search Indexes\n",
    "\n",
    "With the index definition loaded, the next step is to create or update the **Vector Search Index** in Couchbase. This step is crucial because it optimizes our database for vector similarity search operations, allowing us to perform searches based on the semantic content of documents rather than just keywords. By creating or updating a Vector Search Index, we enable our search engine to handle complex queries that involve finding semantically similar documents using vector embeddings, which is essential for a robust semantic search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 15:50:35,847 - INFO - Index 'vector_search_bedrock' found\n",
      "2025-01-08 15:50:36,835 - INFO - Index 'vector_search_bedrock' already exists. Skipping creation/update.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    scope_index_manager = cluster.bucket(CB_BUCKET_NAME).scope(SCOPE_NAME).search_indexes()\n",
    "\n",
    "    # Check if index already exists\n",
    "    existing_indexes = scope_index_manager.get_all_indexes()\n",
    "    index_name = index_definition[\"name\"]\n",
    "\n",
    "    if index_name in [index.name for index in existing_indexes]:\n",
    "        logging.info(f\"Index '{index_name}' found\")\n",
    "    else:\n",
    "        logging.info(f\"Creating new index '{index_name}'...\")\n",
    "\n",
    "    # Create SearchIndex object from JSON definition\n",
    "    search_index = SearchIndex.from_json(index_definition)\n",
    "\n",
    "    # Upsert the index (create if not exists, update if exists)\n",
    "    scope_index_manager.upsert_index(search_index)\n",
    "    logging.info(f\"Index '{index_name}' successfully created/updated.\")\n",
    "\n",
    "except QueryIndexAlreadyExistsException:\n",
    "    logging.info(f\"Index '{index_name}' already exists. Skipping creation/update.\")\n",
    "\n",
    "except InternalServerFailureException as e:\n",
    "    error_message = str(e)\n",
    "    logging.error(f\"InternalServerFailureException raised: {error_message}\")\n",
    "\n",
    "    try:\n",
    "        # Accessing the response_body attribute from the context\n",
    "        error_context = e.context\n",
    "        response_body = error_context.response_body\n",
    "        if response_body:\n",
    "            error_details = json.loads(response_body)\n",
    "            error_message = error_details.get('error', '')\n",
    "\n",
    "            if \"collection: 'bedrock' doesn't belong to scope: 'shared'\" in error_message:\n",
    "                raise ValueError(\"Collection 'bedrock' does not belong to scope 'shared'. Please check the collection and scope names.\")\n",
    "\n",
    "    except ValueError as ve:\n",
    "        logging.error(str(ve))\n",
    "        raise\n",
    "\n",
    "    except Exception as json_error:\n",
    "        logging.error(f\"Failed to parse the error message: {json_error}\")\n",
    "        raise RuntimeError(f\"Internal server error while creating/updating search index: {error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Amazon Bedrock Client and Embeddings\n",
    "\n",
    "Embeddings are at the heart of semantic search. They are numerical representations of text that capture the semantic meaning of the words and phrases. We'll use Amazon Bedrock's Titan embedding model for embeddings.\n",
    "\n",
    "## Using Amazon Bedrock's Titan Model\n",
    "\n",
    "Language models are AI systems that are trained to understand and generate human language. We'll be using Amazon Bedrock's Titan model to process user queries and generate meaningful responses. The Titan model family includes both embedding models for converting text into vector representations and text generation models for producing human-like responses.\n",
    "\n",
    "Key features of Amazon Bedrock's Titan models:\n",
    "- Titan Embeddings model for embedding vector generation\n",
    "- Titan Text model for natural language understanding and generation\n",
    "- Seamless integration with AWS infrastructure\n",
    "- Enterprise-grade security and scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 15:50:37,019 - INFO - Successfully created Bedrock embeddings client\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bedrock_client = boto3.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name=AWS_REGION,\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
    "    )\n",
    "    \n",
    "    embeddings = BedrockEmbeddings(\n",
    "        client=bedrock_client,\n",
    "        model_id=\"amazon.titan-embed-text-v2:0\"\n",
    "    )\n",
    "    logging.info(\"Successfully created Bedrock embeddings client\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating Bedrock embeddings client: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Setting Up the Couchbase Vector Store\n",
    "A vector store is where we'll keep our embeddings. Unlike the FTS index, which is used for text-based search, the vector store is specifically designed to handle embeddings and perform similarity searches. When a user inputs a query, the search engine converts the query into an embedding and compares it against the embeddings stored in the vector store. This allows the engine to find documents that are semantically similar to the query, even if they don't contain the exact same words. By setting up the vector store in Couchbase, we create a powerful tool that enables our search engine to understand and retrieve information based on the meaning and context of the query, rather than just the specific words used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-08 15:50:41,225 - INFO - Successfully created vector store\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vector_store = CouchbaseVectorStore(\n",
    "        cluster=cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        embedding=embeddings,\n",
    "        index_name=INDEX_NAME,\n",
    "    )\n",
    "    logging.info(\"Successfully created vector store\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to create vector store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the BBC News Article Dataset\n",
    "To build a search engine, we need data to search through. We use the BBC News Article dataset, which provides a comprehensive collection of news articles across various categories. This dataset contains a wide variety of text data that we'll use to train our search engine. Loading the dataset is a crucial step because it provides the raw material that our search engine will work with. The quality and diversity of the data in the BBC News Article dataset make it an excellent choice for testing and refining our search engine, ensuring that it can handle a wide range of queries effectively.\n",
    "\n",
    "The BBC News Article dataset's rich content allows us to simulate real-world scenarios where users ask complex questions, enabling us to fine-tune our search engine's ability to understand and respond to various types of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in dataset directory: ['bbc_news_20220307_20240703.csv']\n",
      "\n",
      "First few rows:\n",
      "                                               title  \\\n",
      "0  Ukraine: Angry Zelensky vows to punish Russian...   \n",
      "1  War in Ukraine: Taking cover in a town under a...   \n",
      "2         Ukraine war 'catastrophic for global food'   \n",
      "3  Manchester Arena bombing: Saffie Roussos's par...   \n",
      "4  Ukraine conflict: Oil price soars to highest l...   \n",
      "\n",
      "                         pubDate  \\\n",
      "0  Mon, 07 Mar 2022 08:01:56 GMT   \n",
      "1  Sun, 06 Mar 2022 22:49:58 GMT   \n",
      "2  Mon, 07 Mar 2022 00:14:42 GMT   \n",
      "3  Mon, 07 Mar 2022 00:05:40 GMT   \n",
      "4  Mon, 07 Mar 2022 08:15:53 GMT   \n",
      "\n",
      "                                               guid  \\\n",
      "0  https://www.bbc.co.uk/news/world-europe-60638042   \n",
      "1  https://www.bbc.co.uk/news/world-europe-60641873   \n",
      "2      https://www.bbc.co.uk/news/business-60623941   \n",
      "3            https://www.bbc.co.uk/news/uk-60579079   \n",
      "4      https://www.bbc.co.uk/news/business-60642786   \n",
      "\n",
      "                                                link  \\\n",
      "0  https://www.bbc.co.uk/news/world-europe-606380...   \n",
      "1  https://www.bbc.co.uk/news/world-europe-606418...   \n",
      "2  https://www.bbc.co.uk/news/business-60623941?a...   \n",
      "3  https://www.bbc.co.uk/news/uk-60579079?at_medi...   \n",
      "4  https://www.bbc.co.uk/news/business-60642786?a...   \n",
      "\n",
      "                                         description  \n",
      "0  The Ukrainian president says the country will ...  \n",
      "1  Jeremy Bowen was on the frontline in Irpin, as...  \n",
      "2  One of the world's biggest fertiliser firms sa...  \n",
      "3  The parents of the Manchester Arena bombing's ...  \n",
      "4  Consumers are feeling the impact of higher ene...  \n"
     ]
    }
   ],
   "source": [
    "# Download to current directory\n",
    "dataset_path = kagglehub.dataset_download(\n",
    "    'bhavikjikadara/bbc-news-articles'\n",
    ")\n",
    "\n",
    "# List all files in the dataset directory\n",
    "print(\"Files in dataset directory:\", os.listdir(dataset_path))\n",
    "\n",
    "# Find and read the CSV file with full path\n",
    "csv_file = [f for f in os.listdir(dataset_path) if f.endswith('.csv') and 'bbc' in f.lower()][0]\n",
    "full_path = os.path.join(dataset_path, csv_file)\n",
    "df = pd.read_csv(full_path)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Data to the Vector Store\n",
    "With the vector store set up, the next step is to populate it with data. We save the TREC dataset to the vector store in batches. This method is efficient and ensures that our search engine can handle large datasets without running into performance issues. By saving the data in this way, we prepare our search engine to quickly and accurately respond to user queries. This step is essential for making the dataset searchable, transforming raw data into a format that can be easily queried by our search engine.\n",
    "\n",
    "Batch processing is particularly important when dealing with large datasets, as it prevents memory overload and ensures that the data is stored in a structured and retrievable manner. This approach not only optimizes performance but also ensures the scalability of our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_news_to_vectorstore(\n",
    "    df,\n",
    "    vector_store,\n",
    "    sample_size=None,\n",
    "    batch_size=50,\n",
    "    start_index=0,\n",
    "    end_index=None,\n",
    "    quiet=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Load news articles into Couchbase vector store with configurable parameters.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe containing news articles\n",
    "        vector_store (CouchbaseVectorStore): Initialized vector store\n",
    "        sample_size (int, optional): Number of articles to process. If None, processes all\n",
    "        batch_size (int): Number of articles to process in each batch\n",
    "        start_index (int): Starting index in the dataframe\n",
    "        end_index (int): Ending index in the dataframe\n",
    "        quiet (bool): If True, reduces progress output\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (start_index processed, end_index processed)\n",
    "    \"\"\"\n",
    "    # Configure logging\n",
    "    if quiet:\n",
    "        logging.getLogger().setLevel(logging.WARNING)\n",
    "        logging.getLogger('openai').setLevel(logging.WARNING)\n",
    "        logging.getLogger('httpx').setLevel(logging.WARNING)\n",
    "    \n",
    "    # Determine processing range\n",
    "    if sample_size:\n",
    "        df = df.head(sample_size)\n",
    "    if end_index is None:\n",
    "        end_index = len(df)\n",
    "    \n",
    "    df_slice = df[start_index:end_index]\n",
    "    total_records = len(df_slice)\n",
    "    \n",
    "    for i in range(0, total_records, batch_size):\n",
    "        batch_df = df_slice[i:i + batch_size]\n",
    "        \n",
    "        texts = batch_df['description'].tolist()\n",
    "        metadatas = batch_df.apply(\n",
    "            lambda row: {\n",
    "                'title': row['title'],\n",
    "                'pubDate': row['pubDate'],\n",
    "                'guid': row['guid'],\n",
    "                'link': row['link']\n",
    "            }, \n",
    "            axis=1\n",
    "        ).tolist()\n",
    "        \n",
    "        ids = [f\"article_{j}\" for j in range(start_index + i, start_index + i + len(batch_df))]\n",
    "        \n",
    "        try:\n",
    "            vector_store.add_texts(\n",
    "                texts=texts,\n",
    "                metadatas=metadatas,\n",
    "                ids=ids,\n",
    "                batch_size=batch_size\n",
    "            )\n",
    "            print(f\"Progress: {(i + batch_size) * 100 // total_records}% \"\n",
    "                  f\"({start_index + i + len(batch_df)}/{start_index + total_records} documents)\", \n",
    "                  end='\\r')\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError at batch starting at index {start_index + i}: {str(e)}\")\n",
    "            return start_index + i, end_index\n",
    "            \n",
    "    print(f\"\\nCompleted loading data from index {start_index} to {start_index + total_records}\")\n",
    "    return start_index, start_index + total_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100% (1000/1000 documents)\n",
      "Completed loading data from index 0 to 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1: Process just 1000 articles\n",
    "load_news_to_vectorstore(df, vector_store, sample_size=1000, quiet=True)\n",
    "\n",
    "# Example 2: Process in chunks of 1000\n",
    "# chunk_size = 1000\n",
    "# for start in range(0, len(df), chunk_size):\n",
    "#     print(f\"\\nProcessing chunk starting at {start}\")\n",
    "#     load_news_to_vectorstore(\n",
    "#         df, \n",
    "#         vector_store,\n",
    "#         start_index=start,\n",
    "#         end_index=start + chunk_size,\n",
    "#         batch_size=50,\n",
    "#         quiet=True\n",
    "#     )\n",
    "\n",
    "# Example 3: Process specific range\n",
    "# load_news_to_vectorstore(\n",
    "#     df,\n",
    "#     vector_store,\n",
    "#     start_index=5000,\n",
    "#     end_index=6000,\n",
    "#     batch_size=50,\n",
    "#     quiet=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up a Couchbase Cache\n",
    "To further optimize our system, we set up a Couchbase-based cache. A cache is a temporary storage layer that holds data that is frequently accessed, speeding up operations by reducing the need to repeatedly retrieve the same information from the database. In our setup, the cache will help us accelerate repetitive tasks, such as looking up similar documents. By implementing a cache, we enhance the overall performance of our search engine, ensuring that it can handle high query volumes and deliver results quickly.\n",
    "\n",
    "Caching is particularly valuable in scenarios where users may submit similar queries multiple times or where certain pieces of information are frequently requested. By storing these in a cache, we can significantly reduce the time it takes to respond to these queries, improving the user experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cache = CouchbaseCache(\n",
    "        cluster=cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=CACHE_COLLECTION,\n",
    "    )\n",
    "    logging.info(\"Successfully created cache\")\n",
    "    set_llm_cache(cache)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to create cache: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Amazon Bedrock's Titan Text Express v1 Model\n",
    "\n",
    "Amazon Bedrock's Titan Text Express v1 is a state-of-the-art foundation model designed for fast and efficient text generation tasks. This model excels at:\n",
    "\n",
    "- Text generation and completion\n",
    "- Question answering \n",
    "- Summarization\n",
    "- Content rewriting\n",
    "- Analysis and extraction\n",
    "\n",
    "Key features of Titan Text Express v1:\n",
    "\n",
    "- Optimized for low-latency responses while maintaining high quality output\n",
    "- Supports up to 8K tokens context window\n",
    "- Built-in content filtering and safety controls\n",
    "- Cost-effective compared to larger models\n",
    "- Seamlessly integrates with AWS services\n",
    "\n",
    "The model uses a temperature parameter (0-1) to control randomness in responses:\n",
    "- Lower values (e.g. 0) produce more focused, deterministic outputs\n",
    "- Higher values introduce more creativity and variation\n",
    "\n",
    "We'll be using this model through Amazon Bedrock's API to process user queries and generate contextually relevant responses based on our vector database content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    llm = ChatBedrock(\n",
    "        client=bedrock_client,\n",
    "        model_id=\"amazon.titan-text-express-v1\",\n",
    "        model_kwargs={\"temperature\": 0}\n",
    "    )\n",
    "    logging.info(\"Successfully created Bedrock LLM client\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error creating Bedrock LLM client: {str(e)}. Please check your AWS credentials and Bedrock access.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Semantic Search\n",
    "Semantic search in Couchbase involves converting queries and documents into vector representations using an embeddings model. These vectors capture the semantic meaning of the text and are stored directly in Couchbase. When a query is made, Couchbase performs a similarity search by comparing the query vector against the stored document vectors. The similarity metric used for this comparison is configurable, allowing flexibility in how the relevance of documents is determined. Common metrics include cosine similarity, Euclidean distance, or dot product, but other metrics can be implemented based on specific use cases. Different embedding models like BERT, Word2Vec, or GloVe can also be used depending on the application's needs, with the vectors generated by these models stored and searched within Couchbase itself.\n",
    "\n",
    "In the provided code, the search process begins by recording the start time, followed by executing the similarity_search_with_score method of the CouchbaseVectorStore. This method searches Couchbase for the most relevant documents based on the vector similarity to the query. The search results include the document content and a similarity score that reflects how closely each document aligns with the query in the defined semantic space. The time taken to perform this search is then calculated and logged, and the results are displayed, showing the most relevant documents along with their similarity scores. This approach leverages Couchbase as both a storage and retrieval engine for vector data, enabling efficient and scalable semantic searches. The integration of vector storage and search capabilities within Couchbase allows for sophisticated semantic search operations without relying on external services for vector storage or comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic Search Results (completed in 1.56 seconds):\n",
      "Score: 0.5062, Text: Ros Atkins looks at why global food price rises appear inevitable as a consequence of Russia invading Ukraine.\n",
      "Score: 0.4458, Text: The Ukraine war could push millions closer to starvation, the boss of the World Food Programme says.\n",
      "Score: 0.3963, Text: One of the world's biggest fertiliser firms says the conflict could deliver a shock to food supplies.\n",
      "Score: 0.3823, Text: War in Ukraine has focused minds on how to get enough affordable energy for homes and businesses.\n",
      "Score: 0.3450, Text: The global internet is under threat as the conflict in Ukraine continues.\n",
      "Score: 0.3282, Text: After Russia seizes two nuclear plant sites, experts weigh the risk to Ukraine and the world at large.\n",
      "Score: 0.3012, Text: In the month since Russia's military invaded, it has laid waste to cities across Ukraine.\n",
      "Score: 0.3001, Text: The war in Ukraine will lead to an even tighter squeeze on living standards this spring, a think tank says.\n",
      "Score: 0.2941, Text: The growing humanitarian crisis in Ukraine and soaring energy prices lead Tuesday's front pages.\n",
      "Score: 0.2883, Text: President Putin's devastating attack on Ukraine leaves countries in the region worried about their future.\n"
     ]
    }
   ],
   "source": [
    "query = \"What was the impact of the Ukraine war on global food supplies?\"\n",
    "\n",
    "try:\n",
    "    # Perform the semantic search\n",
    "    start_time = time.time()\n",
    "    search_results = vector_store.similarity_search_with_score(query, k=10)\n",
    "    search_elapsed_time = time.time() - start_time\n",
    "\n",
    "    logging.info(f\"Semantic search completed in {search_elapsed_time:.2f} seconds\")\n",
    "\n",
    "    # Display search results\n",
    "    print(f\"\\nSemantic Search Results (completed in {search_elapsed_time:.2f} seconds):\")\n",
    "\n",
    "    for doc, score in search_results:\n",
    "        print(f\"Score: {score:.4f}, Text: {doc.page_content}\")\n",
    "\n",
    "except CouchbaseException as e:\n",
    "    raise RuntimeError(f\"Error performing semantic search: {str(e)}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Unexpected error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) with Couchbase and LangChain\n",
    "Couchbase and LangChain can be seamlessly integrated to create RAG (Retrieval-Augmented Generation) chains, enhancing the process of generating contextually relevant responses. In this setup, Couchbase serves as the vector store, where embeddings of documents are stored. When a query is made, LangChain retrieves the most relevant documents from Couchbase by comparing the query’s embedding with the stored document embeddings. These documents, which provide contextual information, are then passed to a generative language model within LangChain.\n",
    "\n",
    "The language model, equipped with the context from the retrieved documents, generates a response that is both informed and contextually accurate. This integration allows the RAG chain to leverage Couchbase’s efficient storage and retrieval capabilities, while LangChain handles the generation of responses based on the context provided by the retrieved documents. Together, they create a powerful system that can deliver highly relevant and accurate answers by combining the strengths of both retrieval and generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever from vector store\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create RAG prompt template\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that answers questions based on the provided context.\"),\n",
    "    (\"human\", \"Context: {context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# Create RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "logging.info(\"Successfully created RAG chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Response:  The Ukraine war has focused minds on how to get enough affordable energy for homes and businesses.\n",
      "RAG response generated in 3.77 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "rag_response = rag_chain.invoke(query)\n",
    "rag_elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"RAG Response: {rag_response}\")\n",
    "print(f\"RAG response generated in {rag_elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Couchbase as a caching mechanism\n",
    "Couchbase can be effectively used as a caching mechanism for RAG (Retrieval-Augmented Generation) responses by storing and retrieving precomputed results for specific queries. This approach enhances the system's efficiency and speed, particularly when dealing with repeated or similar queries. When a query is first processed, the RAG chain retrieves relevant documents, generates a response using the language model, and then stores this response in Couchbase, with the query serving as the key.\n",
    "\n",
    "For subsequent requests with the same query, the system checks Couchbase first. If a cached response is found, it is retrieved directly from Couchbase, bypassing the need to re-run the entire RAG process. This significantly reduces response time because the computationally expensive steps of document retrieval and response generation are skipped. Couchbase's role in this setup is to provide a fast and scalable storage solution for caching these responses, ensuring that frequently asked queries can be answered more quickly and efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query 1: What was the impact of the Ukraine war on global food supplies?\n",
      "Response:  The Ukraine war has focused minds on how to get enough affordable energy for homes and businesses.\n",
      "Time taken: 1.88 seconds\n",
      "\n",
      "Query 2: How did oil prices change during the Ukraine conflict?\n",
      "Response:  Oil prices have increased due to the Ukraine conflict.\n",
      "Time taken: 3.52 seconds\n",
      "\n",
      "Query 3: What measures did the UK take regarding Ukrainian refugees?\n",
      "Response:  The UK government has announced a new scheme that will allow people in the UK to host Ukrainian refugees.\n",
      "Time taken: 3.42 seconds\n",
      "\n",
      "Query 4: How did oil prices change during the Ukraine conflict?\n",
      "Response:  Oil prices have increased due to the Ukraine conflict.\n",
      "Time taken: 1.84 seconds\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    queries = [\n",
    "        \"What was the impact of the Ukraine war on global food supplies?\",\n",
    "        \"How did oil prices change during the Ukraine conflict?\",\n",
    "        \"What measures did the UK take regarding Ukrainian refugees?\",\n",
    "        \"How did oil prices change during the Ukraine conflict?\",\n",
    "    ]\n",
    "\n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\nQuery {i}: {query}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = rag_chain.invoke(query)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error generating RAG response: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
