{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this guide, we will walk you through building a powerful semantic search engine using Couchbase as the backend database, [Mistral AI](https://mistral.ai/) as the AI-powered embedding Model. Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval. This tutorial is designed to be beginner-friendly, with clear, step-by-step instructions that will equip you with the knowledge to create a fully functional semantic search system from scratch. Alternatively, if you want to perform semantic search using the Search Vector Index, please take a look at [this.](https://developer.couchbase.com/tutorial-mistralai-couchbase-vector-search-with-search-vector-index)\n",
    "\n",
    "Couchbase is a NoSQL distributed document database (JSON) with many of the best features of a relational DBMS: SQL, distributed ACID transactions, and much more. [Couchbase Capella‚Ñ¢](https://cloud.couchbase.com/sign-up) is the easiest way to get started, but you can also download and run [Couchbase Server](http://couchbase.com/downloads) on-premises.\n",
    "\n",
    "Mistral AI is a research lab building the best open source models in the world. La Plateforme enables developers and enterprises to build new products and applications, powered by Mistral's open source and commercial LLMs. \n",
    "\n",
    "The [Mistral AI APIs](https://console.mistral.ai/) empower LLM applications via:\n",
    "\n",
    "- [Text generation](https://docs.mistral.ai/capabilities/completion/), enables streaming and provides the ability to display partial model results in real-time\n",
    "- [Code generation](https://docs.mistral.ai/capabilities/code_generation/), empowers code generation tasks, including fill-in-the-middle and code completion\n",
    "- [Embeddings](https://docs.mistral.ai/capabilities/embeddings/), useful for RAG where it represents the meaning of text as a list of numbers\n",
    "- [Function calling](https://docs.mistral.ai/capabilities/function_calling/), enables Mistral models to connect to external tools\n",
    "- [Fine-tuning](https://docs.mistral.ai/capabilities/finetuning/), enables developers to create customized and specialized models\n",
    "- [JSON mode](https://docs.mistral.ai/capabilities/json_mode/), enables developers to set the response format to json_object\n",
    "- [Guardrailing](https://docs.mistral.ai/capabilities/guardrailing/), enables developers to enforce policies at the system level of Mistral models\n",
    "\n",
    "This tutorial demonstrates how to use Mistral AI's embedding capabilities with Couchbase's **Hyperscale and Composite Vector Indexes** for optimized vector search operations. Hyperscale and Composite Vector Indexes provide superior performance for vector operations compared to traditional search methods, especially for large-scale applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "## Get Credentials for Mistral AI\n",
    "\n",
    "Please follow the [instructions](https://console.mistral.ai/api-keys/) to generate the Mistral AI credentials.\n",
    "\n",
    "## Create and Deploy Your Free Tier Operational cluster on Capella\n",
    "\n",
    "To get started with Couchbase Capella, create an account and use it to deploy a forever free tier operational cluster. This account provides you with a environment where you can explore and learn about Capella with no time constraint.\n",
    "\n",
    "To know more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html).\n",
    "\n",
    "**Note: Hyperscale and Composite Vector Indexes require Couchbase Server 8.0 or above, unlike Search Vector Index which works with 7.6+**\n",
    "\n",
    "### Couchbase Capella Configuration\n",
    "\n",
    "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met.\n",
    "\n",
    "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the travel-sample bucket (Read and Write) used in the application.\n",
    "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: couchbase==4.4.0 in ./venv/lib/python3.12/site-packages (4.4.0)\n",
      "Requirement already satisfied: mistralai==1.9.10 in ./venv/lib/python3.12/site-packages (1.9.10)\n",
      "Requirement already satisfied: langchain-couchbase==0.5.0 in ./venv/lib/python3.12/site-packages (0.5.0)\n",
      "Requirement already satisfied: langchain-core==0.3.76 in ./venv/lib/python3.12/site-packages (0.3.76)\n",
      "Requirement already satisfied: python-dotenv==1.1.1 in ./venv/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: eval-type-backport>=0.2.0 in ./venv/lib/python3.12/site-packages (from mistralai==1.9.10) (0.3.1)\n",
      "Requirement already satisfied: httpx>=0.28.1 in ./venv/lib/python3.12/site-packages (from mistralai==1.9.10) (0.28.1)\n",
      "Requirement already satisfied: invoke<3.0.0,>=2.2.0 in ./venv/lib/python3.12/site-packages (from mistralai==1.9.10) (2.2.1)\n",
      "Requirement already satisfied: pydantic>=2.10.3 in ./venv/lib/python3.12/site-packages (from mistralai==1.9.10) (2.12.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from mistralai==1.9.10) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in ./venv/lib/python3.12/site-packages (from mistralai==1.9.10) (6.0.3)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.12/site-packages (from mistralai==1.9.10) (0.4.2)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in ./venv/lib/python3.12/site-packages (from langchain-core==0.3.76) (0.6.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./venv/lib/python3.12/site-packages (from langchain-core==0.3.76) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./venv/lib/python3.12/site-packages (from langchain-core==0.3.76) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./venv/lib/python3.12/site-packages (from langchain-core==0.3.76) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in ./venv/lib/python3.12/site-packages (from langchain-core==0.3.76) (26.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.3.76) (3.0.0)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.12/site-packages (from httpx>=0.28.1->mistralai==1.9.10) (4.12.1)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx>=0.28.1->mistralai==1.9.10) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx>=0.28.1->mistralai==1.9.10) (1.0.9)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.12/site-packages (from httpx>=0.28.1->mistralai==1.9.10) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.28.1->mistralai==1.9.10) (0.16.0)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core==0.3.76) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core==0.3.76) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in ./venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core==0.3.76) (2.32.5)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in ./venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core==0.3.76) (0.14.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./venv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core==0.3.76) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.12/site-packages (from pydantic>=2.10.3->mistralai==1.9.10) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./venv/lib/python3.12/site-packages (from pydantic>=2.10.3->mistralai==1.9.10) (2.41.5)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->mistralai==1.9.10) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core==0.3.76) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core==0.3.76) (2.6.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install couchbase==4.4.0 mistralai==1.9.10 langchain-couchbase==0.5.0 langchain-core==0.3.76 python-dotenv==1.1.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "from mistralai import Mistral\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.options import ClusterOptions\n",
    "from langchain_couchbase.vectorstores import CouchbaseQueryVectorStore\n",
    "from langchain_couchbase.vectorstores import DistanceStrategy, IndexType\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Cluster URL: couchbases://cb.b96outuetbo8z5l9.cloud.couchbase.com\n",
      "Couchbase username: Administrator\n",
      "Couchbase password: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
      "Couchbase bucket: color-vector-sample\n",
      "Couchbase scope: color\n",
      "Couchbase collection: rgb\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "## Load environment variables from .env file if it exists\n",
    "load_dotenv()\n",
    "\n",
    "## Load from environment variables or prompt for input\n",
    "CB_HOST = os.getenv('CB_HOST') or input(\"Cluster URL:\")\n",
    "CB_USERNAME = os.getenv('CB_USERNAME') or input(\"Couchbase username:\")\n",
    "CB_PASSWORD = os.getenv('CB_PASSWORD') or getpass.getpass(\"Couchbase password:\")\n",
    "CB_BUCKET_NAME = os.getenv('CB_BUCKET_NAME') or input(\"Couchbase bucket:\")\n",
    "SCOPE_NAME = os.getenv('SCOPE_NAME') or input(\"Couchbase scope:\")\n",
    "COLLECTION_NAME = os.getenv('COLLECTION_NAME') or input(\"Couchbase collection:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Couchbase Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = PasswordAuthenticator(\n",
    "    CB_USERNAME,\n",
    "    CB_PASSWORD\n",
    ")\n",
    "\n",
    "options = ClusterOptions(auth)\n",
    "options.apply_profile(\"wan_development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = Cluster(CB_HOST, options)\n",
    "cluster.wait_until_ready(timedelta(seconds=40))\n",
    "\n",
    "bucket = cluster.bucket(CB_BUCKET_NAME)\n",
    "scope = bucket.scope(SCOPE_NAME)\n",
    "collection = scope.collection(COLLECTION_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Collections in Couchbase\n",
    "\n",
    "The setup_collection() function handles creating and configuring the hierarchical data organization in Couchbase:\n",
    "\n",
    "1. Bucket Creation:\n",
    "   - Checks if specified bucket exists, creates it if not\n",
    "   - Sets bucket properties like RAM quota (1024MB) and replication (disabled)\n",
    "   - Note: You will not be able to create a bucket on Capella\n",
    "\n",
    "2. Scope Management:  \n",
    "   - Verifies if requested scope exists within bucket\n",
    "   - Creates new scope if needed (unless it's the default \"_default\" scope)\n",
    "\n",
    "3. Collection Setup:\n",
    "   - Checks for collection existence within scope\n",
    "   - Creates collection if it doesn't exist\n",
    "   - Waits 2 seconds for collection to be ready\n",
    "\n",
    "Additional Tasks:\n",
    "- Clears any existing documents for clean state\n",
    "- Implements comprehensive error handling and logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<couchbase.collection.Collection at 0x1118984a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup_collection(cluster, bucket_name, scope_name, collection_name):\n",
    "    try:\n",
    "        ## Check if bucket exists, create if it doesn't\n",
    "        try:\n",
    "            bucket = cluster.bucket(bucket_name)\n",
    "        except Exception as e:\n",
    "            bucket_settings = CreateBucketSettings(\n",
    "                name=bucket_name,\n",
    "                bucket_type='couchbase',\n",
    "                ram_quota_mb=1024,\n",
    "                flush_enabled=True,\n",
    "                num_replicas=0\n",
    "            )\n",
    "            cluster.buckets().create_bucket(bucket_settings)\n",
    "            time.sleep(2)  # Wait for bucket creation to complete and become available\n",
    "            bucket = cluster.bucket(bucket_name)\n",
    "\n",
    "        bucket_manager = bucket.collections()\n",
    "\n",
    "        ## Check if scope exists, create if it doesn't\n",
    "        scopes = bucket_manager.get_all_scopes()\n",
    "        scope_exists = any(scope.name == scope_name for scope in scopes)\n",
    "        \n",
    "        if not scope_exists and scope_name != \"_default\":\n",
    "            bucket_manager.create_scope(scope_name)\n",
    "\n",
    "        ## Check if collection exists, create if it doesn't\n",
    "        collections = bucket_manager.get_all_scopes()\n",
    "        collection_exists = any(\n",
    "            scope.name == scope_name and collection_name in [col.name for col in scope.collections]\n",
    "            for scope in collections\n",
    "        )\n",
    "\n",
    "        if not collection_exists:\n",
    "            bucket_manager.create_collection(scope_name, collection_name)\n",
    "\n",
    "        ## Wait for collection to be ready\n",
    "        collection = bucket.scope(scope_name).collection(collection_name)\n",
    "        time.sleep(2)  # Give the collection time to be ready for queries\n",
    "\n",
    "        ## Clear all documents in the collection\n",
    "        try:\n",
    "            query = f\"DELETE FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
    "            cluster.query(query).execute()\n",
    "        except Exception as e:\n",
    "            print(f\"Error while clearing documents: {str(e)}. The collection might be empty.\")\n",
    "\n",
    "        return collection\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error setting up collection: {str(e)}\")\n",
    "    \n",
    "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Mistral AI Embeddings Wrapper\n",
    "\n",
    "Since Mistral AI doesn't have native LangChain integration, we need to create a custom wrapper class that implements the LangChain Embeddings interface. This will allow us to use Mistral AI's embedding model with Couchbase's Hyperscale and Composite vector indexes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralAIEmbeddings(Embeddings):\n",
    "    \"\"\"Custom Mistral AI Embeddings wrapper for LangChain compatibility.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"mistral-embed\"):\n",
    "        self.client = Mistral(api_key=api_key)\n",
    "        self.model = model\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed search docs.\"\"\"\n",
    "        try:\n",
    "            response = self.client.embeddings.create(\n",
    "                model=self.model,\n",
    "                inputs=texts,\n",
    "            )\n",
    "            return [embedding.embedding for embedding in response.data]\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error generating embeddings: {str(e)}\")\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Embed query text.\"\"\"\n",
    "        try:\n",
    "            response = self.client.embeddings.create(\n",
    "                model=self.model,\n",
    "                inputs=[text],\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error generating query embedding: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Mistral API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "MISTRAL_API_KEY = os.getenv('MISTRAL_API_KEY') or getpass.getpass(\"Mistral API Key:\")\n",
    "embeddings = MistralAIEmbeddings(api_key=MISTRAL_API_KEY, model=\"mistral-embed\")\n",
    "mistral_client = Mistral(api_key=MISTRAL_API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Hyperscale and Composite Vector Indexes\n",
    "\n",
    "### Optimizing Vector Search with Hyperscale and Composite Vector Indexes\n",
    "\n",
    "With Couchbase 8.0+, you can leverage the power of Hyperscale and Composite Vector Indexes, which offer significant performance improvements over Search Vector Index approaches for vector-first workloads. These indexes provide high-performance vector similarity search with advanced filtering capabilities and are designed to scale to billions of vectors.\n",
    "\n",
    "#### Hyperscale and Composite Vector Indexes vs Search Vector Indexes: Choosing the Right Approach\n",
    "\n",
    "| Feature               | Hyperscale and Composite Vector Index                                               | Search Vector Index                       |\n",
    "| --------------------- | --------------------------------------------------------------- | ----------------------------------------- |\n",
    "| **Best For**          | Vector-first workloads, complex filtering, high QPS performance| Hybrid search and high recall rates      |\n",
    "| **Couchbase Version** | 8.0.0+                                                         | 7.6+                                      |\n",
    "| **Filtering**         | Pre-filtering with `WHERE` clauses (Composite) or post-filtering (Hyperscale) | Pre-filtering with flexible ordering |\n",
    "| **Scalability**       | Up to billions of vectors (Hyperscale)                              | Up to 10 million vectors                  |\n",
    "| **Performance**       | Optimized for concurrent operations with low memory footprint  | Good for mixed text and vector queries   |\n",
    "\n",
    "\n",
    "#### Vector Index Types\n",
    "\n",
    "Couchbase offers two distinct query-based vector index types, each optimized for different use cases:\n",
    "\n",
    "##### Hyperscale Vector Indexes \n",
    "\n",
    "- **Best for**: Pure vector searches like content discovery, recommendations, and semantic search\n",
    "- **Use when**: You primarily perform vector-only queries without complex scalar filtering\n",
    "- **Features**: \n",
    "  - High performance with low memory footprint\n",
    "  - Optimized for concurrent operations\n",
    "  - Designed to scale to billions of vectors\n",
    "  - Supports post-scan filtering for basic metadata filtering\n",
    "\n",
    "##### Composite Vector Indexes\n",
    "\n",
    "  - **Best for**: Filtered vector searches that combine vector similarity with scalar value filtering\n",
    "- **Use when**: Your queries combine vector similarity with scalar filters that eliminate large portions of data\n",
    "- **Features**: \n",
    "  - Efficient pre-filtering where scalar attributes reduce the vector comparison scope\n",
    "  - Best for well-defined workloads requiring complex filtering using Composite Vector Index features\n",
    "  - Supports range lookups combined with vector search\n",
    "\n",
    "#### Index Type Selection for This Tutorial\n",
    "\n",
    "In this tutorial, we'll demonstrate creating a **Hyperscale Vector Index** and running vector similarity queries. Hyperscale Vector Index is ideal for semantic search scenarios where you want:\n",
    "\n",
    "1. **High-performance vector search** across large datasets\n",
    "2. **Low latency** for real-time applications\n",
    "3. **Scalability** to handle growing vector collections\n",
    "4. **Concurrent operations** for multi-user environments\n",
    "\n",
    "The Hyperscale Vector Index will provide optimal performance for our OpenAI embedding-based semantic search implementation.\n",
    "\n",
    "#### Alternative: Composite Vector Index\n",
    "\n",
    "If your use case requires complex filtering with scalar attributes, you may want to consider using a **Composite Vector Index** instead:\n",
    "\n",
    "```python\n",
    "## Alternative: Create a Composite index for filtered searches\n",
    "vector_store.create_index(\n",
    "    index_type=IndexType.COMPOSITE,\n",
    "    index_description=\"IVF,SQ8\",\n",
    "    distance_metric=DistanceStrategy.COSINE,\n",
    "    index_name=\"pydantic_composite_index\",\n",
    ")\n",
    "```\n",
    "\n",
    "**Use Composite indexes when:**\n",
    "- You need to filter by document metadata or attributes before vector similarity\n",
    "- Your queries combine vector search with WHERE clauses\n",
    "- You have well-defined filtering requirements that can reduce the search space\n",
    "\n",
    "**Note**: Composite indexes enable pre-filtering with scalar attributes, making them ideal for applications where you need to search within specific categories, date ranges, or user-specific data segments.\n",
    "\n",
    "#### Understanding Index Configuration (Couchbase 8.0 Feature)\n",
    "\n",
    "Before creating our Hyperscale index, it's important to understand the configuration parameters that optimize vector storage and search performance. The `index_description` parameter controls how Couchbase optimizes vector storage through centroids and quantization.\n",
    "\n",
    "##### Index Description Format: `'IVF[<centroids>],{PQ|SQ}<settings>'`\n",
    "\n",
    "##### Centroids (IVF - Inverted File)\n",
    "\n",
    "- Controls how the dataset is subdivided for faster searches\n",
    "- **More centroids** = faster search, slower training time\n",
    "- **Fewer centroids** = slower search, faster training time\n",
    "- If omitted (like `IVF,SQ8`), Couchbase auto-selects based on dataset size\n",
    "\n",
    "###### Quantization Options\n",
    "\n",
    "**Scalar Quantization (SQ):**\n",
    "- `SQ4`, `SQ6`, `SQ8` (4, 6, or 8 bits per dimension)\n",
    "- Lower memory usage, faster search, slightly reduced accuracy\n",
    "\n",
    "**Product Quantization (PQ):**\n",
    "- Format: `PQ<subquantizers>x<bits>` (e.g., `PQ32x8`)\n",
    "- Better compression for very large datasets\n",
    "- More complex but can maintain accuracy with smaller index size\n",
    "\n",
    "##### Common Configuration Examples\n",
    "\n",
    "- **`IVF,SQ8`** - Auto centroids, 8-bit scalar quantization (good default)\n",
    "- **`IVF1000,SQ6`** - 1000 centroids, 6-bit scalar quantization\n",
    "- **`IVF,PQ32x8`** - Auto centroids, 32 subquantizers with 8 bits\n",
    "\n",
    "For detailed configuration options, see the [Quantization & Centroid Settings](https://docs.couchbase.com/cloud/vector-index/hyperscale-vector-index.html#algo_settings).\n",
    "\n",
    "For more information on Hyperscale and Composite Vector Indexes, see [Couchbase Hyperscale and Composite Vector Indexes Documentation](https://docs.couchbase.com/cloud/vector-index/use-vector-indexes.html).\n",
    "\n",
    "##### Our Configuration Choice\n",
    "\n",
    "In this tutorial, we use `IVF,SQ8` which provides:\n",
    "- **Auto-selected centroids** optimized for our dataset size\n",
    "- **8-bit scalar quantization** for good balance of speed, memory usage, and accuracy\n",
    "- **COSINE distance metric** ideal for semantic similarity search\n",
    "- **Optimal performance** for most semantic search use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Couchbase Hyperscale Vector Store\n",
    "\n",
    "Instead of using FTS (Full-Text Search), we'll use Couchbase's Hyperscale Index for vector operations. Hyperscale Indexes provide better performance for vector search operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperscale Vector Store created successfully!\n"
     ]
    }
   ],
   "source": [
    "vector_store = CouchbaseQueryVectorStore(\n",
    "    cluster=cluster,\n",
    "    bucket_name=CB_BUCKET_NAME,\n",
    "    scope_name=SCOPE_NAME,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embeddings,\n",
    "    distance_metric=DistanceStrategy.COSINE\n",
    ")\n",
    "\n",
    "print(\"Hyperscale Vector Store created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Documents\n",
    "\n",
    "Mistral client can be used to generate vector embeddings for given text fragments. These embeddings represent the sentiment of corresponding fragments and can be stored in Couchbase for further retrieval. A custom embedding text can also be added into the embedding texts array by running this code block:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "custom embedding text Couchbase is commonly used to store and search embeddings for GenAI and retrieval augmented generation applications.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents added to Hyperscale Vector store successfully!\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON's versatility, with a foundation that is extremely fast and scalable.\",\n",
    "    \"It's used across industries for things like user profiles, dynamic product catalogs, GenAI apps, vector search, high-speed caching, and much more.\",\n",
    "    input(\"custom embedding text\")\n",
    "]\n",
    "\n",
    "# Store documents in the Hyperscale Vector store\n",
    "vector_store.add_texts(texts)\n",
    "\n",
    "print(\"Documents added to Hyperscale Vector store successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Semantic Search in Couchbase\n",
    "\n",
    "Semantic search goes beyond traditional keyword matching by understanding the meaning and context behind queries. Here's how it works in Couchbase:\n",
    "\n",
    "## How Semantic Search Works\n",
    "\n",
    "1. **Vector Embeddings**: Documents and queries are converted into high-dimensional vectors using an embeddings model (in our case, Mistral AI's mistral-embed)\n",
    "\n",
    "2. **Similarity Calculation**: When a query is made, Couchbase compares the query vector against stored document vectors using the COSINE distance metric\n",
    "\n",
    "3. **Result Ranking**: Documents are ranked by their vector distance (lower distance = more similar meaning)\n",
    "\n",
    "4. **Flexible Configuration**: Different distance metrics (cosine, euclidean, dot product) and embedding models can be used based on your needs\n",
    "\n",
    "The `similarity_search_with_score` method performs this entire process, returning documents along with their similarity scores. This enables you to find semantically related content even when exact keywords don't match.\n",
    "\n",
    "Now let's see semantic search in action and measure its performance with different optimization strategies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Search Performance Optimization\n",
    "\n",
    "Now let's measure and compare the performance benefits of different optimization strategies. We'll conduct a comprehensive performance analysis across two phases:\n",
    "\n",
    "## Performance Testing Phases\n",
    "\n",
    "1. **Phase 1 - Baseline Performance**: Test vector search without Hyperscale & Composite Vector indexes to establish baseline metrics\n",
    "\n",
    "2. **Phase 2 - Hyperscale & Composite Vector Index-Optimized Search**: Create Hyperscale index and measure performance improvements\n",
    "\n",
    "**Important Context:**\n",
    "\n",
    "- Hyperscale & Composite Vector Index performance benefits scale with dataset size and concurrent load\n",
    "- With our dataset (~3 documents), improvements may be modest\n",
    "- Production environments with millions of vectors show significant Hyperscale and Composite Vector Index advantages\n",
    "- The combination of Hyperscale and Composite Vector Index + embeddings provides optimal semantic search performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Baseline Performance (Without Hyperscale Vector Index or Composite Vector Index)\n",
    "\n",
    "First, let's test the search performance without Hyperscale Vector index or Composite Vector Index. This will help us establish a baseline for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 1: BASELINE PERFORMANCE (Without Hyperscale or Composite Vector Index)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 12:17:21,919 - INFO - HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 12:17:22,607 - INFO - Baseline search completed in 0.98 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Search Results (completed in 0.9779 seconds):\n",
      "--------------------------------------------------------------------------------\n",
      "[Result 1] Vector Distance: 0.2870\n",
      "Text: Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON'...\n",
      "--------------------------------------------------------------------------------\n",
      "[Result 2] Vector Distance: 0.3484\n",
      "Text: It's used across industries for things like user profiles, dynamic product catalogs, GenAI apps, vector search, high-speed caching, and much more.\n",
      "--------------------------------------------------------------------------------\n",
      "[Result 3] Vector Distance: 0.3865\n",
      "Text: Couchbase is commonly used to store and search embeddings for GenAI and retrieval augmented generation applications.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "## Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Phase 1: Baseline Performance (Without Hyperscale or Composite Vector Index)\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 1: BASELINE PERFORMANCE (Without Hyperscale or Composite Vector Index)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "query = \"name a multipurpose database with distributed capability\"\n",
    "\n",
    "try:\n",
    "    # Perform the semantic search\n",
    "    start_time = time.time()\n",
    "    search_results = vector_store.similarity_search_with_score(query, k=3)\n",
    "    baseline_time = time.time() - start_time\n",
    "\n",
    "    logging.info(f\"Baseline search completed in {baseline_time:.2f} seconds\")\n",
    "\n",
    "    # Display search results\n",
    "    print(f\"\\nBaseline Search Results (completed in {baseline_time:.4f} seconds):\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, (doc, distance) in enumerate(search_results, 1):\n",
    "        print(f\"[Result {i}] Vector Distance: {distance:.4f}\")\n",
    "        # Truncate for readability\n",
    "        content_preview = doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
    "        print(f\"Text: {content_preview}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error performing semantic search: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Hyperscale Vector Index-Optimized Performance \n",
    "\n",
    "Now let's create a Hyperscale index and measure the performance improvements when searching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Hyperscale index for index optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 12:18:39,480 - INFO - HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperscale index created successfully!\n"
     ]
    }
   ],
   "source": [
    "## Create a Hyperscale index for optimal vector search performance\n",
    "print(\"\\nCreating Hyperscale index for index optimization...\")\n",
    "vector_store.create_index(\n",
    "    index_type=IndexType.BHIVE, \n",
    "    index_name=\"mistral_hyperscale_index_optimized\",\n",
    "    index_description=\"IVF,SQ8\"\n",
    ")\n",
    "print(\"Hyperscale index created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To create a COMPOSITE index, the below code can be used.\n",
    "Choose based on your specific use case and query patterns. For this tutorial's news search scenario, either index type would work, but Hyperscale might be more efficient for pure semantic search across news articles.\n",
    "\n",
    "vector_store.create_index(index_type=IndexType.COMPOSITE, index_name=\"pydantic_ai_composite_index\", index_description=\"IVF,SQ8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 2: Hyperscale Index-OPTIMIZED PERFORMANCE \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-02 12:18:45,737 - INFO - HTTP Request: POST https://api.mistral.ai/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2026-02-02 12:18:46,873 - INFO - Hyperscale Vector index-optimized search completed in 1.51 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hyperscale Vector index-Optimized Search Results (completed in 1.5112 seconds):\n",
      "--------------------------------------------------------------------------------\n",
      "[Result 1] Vector Distance: 0.2870\n",
      "Text: Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON'...\n",
      "--------------------------------------------------------------------------------\n",
      "[Result 2] Vector Distance: 0.3484\n",
      "Text: It's used across industries for things like user profiles, dynamic product catalogs, GenAI apps, vector search, high-speed caching, and much more.\n",
      "--------------------------------------------------------------------------------\n",
      "[Result 3] Vector Distance: 0.3865\n",
      "Text: Couchbase is commonly used to store and search embeddings for GenAI and retrieval augmented generation applications.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Hyperscale Index-Optimized Performance\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: Hyperscale Index-OPTIMIZED PERFORMANCE \")\n",
    "print(\"=\"*80)\n",
    "\n",
    "query = \"name a multipurpose database with distributed capability\"\n",
    "\n",
    "try:\n",
    "    # Perform the semantic search with Hyperscale Vector Index\n",
    "    start_time = time.time()\n",
    "    search_results = vector_store.similarity_search_with_score(query, k=3)\n",
    "    hyperscale_time = time.time() - start_time\n",
    "\n",
    "    logging.info(f\"Hyperscale Vector index-optimized search completed in {hyperscale_time:.2f} seconds\")\n",
    "\n",
    "    # Display search results\n",
    "    print(f\"\\nHyperscale Vector index-Optimized Search Results (completed in {hyperscale_time:.4f} seconds):\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, (doc, distance) in enumerate(search_results, 1):\n",
    "        print(f\"[Result {i}] Vector Distance: {distance:.4f}\")\n",
    "        # Truncate for readability\n",
    "        content_preview = doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content\n",
    "        print(f\"Text: {content_preview}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error performing semantic search: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Summary\n",
    "\n",
    "Let's analyze the performance improvements achieved through Hyperscale Vector Index optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VECTOR SEARCH PERFORMANCE OPTIMIZATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìä Performance Comparison:\n",
      "Optimization Level                  Time (seconds)       Status\n",
      "--------------------------------------------------------------------------------\n",
      "Phase 1 - Baseline (No Index)       0.9779                 ‚ö™ Baseline\n",
      "Phase 2 - Hyperscale & Composite Vector Index-Optimized (Hyperscale) 1.5112                 ‚úÖ Optimized\n",
      "\n",
      "‚ö†Ô∏è  Note: Hyperscale Vector Index was 54.5% slower than baseline in this run\n",
      "   This can happen with small datasets. Hyperscale Vector Index benefits emerge with scale.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "KEY INSIGHTS:\n",
      "--------------------------------------------------------------------------------\n",
      "1. üöÄ Hyperscale & Composite Vector Index Optimization:\n",
      "   ‚Ä¢ Hyperscale vector indexes excel with large-scale datasets (millions+ vectors)\n",
      "   ‚Ä¢ Performance gains increase with dataset size and concurrent queries\n",
      "   ‚Ä¢ Optimal for production workloads with sustained traffic patterns\n",
      "\n",
      "2. üì¶ Dataset Size Impact:\n",
      "   ‚Ä¢ Current dataset: ~3 sample documents\n",
      "   ‚Ä¢ At this scale, performance differences may be minimal or variable\n",
      "   ‚Ä¢ Significant gains typically seen with 10M+ vectors\n",
      "\n",
      "3. üéØ When to Use Hyperscale & Composite Vector Index:\n",
      "   ‚Ä¢ Large-scale vector search applications\n",
      "   ‚Ä¢ High query-per-second (QPS) requirements\n",
      "   ‚Ä¢ Multi-user concurrent access scenarios\n",
      "   ‚Ä¢ Production environments requiring scalability\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VECTOR SEARCH PERFORMANCE OPTIMIZATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Performance Comparison:\")\n",
    "print(f\"{'Optimization Level':<35} {'Time (seconds)':<20} {'Status'}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Phase 1 - Baseline (No Index)':<35} {baseline_time:.4f}{'':16} ‚ö™ Baseline\")\n",
    "print(f\"{'Phase 2 - Hyperscale & Composite Vector Index-Optimized (Hyperscale)':<35} {hyperscale_time:.4f}{'':16} ‚úÖ Optimized\")\n",
    "\n",
    "# Calculate improvement\n",
    "if baseline_time > hyperscale_time:\n",
    "    speedup = baseline_time / hyperscale_time\n",
    "    improvement = ((baseline_time - hyperscale_time) / baseline_time) * 100\n",
    "    print(f\"\\n‚ú® Index Performance Gain: {speedup:.2f}x faster ({improvement:.1f}% improvement)\")\n",
    "elif hyperscale_time > baseline_time:\n",
    "    slowdown_pct = ((hyperscale_time - baseline_time) / baseline_time) * 100\n",
    "    print(f\"\\n‚ö†Ô∏è  Note: Hyperscale Vector Index was {slowdown_pct:.1f}% slower than baseline in this run\")\n",
    "    print(f\"   This can happen with small datasets. Hyperscale Vector Index benefits emerge with scale.\")\n",
    "else:\n",
    "    print(f\"\\n‚öñÔ∏è  Performance: Comparable to baseline\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"-\"*80)\n",
    "print(\"1. üöÄ Hyperscale & Composite Vector Index Optimization:\")\n",
    "print(\"   ‚Ä¢ Hyperscale vector indexes excel with large-scale datasets (millions+ vectors)\")\n",
    "print(\"   ‚Ä¢ Performance gains increase with dataset size and concurrent queries\")\n",
    "print(\"   ‚Ä¢ Optimal for production workloads with sustained traffic patterns\")\n",
    "\n",
    "print(\"\\n2. üì¶ Dataset Size Impact:\")\n",
    "print(f\"   ‚Ä¢ Current dataset: ~3 sample documents\")\n",
    "print(\"   ‚Ä¢ At this scale, performance differences may be minimal or variable\")\n",
    "print(\"   ‚Ä¢ Significant gains typically seen with 10M+ vectors\")\n",
    "\n",
    "print(\"\\n3. üéØ When to Use Hyperscale & Composite Vector Index:\")\n",
    "print(\"   ‚Ä¢ Large-scale vector search applications\")\n",
    "print(\"   ‚Ä¢ High query-per-second (QPS) requirements\")\n",
    "print(\"   ‚Ä¢ Multi-user concurrent access scenarios\")\n",
    "print(\"   ‚Ä¢ Production environments requiring scalability\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This tutorial demonstrated how to use Mistral AI's embedding capabilities with Couchbase's Hyperscale and Composite Vector search, including comprehensive performance analysis. The combination of Mistral AI's embeddings and Couchbase's Hyperscale and Composite Vector Index provides a powerful, scalable foundation for building intelligent search applications.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
