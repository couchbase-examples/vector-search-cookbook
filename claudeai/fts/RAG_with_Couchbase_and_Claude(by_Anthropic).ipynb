{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNdImxzypDlm"
      },
      "source": [
        "# Introduction\n",
        "In this guide, we will walk you through building a powerful semantic search engine using Couchbase as the backend database, [OpenAI](https://openai.com/) as the AI-powered embedding and [Anthropic](https://claude.ai/) as the language model provider. Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval. This tutorial is designed to be beginner-friendly, with clear, step-by-step instructions that will equip you with the knowledge to create a fully functional semantic search system using the FTS service from scratch. Alternatively if you want to perform semantic search using the GSI index, please take a look at [this.](https://developer.couchbase.com/tutorial-openai-claude-couchbase-rag-with-global-secondary-index/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How to run this tutorial\n",
        "\n",
        "This tutorial is available as a Jupyter Notebook (`.ipynb` file) that you can run interactively. You can access the original notebook [here](https://github.com/couchbase-examples/vector-search-cookbook/blob/main/claudeai/fts/RAG_with_Couchbase_and_Claude(by_Anthropic).ipynb).\n",
        "\n",
        "You can either download the notebook file and run it on [Google Colab](https://colab.research.google.com/) or run it on your system by setting up the Python environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Before you start\n",
        "\n",
        "## Get Credentials for OpenAI and Anthropic\n",
        "\n",
        "* Please follow the [instructions](https://platform.openai.com/docs/quickstart) to generate the OpenAI credentials.\n",
        "* Please follow the [instructions](https://docs.anthropic.com/en/api/getting-started) to generate the Anthropic credentials.\n",
        "\n",
        "## Create and Deploy Your Free Tier Operational cluster on Capella\n",
        "\n",
        "To get started with Couchbase Capella, create an account and use it to deploy a forever free tier operational cluster. This account provides you with an environment where you can explore and learn about Capella with no time constraint.\n",
        "\n",
        "To learn more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html).\n",
        "\n",
        "### Couchbase Capella Configuration\n",
        "\n",
        "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met.\n",
        "\n",
        "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the required bucket (Read and Write) used in the application.\n",
        "\n",
        "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH2o6pqa69oG"
      },
      "source": [
        "# Setting the Stage: Installing Necessary Libraries\n",
        "To build our semantic search engine, we need a robust set of tools. The libraries we install handle everything from connecting to databases to performing complex machine learning tasks. Each library has a specific role: Couchbase libraries manage database operations, LangChain handles AI model integrations, and OpenAI provides advanced AI models for generating embeddings and Claude(by Anthropic) for understanding natural language. By setting up these libraries, we ensure our environment is equipped to handle the data-intensive and computationally complex tasks required for semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DYhPj0Ta8l_A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --quiet datasets==3.5.0 langchain-couchbase==0.3.0 langchain-anthropic==0.3.11 langchain-openai==0.3.13 python-dotenv==1.1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pp7GtNg8mB9"
      },
      "source": [
        "# Importing Necessary Libraries\n",
        "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading. These libraries provide essential functions for working with data, managing database connections, and processing machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8GzS6tfL8mFP"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "from datetime import timedelta\n",
        "from multiprocessing import AuthenticationError\n",
        "\n",
        "from couchbase.auth import PasswordAuthenticator\n",
        "from couchbase.cluster import Cluster\n",
        "from couchbase.exceptions import (CouchbaseException,\n",
        "                                  InternalServerFailureException,\n",
        "                                  QueryIndexAlreadyExistsException,\n",
        "                                  ServiceUnavailableException)\n",
        "from couchbase.management.buckets import CreateBucketSettings\n",
        "from couchbase.management.search import SearchIndex\n",
        "from couchbase.options import ClusterOptions\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.globals import set_llm_cache\n",
        "from langchain_core.prompts.chat import (ChatPromptTemplate,\n",
        "                                         HumanMessagePromptTemplate,\n",
        "                                         SystemMessagePromptTemplate)\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_couchbase.cache import CouchbaseCache\n",
        "from langchain_couchbase.vectorstores import CouchbaseSearchVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBnMp5vb8mIb"
      },
      "source": [
        "# Setup Logging\n",
        "Logging is configured to track the progress of the script and capture any errors or warnings. This is crucial for debugging and understanding the flow of execution. The logging output includes timestamps, log levels (e.g., INFO, ERROR), and messages that describe what is happening in the script.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Yv8kWcuf8mLx"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)\n",
        "\n",
        "# Disable all logging except critical to prevent OpenAI API request logs\n",
        "logging.getLogger(\"httpx\").setLevel(logging.CRITICAL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9G5a0en8mPA"
      },
      "source": [
        "# Loading Sensitive Informnation\n",
        "In this section, we prompt the user to input essential configuration settings needed. These settings include sensitive information like API keys, database credentials, and specific configuration names. Instead of hardcoding these details into the script, we request the user to provide them at runtime, ensuring flexibility and security.\n",
        "\n",
        "The script also validates that all required inputs are provided, raising an error if any crucial information is missing. This approach ensures that your integration is both secure and correctly configured without hardcoding sensitive information, enhancing the overall security and maintainability of your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PFGyHll18mSe"
      },
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "\n",
        "# Load from environment variables or prompt for input in one-liners\n",
        "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY') or getpass.getpass('Enter your Anthropic API key: ')\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or getpass.getpass('Enter your OpenAI API key: ')\n",
        "CB_HOST = os.getenv('CB_HOST', 'couchbase://localhost') or input('Enter your Couchbase host (default: couchbase://localhost): ') or 'couchbase://localhost'\n",
        "CB_USERNAME = os.getenv('CB_USERNAME', 'Administrator') or input('Enter your Couchbase username (default: Administrator): ') or 'Administrator'\n",
        "CB_PASSWORD = os.getenv('CB_PASSWORD', 'password') or getpass.getpass('Enter your Couchbase password (default: password): ') or 'password'\n",
        "CB_BUCKET_NAME = os.getenv('CB_BUCKET_NAME', 'vector-search-testing') or input('Enter your Couchbase bucket name (default: vector-search-testing): ') or 'vector-search-testing'\n",
        "INDEX_NAME = os.getenv('INDEX_NAME', 'vector_search_claude') or input('Enter your index name (default: vector_search_claude): ') or 'vector_search_claude'\n",
        "SCOPE_NAME = os.getenv('SCOPE_NAME', 'shared') or input('Enter your scope name (default: shared): ') or 'shared'\n",
        "COLLECTION_NAME = os.getenv('COLLECTION_NAME', 'claude') or input('Enter your collection name (default: claude): ') or 'claude'\n",
        "CACHE_COLLECTION = os.getenv('CACHE_COLLECTION', 'cache') or input('Enter your cache collection name (default: cache): ') or 'cache'\n",
        "# Check if the variables are correctly loaded\n",
        "if not ANTHROPIC_API_KEY:\n",
        "    raise ValueError(\"ANTHROPIC_API_KEY is not set in the environment.\")\n",
        "if not OPENAI_API_KEY:\n",
        "    raise ValueError(\"OPENAI_API_KEY is not set in the environment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtGrYzUY8mV3"
      },
      "source": [
        "# Connecting to the Couchbase Cluster\n",
        "Connecting to a Couchbase cluster is the foundation of our project. Couchbase will serve as our primary data store, handling all the storage and retrieval operations required for our semantic search engine. By establishing this connection, we enable our application to interact with the database, allowing us to perform operations such as storing embeddings, querying data, and managing collections. This connection is the gateway through which all data will flow, so ensuring it's set up correctly is paramount.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Zb3kK-7W8mZK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-25 21:48:21,579 - INFO - Successfully connected to Couchbase\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
        "    options = ClusterOptions(auth)\n",
        "    cluster = Cluster(CB_HOST, options)\n",
        "    cluster.wait_until_ready(timedelta(seconds=5))\n",
        "    logging.info(\"Successfully connected to Couchbase\")\n",
        "except Exception as e:\n",
        "    raise ConnectionError(f\"Failed to connect to Couchbase: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Gpy32N8mcZ"
      },
      "source": [
        "## Setting Up Collections in Couchbase\n",
        "\n",
        "The setup_collection() function handles creating and configuring the hierarchical data organization in Couchbase:\n",
        "\n",
        "1. Bucket Creation:\n",
        "   - Checks if specified bucket exists, creates it if not\n",
        "   - Sets bucket properties like RAM quota (1024MB) and replication (disabled)\n",
        "   - Note: You will not be able to create a bucket on Capella\n",
        "\n",
        "\n",
        "2. Scope Management:  \n",
        "   - Verifies if requested scope exists within bucket\n",
        "   - Creates new scope if needed (unless it's the default \"_default\" scope)\n",
        "\n",
        "3. Collection Setup:\n",
        "   - Checks for collection existence within scope\n",
        "   - Creates collection if it doesn't exist\n",
        "   - Waits 2 seconds for collection to be ready\n",
        "\n",
        "Additional Tasks:\n",
        "- Creates primary index on collection for query performance\n",
        "- Clears any existing documents for clean state\n",
        "- Implements comprehensive error handling and logging\n",
        "\n",
        "The function is called twice to set up:\n",
        "1. Main collection for vector embeddings\n",
        "2. Cache collection for storing results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ACZcwUnG8mf2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-25 21:48:28,237 - INFO - Bucket 'vector-search-testing' does not exist. Creating it...\n",
            "2025-02-25 21:48:28,800 - INFO - Bucket 'vector-search-testing' created successfully.\n",
            "2025-02-25 21:48:28,802 - INFO - Scope 'shared' does not exist. Creating it...\n",
            "2025-02-25 21:48:28,851 - INFO - Scope 'shared' created successfully.\n",
            "2025-02-25 21:48:28,855 - INFO - Collection 'claude' does not exist. Creating it...\n",
            "2025-02-25 21:48:28,943 - INFO - Collection 'claude' created successfully.\n",
            "2025-02-25 21:48:32,802 - INFO - Primary index present or created successfully.\n",
            "2025-02-25 21:48:41,954 - INFO - All documents cleared from the collection.\n",
            "2025-02-25 21:48:41,955 - INFO - Bucket 'vector-search-testing' exists.\n",
            "2025-02-25 21:48:41,959 - INFO - Collection 'cache' does not exist. Creating it...\n",
            "2025-02-25 21:48:42,003 - INFO - Collection 'cache' created successfully.\n",
            "2025-02-25 21:48:46,902 - INFO - Primary index present or created successfully.\n",
            "2025-02-25 21:48:46,904 - INFO - All documents cleared from the collection.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<couchbase.collection.Collection at 0x742954ceae40>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def setup_collection(cluster, bucket_name, scope_name, collection_name):\n",
        "    try:\n",
        "        # Check if bucket exists, create if it doesn't\n",
        "        try:\n",
        "            bucket = cluster.bucket(bucket_name)\n",
        "            logging.info(f\"Bucket '{bucket_name}' exists.\")\n",
        "        except Exception as e:\n",
        "            logging.info(f\"Bucket '{bucket_name}' does not exist. Creating it...\")\n",
        "            bucket_settings = CreateBucketSettings(\n",
        "                name=bucket_name,\n",
        "                bucket_type='couchbase',\n",
        "                ram_quota_mb=1024,\n",
        "                flush_enabled=True,\n",
        "                num_replicas=0\n",
        "            )\n",
        "            cluster.buckets().create_bucket(bucket_settings)\n",
        "            time.sleep(2)  # Wait for bucket creation to complete and become available\n",
        "            bucket = cluster.bucket(bucket_name)\n",
        "            logging.info(f\"Bucket '{bucket_name}' created successfully.\")\n",
        "\n",
        "        bucket_manager = bucket.collections()\n",
        "\n",
        "        # Check if scope exists, create if it doesn't\n",
        "        scopes = bucket_manager.get_all_scopes()\n",
        "        scope_exists = any(scope.name == scope_name for scope in scopes)\n",
        "        \n",
        "        if not scope_exists and scope_name != \"_default\":\n",
        "            logging.info(f\"Scope '{scope_name}' does not exist. Creating it...\")\n",
        "            bucket_manager.create_scope(scope_name)\n",
        "            logging.info(f\"Scope '{scope_name}' created successfully.\")\n",
        "\n",
        "        # Check if collection exists, create if it doesn't\n",
        "        collections = bucket_manager.get_all_scopes()\n",
        "        collection_exists = any(\n",
        "            scope.name == scope_name and collection_name in [col.name for col in scope.collections]\n",
        "            for scope in collections\n",
        "        )\n",
        "\n",
        "        if not collection_exists:\n",
        "            logging.info(f\"Collection '{collection_name}' does not exist. Creating it...\")\n",
        "            bucket_manager.create_collection(scope_name, collection_name)\n",
        "            logging.info(f\"Collection '{collection_name}' created successfully.\")\n",
        "        else:\n",
        "            logging.info(f\"Collection '{collection_name}' already exists. Skipping creation.\")\n",
        "\n",
        "        # Wait for collection to be ready\n",
        "        collection = bucket.scope(scope_name).collection(collection_name)\n",
        "        time.sleep(2)  # Give the collection time to be ready for queries\n",
        "\n",
        "        # Ensure primary index exists\n",
        "        try:\n",
        "            cluster.query(f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{bucket_name}`.`{scope_name}`.`{collection_name}`\").execute()\n",
        "            logging.info(\"Primary index present or created successfully.\")\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error creating primary index: {str(e)}\")\n",
        "\n",
        "        # Clear all documents in the collection\n",
        "        try:\n",
        "            query = f\"DELETE FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "            cluster.query(query).execute()\n",
        "            logging.info(\"All documents cleared from the collection.\")\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error while clearing documents: {str(e)}. The collection might be empty.\")\n",
        "\n",
        "        return collection\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error setting up collection: {str(e)}\")\n",
        "    \n",
        "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)\n",
        "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, CACHE_COLLECTION)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMJ7RRYp8mjV"
      },
      "source": [
        "# Loading Couchbase Vector Search Index\n",
        "\n",
        "Semantic search requires an efficient way to retrieve relevant documents based on a user's query. This is where the Couchbase **Vector Search Index** comes into play. In this step, we load the Vector Search Index definition from a JSON file, which specifies how the index should be structured. This includes the fields to be indexed, the dimensions of the vectors, and other parameters that determine how the search engine processes queries based on vector similarity.\n",
        "\n",
        "For more information on creating a vector search index, please follow the [instructions](https://docs.couchbase.com/cloud/vector-search/create-vector-search-index-ui.html).\n",
        "\n",
        "> Note: Index creation will not fail if used with the concerned bucket(vector-search-testing) instead of travel-sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "y7xiCrOc8mmj"
      },
      "outputs": [],
      "source": [
        "# If you are running this script locally (not in Google Colab), uncomment the following line\n",
        "# and provide the path to your index definition file.\n",
        "\n",
        "# index_definition_path = '/path_to_your_index_file/claude_index.json'  # Local setup: specify your file path here\n",
        "\n",
        "# # Version for Google Colab\n",
        "# def load_index_definition_colab():\n",
        "#     from google.colab import files\n",
        "#     print(\"Upload your index definition file\")\n",
        "#     uploaded = files.upload()\n",
        "#     index_definition_path = list(uploaded.keys())[0]\n",
        "\n",
        "#     try:\n",
        "#         with open(index_definition_path, 'r') as file:\n",
        "#             index_definition = json.load(file)\n",
        "#         return index_definition\n",
        "#     except Exception as e:\n",
        "#         raise ValueError(f\"Error loading index definition from {index_definition_path}: {str(e)}\")\n",
        "\n",
        "# Version for Local Environment\n",
        "def load_index_definition_local(index_definition_path):\n",
        "    try:\n",
        "        with open(index_definition_path, 'r') as file:\n",
        "            index_definition = json.load(file)\n",
        "        return index_definition\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error loading index definition from {index_definition_path}: {str(e)}\")\n",
        "\n",
        "# Usage\n",
        "# Uncomment the appropriate line based on your environment\n",
        "# index_definition = load_index_definition_colab()\n",
        "index_definition = load_index_definition_local('claude_index.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_ddPQ_Y8mpm"
      },
      "source": [
        "# Creating or Updating Search Indexes\n",
        "\n",
        "With the index definition loaded, the next step is to create or update the **Vector Search Index** in Couchbase. This step is crucial because it optimizes our database for vector similarity search operations, allowing us to perform searches based on the semantic content of documents rather than just keywords. By creating or updating a Vector Search Index, we enable our search engine to handle complex queries that involve finding semantically similar documents using vector embeddings, which is essential for a robust semantic search engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bHEpUu1l8msx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-25 21:48:52,980 - INFO - Creating new index 'vector_search_claude'...\n",
            "2025-02-25 21:48:53,069 - INFO - Index 'vector_search_claude' successfully created/updated.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    scope_index_manager = cluster.bucket(CB_BUCKET_NAME).scope(SCOPE_NAME).search_indexes()\n",
        "\n",
        "    # Check if index already exists\n",
        "    existing_indexes = scope_index_manager.get_all_indexes()\n",
        "    index_name = index_definition[\"name\"]\n",
        "\n",
        "    if index_name in [index.name for index in existing_indexes]:\n",
        "        logging.info(f\"Index '{index_name}' found\")\n",
        "    else:\n",
        "        logging.info(f\"Creating new index '{index_name}'...\")\n",
        "\n",
        "    # Create SearchIndex object from JSON definition\n",
        "    search_index = SearchIndex.from_json(index_definition)\n",
        "\n",
        "    # Upsert the index (create if not exists, update if exists)\n",
        "    scope_index_manager.upsert_index(search_index)\n",
        "    logging.info(f\"Index '{index_name}' successfully created/updated.\")\n",
        "\n",
        "except QueryIndexAlreadyExistsException:\n",
        "    logging.info(f\"Index '{index_name}' already exists. Skipping creation/update.\")\n",
        "except ServiceUnavailableException:\n",
        "    raise RuntimeError(\"Search service is not available. Please ensure the Search service is enabled in your Couchbase cluster.\")\n",
        "except InternalServerFailureException as e:\n",
        "    logging.error(f\"Internal server error: {str(e)}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FvxRsg38m3G"
      },
      "source": [
        "# Creating OpenAI Embeddings\n",
        "Embeddings are at the heart of semantic search. They are numerical representations of text that capture the semantic meaning of the words and phrases. Unlike traditional keyword-based search, which looks for exact matches, embeddings allow our search engine to understand the context and nuances of language, enabling it to retrieve documents that are semantically similar to the query, even if they don't contain the exact keywords. By creating embeddings using OpenAI, we equip our search engine with the ability to understand and process natural language in a way that's much closer to how humans understand language. This step transforms our raw text data into a format that the search engine can use to find and rank relevant documents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_75ZyCRh8m6m"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-25 21:48:56,274 - INFO - Successfully created OpenAIEmbeddings\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model='text-embedding-3-small')\n",
        "    logging.info(\"Successfully created OpenAIEmbeddings\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error creating OpenAIEmbeddings: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IwZMUnF8m-N"
      },
      "source": [
        "#  Setting Up the Couchbase Vector Store\n",
        "A vector store is where we'll keep our embeddings. Unlike the FTS index, which is used for text-based search, the vector store is specifically designed to handle embeddings and perform similarity searches. When a user inputs a query, the search engine converts the query into an embedding and compares it against the embeddings stored in the vector store. This allows the engine to find documents that are semantically similar to the query, even if they don't contain the exact same words. By setting up the vector store in Couchbase, we create a powerful tool that enables our search engine to understand and retrieve information based on the meaning and context of the query, rather than just the specific words used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DwIJQjYT9RV_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-25 21:48:59,450 - INFO - Successfully created vector store\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    vector_store = CouchbaseSearchVectorStore(\n",
        "        cluster=cluster,\n",
        "        bucket_name=CB_BUCKET_NAME,\n",
        "        scope_name=SCOPE_NAME,\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        embedding=embeddings,\n",
        "        index_name=INDEX_NAME,\n",
        "    )\n",
        "    logging.info(\"Successfully created vector store\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Failed to create vector store: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load the BBC News Dataset\n",
        "To build a search engine, we need data to search through. We use the BBC News dataset from RealTimeData, which provides real-world news articles. This dataset contains news articles from BBC covering various topics and time periods. Loading the dataset is a crucial step because it provides the raw material that our search engine will work with. The quality and diversity of the news articles make it an excellent choice for testing and refining our search engine, ensuring it can handle real-world news content effectively.\n",
        "\n",
        "The BBC News dataset allows us to work with authentic news articles, enabling us to build and test a search engine that can effectively process and retrieve relevant news content. The dataset is loaded using the Hugging Face datasets library, specifically accessing the \"RealTimeData/bbc_news_alltime\" dataset with the \"2024-12\" version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-25 21:49:09,255 - INFO - Successfully loaded the BBC News dataset with 2687 rows.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded the BBC News dataset with 2687 rows\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    news_dataset = load_dataset(\n",
        "        \"RealTimeData/bbc_news_alltime\", \"2024-12\", split=\"train\"\n",
        "    )\n",
        "    print(f\"Loaded the BBC News dataset with {len(news_dataset)} rows\")\n",
        "    logging.info(f\"Successfully loaded the BBC News dataset with {len(news_dataset)} rows.\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error loading the BBC News dataset: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleaning up the Data\n",
        "We will use the content of the news articles for our RAG system.\n",
        "\n",
        "The dataset contains a few duplicate records. We are removing them to avoid duplicate results in the retrieval stage of our RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have 1749 unique articles in our database.\n"
          ]
        }
      ],
      "source": [
        "news_articles = news_dataset[\"content\"]\n",
        "unique_articles = set()\n",
        "for article in news_articles:\n",
        "    if article:\n",
        "        unique_articles.add(article)\n",
        "unique_news_articles = list(unique_articles)\n",
        "print(f\"We have {len(unique_news_articles)} unique articles in our database.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving Data to the Vector Store\n",
        "To efficiently handle the large number of articles, we process them in batches of articles at a time. This batch processing approach helps manage memory usage and provides better control over the ingestion process.\n",
        "\n",
        "We first filter out any articles that exceed 50,000 characters to avoid potential issues with token limits. Then, using the vector store's add_texts method, we add the filtered articles to our vector database. The batch_size parameter controls how many articles are processed in each iteration.\n",
        "\n",
        "This approach offers several benefits:\n",
        "1. Memory Efficiency: Processing in smaller batches prevents memory overload\n",
        "2. Progress Tracking: Easier to monitor and track the ingestion progress\n",
        "3. Resource Management: Better control over CPU and network resource utilization\n",
        "\n",
        "We use a conservative batch size of 100 to ensure reliable operation.\n",
        "The optimal batch size depends on many factors including:\n",
        "- Document sizes being inserted\n",
        "- Available system resources\n",
        "- Network conditions\n",
        "- Concurrent workload\n",
        "\n",
        "Consider measuring performance with your specific workload before adjusting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-25 21:50:15,064 - INFO - Document ingestion completed successfully.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 100\n",
        "\n",
        "# Automatic Batch Processing\n",
        "articles = [article for article in unique_news_articles if article and len(article) <= 50000]\n",
        "\n",
        "try:\n",
        "    vector_store.add_texts(\n",
        "        texts=articles,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    logging.info(\"Document ingestion completed successfully.\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Failed to save documents to vector store: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pn8-dQw9RfQ"
      },
      "source": [
        "# Setting Up a Couchbase Cache\n",
        "To further optimize our system, we set up a Couchbase-based cache. A cache is a temporary storage layer that holds data that is frequently accessed, speeding up operations by reducing the need to repeatedly retrieve the same information from the database. In our setup, the cache will help us accelerate repetitive tasks, such as looking up similar documents. By implementing a cache, we enhance the overall performance of our search engine, ensuring that it can handle high query volumes and deliver results quickly.\n",
        "\n",
        "Caching is particularly valuable in scenarios where users may submit similar queries multiple times or where certain pieces of information are frequently requested. By storing these in a cache, we can significantly reduce the time it takes to respond to these queries, improving the user experience.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "V2y7dyjf9Rid"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-25 21:50:48,836 - INFO - Successfully created cache\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    cache = CouchbaseCache(\n",
        "        cluster=cluster,\n",
        "        bucket_name=CB_BUCKET_NAME,\n",
        "        scope_name=SCOPE_NAME,\n",
        "        collection_name=CACHE_COLLECTION,\n",
        "    )\n",
        "    logging.info(\"Successfully created cache\")\n",
        "    set_llm_cache(cache)\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Failed to create cache: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uehAx36o9Rlm"
      },
      "source": [
        "# Using the Claude 4 Sonnet Language Model (LLM)\n",
        "Language models are AI systems that are trained to understand and generate human language. We'll be using the `Claude 4 Sonnet` language model to process user queries and generate meaningful responses. This model is a key component of our semantic search engine, allowing it to go beyond simple keyword matching and truly understand the intent behind a query. By creating this language model, we equip our search engine with the ability to interpret complex queries, understand the nuances of language, and provide more accurate and contextually relevant responses.\n",
        "\n",
        "The language model's ability to understand context and generate coherent responses is what makes our search engine truly intelligent. It can not only find the right information but also present it in a way that is useful and understandable to the user.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRAfBRLH9RpO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-25 21:50:52,173 - INFO - Successfully created ChatAnthropic\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    llm = ChatAnthropic(temperature=0.1, anthropic_api_key=ANTHROPIC_API_KEY, model_name='claude-sonnet-4-20250514') \n",
        "    logging.info(\"Successfully created ChatAnthropic\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error creating ChatAnthropic: {str(e)}. Please check your API key and network connection.\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_XDfCx19UvG"
      },
      "source": [
        "# Perform Semantic Search\n",
        "Semantic search in Couchbase involves converting queries and documents into vector representations using an embeddings model. These vectors capture the semantic meaning of the text and are stored directly in Couchbase. When a query is made, Couchbase performs a similarity search by comparing the query vector against the stored document vectors. The similarity metric used for this comparison is configurable, allowing flexibility in how the relevance of documents is determined. \n",
        "\n",
        "In the provided code, the search process begins by recording the start time, followed by executing the similarity_search_with_score method of the CouchbaseSearchVectorStore. This method searches Couchbase for the most relevant documents based on the vector similarity to the query. The search results include the document content and a similarity score that reflects how closely each document aligns with the query in the defined semantic space. The time taken to perform this search is then calculated and logged, and the results are displayed, showing the most relevant documents along with their similarity scores. This approach leverages Couchbase as both a storage and retrieval engine for vector data, enabling efficient and scalable semantic searches. The integration of vector storage and search capabilities within Couchbase allows for sophisticated semantic search operations without relying on external services for vector storage or comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Pk-oFbnC9Uym"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-25 21:53:55,462 - INFO - Semantic search completed in 0.55 seconds\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Semantic Search Results (completed in 0.55 seconds):\n",
            "--------------------------------------------------------------------------------\n",
            "Score: 0.7498, Text: A map shown during the draw for the 2026 Fifa World Cup has been criticised by Ukraine as an \"unacceptable error\" after it appeared to exclude Crimea as part of the country. The graphic - showing countries that cannot be drawn to play each other for geopolitical reasons - highlighted Ukraine but did not include the peninsula that is internationally recognised to be part of it. Crimea has been under Russian occupation since 2014 and just a handful of countries recognise the peninsula as Russian territory. Ukraine Foreign Ministry spokesman Heorhiy Tykhy said that the nation expects \"a public apology\". Fifa said it was \"aware of an issue\" and the image had been removed.\n",
            "\n",
            "Writing on X, Tykhy said that Fifa had not only \"acted against international law\" but had also \"supported Russian propaganda, war crimes, and the crime of aggression against Ukraine\". He added a \"fixed\" version of the map to his post, highlighting Crimea as part of Ukraine's territory. Among the countries that cannot play each other are Ukraine and Belarus, Spain and Gibraltar and Kosovo versus either Bosnia and Herzegovina or Serbia.\n",
            "\n",
            "This Twitter post cannot be displayed in your browser. Please enable Javascript or try a different browser. View original content on Twitter The BBC is not responsible for the content of external sites. Skip twitter post by Heorhii Tykhyi This article contains content provided by Twitter. We ask for your permission before anything is loaded, as they may be using cookies and other technologies. You may want to read Twitter’s cookie policy, external and privacy policy, external before accepting. To view this content choose ‘accept and continue’. The BBC is not responsible for the content of external sites.\n",
            "\n",
            "The Ukrainian Football Association has also sent a letter to Fifa secretary-general Mathias Grafström and UEFA secretary-general Theodore Theodoridis over the matter. \"We appeal to you to express our deep concern about the infographic map [shown] on December 13, 2024,\" the letter reads. \"Taking into account a number of official decisions and resolutions adopted by the Fifa Council and the UEFA executive committee since 2014... we emphasize that today's version of the cartographic image of Ukraine... is completely unacceptable and looks like an inconsistent position of Fifa and UEFA.\" The 2026 World Cup will start on 11 June that year in Mexico City and end on 19 July in New Jersey. The expanded 48-team tournament will last a record 39 days. Ukraine were placed in Group D alongside Iceland, Azerbaijan and the yet-to-be-determined winners of France's Nations League quarter-final against Croatia.\n",
            "--------------------------------------------------------------------------------\n",
            "Score: 0.4302, Text: Defending champions Manchester City will face Juventus in the group stage of the Fifa Club World Cup next summer, while Chelsea meet Brazilian side Flamengo. Pep Guardiola's City, who beat Brazilian side Fluminense to win the tournament for the first time in 2023, begin their title defence against Morocco's Wydad and also play Al Ain of the United Arab Emirates in Group G. Chelsea, winners of the 2021 final, were also drawn alongside Mexico's Club Leon and Tunisian side Esperance Sportive de Tunisie in Group D. The revamped Fifa Club World Cup, which has been expanded to 32 teams, will take place in the United States between 15 June and 13 July next year.\n",
            "\n",
            "A complex and lengthy draw ceremony was held across two separate Miami locations and lasted more than 90 minutes, during which a new Club World Cup trophy was revealed. There was also a video message from incoming US president Donald Trump, whose daughter Ivanka drew the first team. Lionel Messi's Inter Miami will take on Egyptian side Al Ahly at the Hard Rock Stadium in the opening match, staged in Miami. Elsewhere, Paris St-Germain were drawn against Atletico Madrid in Group B, while Bayern Munich meet Benfica in another all-European group-stage match-up. Teams will play each other once in the group phase and the top two will progress to the knockout stage.\n",
            "\n",
            "This video can not be played To play this video you need to enable JavaScript in your browser. What is the Club World Cup?\n",
            "\n",
            "Teams from each of the six international football confederations will be represented at next summer's tournament, including 12 European clubs - the highest quota of any confederation. The European places were decided by clubs' Champions League performances over the past four seasons, with recent winners Chelsea, Manchester City and Real Madrid guaranteed places. Al Ain, the most successful club in the UAE with 14 league titles, are owned by the country's president Sheikh Mohamed bin Zayed Al Nahyan - the older brother of City owner Sheikh Mansour. Real, who lifted the Fifa Club World Cup trophy for a record-extending fifth time in 2022, will open up against Saudi Pro League champions Al-Hilal, who currently have Neymar in their ranks. One place was reserved for a club from the host nation, which Fifa controversially awarded to Inter Miami, who will contest the tournament curtain-raiser. Messi's side were winners of the regular-season MLS Supporters' Shield but beaten in the MLS play-offs, meaning they are not this season's champions.\n",
            "• None How does the new Club World Cup work & why is it so controversial?\n",
            "\n",
            "Matches will be played across 12 venues in the US which, alongside Canada and Mexico, also host the 2026 World Cup. Fifa is facing legal action from player unions and leagues about the scheduling of the event, which begins two weeks after the Champions League final at the end of the 2024-25 European calendar and ends five weeks before the first Premier League match of the 2025-2026 season. But football's world governing body believes the dates allow sufficient rest time before the start of the domestic campaigns. The Club World Cup will now take place once every four years, when it was previously held annually and involved just seven teams. Streaming platform DAZN has secured exclusive rights to broadcast next summer's tournament, during which 63 matches will take place over 29 days.\n",
            "--------------------------------------------------------------------------------\n",
            "Score: 0.4207, Text: After Fifa awards Saudi Arabia the hosting rights for the men's 2034 World Cup, BBC analysis editor Ros Atkins looks at how we got here and the controversies surrounding the decision.\n",
            "--------------------------------------------------------------------------------\n",
            "Score: 0.4123, Text: FA still to decide on endorsing Saudi World Cup bid\n",
            "... (output truncated for brevity)\n"
          ]
        }
      ],
      "source": [
        "query = \"What happened with the map shown during the 2026 FIFA World Cup draw regarding Ukraine and Crimea? What was the controversy?\"\n",
        "\n",
        "try:\n",
        "    # Perform the semantic search\n",
        "    start_time = time.time()\n",
        "    search_results = vector_store.similarity_search_with_score(query, k=10)\n",
        "    search_elapsed_time = time.time() - start_time\n",
        "\n",
        "    logging.info(f\"Semantic search completed in {search_elapsed_time:.2f} seconds\")\n",
        "\n",
        "    # Display search results\n",
        "    print(f\"\\nSemantic Search Results (completed in {search_elapsed_time:.2f} seconds):\")\n",
        "    print(\"-\" * 80)  # Add separator line\n",
        "    for doc, score in search_results:\n",
        "        print(f\"Score: {score:.4f}, Text: {doc.page_content}\")\n",
        "        print(\"-\" * 80)  # Add separator between results\n",
        "\n",
        "except CouchbaseException as e:\n",
        "    raise RuntimeError(f\"Error performing semantic search: {str(e)}\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Unexpected error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS0FebHI9U1l"
      },
      "source": [
        "# Retrieval-Augmented Generation (RAG) with Couchbase and LangChain\n",
        "Couchbase and LangChain can be seamlessly integrated to create RAG (Retrieval-Augmented Generation) chains, enhancing the process of generating contextually relevant responses. In this setup, Couchbase serves as the vector store, where embeddings of documents are stored. When a query is made, LangChain retrieves the most relevant documents from Couchbase by comparing the query’s embedding with the stored document embeddings. These documents, which provide contextual information, are then passed to a generative language model within LangChain.\n",
        "\n",
        "The language model, equipped with the context from the retrieved documents, generates a response that is both informed and contextually accurate. This integration allows the RAG chain to leverage Couchbase’s efficient storage and retrieval capabilities, while LangChain handles the generation of responses based on the context provided by the retrieved documents. Together, they create a powerful system that can deliver highly relevant and accurate answers by combining the strengths of both retrieval and generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZGUXQQmv9ge4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-25 21:54:00,781 - INFO - Successfully created RAG chain\n"
          ]
        }
      ],
      "source": [
        "system_template = \"You are a helpful assistant that answers questions based on the provided context.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "human_template = \"Context: {context}\\n\\nQuestion: {question}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    system_message_prompt,\n",
        "    human_message_prompt\n",
        "])\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": lambda x: format_docs(vector_store.similarity_search(x)), \"question\": RunnablePassthrough()}\n",
        "    | chat_prompt\n",
        "    | llm\n",
        ")\n",
        "logging.info(\"Successfully created RAG chain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Mia7XxM9978M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG Response: During the draw for the 2026 FIFA World Cup, a map was shown that excluded Crimea as part of Ukraine. This graphic, which was displaying countries that cannot be drawn to play each other for geopolitical reasons, highlighted Ukraine but did not include the Crimean peninsula, which is internationally recognized as Ukrainian territory.\n",
            "\n",
            "This omission sparked significant controversy because Crimea has been under Russian occupation since 2014, but only a handful of countries recognize it as Russian territory. The Ukrainian Foreign Ministry spokesman, Heorhiy Tykhy, called this an \"unacceptable error\" and stated that Ukraine expected \"a public apology\" from FIFA. He criticized FIFA for acting \"against international law\" and supporting \"Russian propaganda, war crimes, and the crime of aggression against Ukraine.\"\n",
            "\n",
            "The Ukrainian Football Association also sent a formal letter of complaint to FIFA and UEFA officials expressing their \"deep concern\" about the cartographic representation. FIFA acknowledged they were \"aware of an issue\" and subsequently removed the image.\n",
            "RAG response generated in 6.58 seconds\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    start_time = time.time()\n",
        "    rag_response = rag_chain.invoke(query)\n",
        "    rag_elapsed_time = time.time() - start_time\n",
        "\n",
        "    print(f\"RAG Response: {rag_response.content}\")\n",
        "    print(f\"RAG response generated in {rag_elapsed_time:.2f} seconds\")\n",
        "except AuthenticationError as e:\n",
        "    print(f\"Authentication error: {str(e)}\")\n",
        "except InternalServerFailureException as e:\n",
        "    if \"query request rejected\" in str(e):\n",
        "        print(\"Error: Search request was rejected due to rate limiting. Please try again later.\")\n",
        "    else:\n",
        "        print(f\"Internal server error occurred: {str(e)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Unexpected error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIdayPzw9glT"
      },
      "source": [
        "# Using Couchbase as a caching mechanism\n",
        "Couchbase can be effectively used as a caching mechanism for RAG (Retrieval-Augmented Generation) responses by storing and retrieving precomputed results for specific queries. This approach enhances the system's efficiency and speed, particularly when dealing with repeated or similar queries. When a query is first processed, the RAG chain retrieves relevant documents, generates a response using the language model, and then stores this response in Couchbase, with the query serving as the key.\n",
        "\n",
        "For subsequent requests with the same query, the system checks Couchbase first. If a cached response is found, it is retrieved directly from Couchbase, bypassing the need to re-run the entire RAG process. This significantly reduces response time because the computationally expensive steps of document retrieval and response generation are skipped. Couchbase's role in this setup is to provide a fast and scalable storage solution for caching these responses, ensuring that frequently asked queries can be answered more quickly and efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0xM2G3ef-GS2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Query 1: What happened when Apple's AI feature generated a false BBC headline about a murder case in New York?\n",
            "Response: According to the context, Apple Intelligence (an AI feature that summarizes notifications) generated a false headline that made it appear as if BBC News had published an article claiming Luigi Mangione, who was arrested for the murder of healthcare insurance CEO Brian Thompson in New York, had shot himself. This was completely false - Mangione had not shot himself.\n",
            "\n",
            "The BBC complained to Apple about this misrepresentation, with a BBC spokesperson stating they had \"contacted Apple to raise this concern and fix the problem.\" The BBC emphasized that as \"the most trusted news media in the world,\" it's essential that audiences can trust information published in their name, including notifications.\n",
            "\n",
            "This wasn't an isolated incident - the context mentions that Apple's AI feature also misrepresented a New York Times article, incorrectly summarizing it as \"Netanyahu arrested\" when the actual article was about the International Criminal Court issuing an arrest warrant for the Israeli prime minister.\n",
            "Time taken: 6.66 seconds\n",
            "\n",
            "Query 2: What happened with the map shown during the 2026 FIFA World Cup draw regarding Ukraine and Crimea? What was the controversy?\n",
            "Response: During the draw for the 2026 FIFA World Cup, a map was shown that excluded Crimea as part of Ukraine. This graphic, which was displaying countries that cannot be drawn to play each other for geopolitical reasons, highlighted Ukraine but did not include the Crimean peninsula, which is internationally recognized as Ukrainian territory.\n",
            "\n",
            "This omission sparked significant controversy because Crimea has been under Russian occupation since 2014, but only a handful of countries recognize it as Russian territory. The Ukrainian Foreign Ministry spokesman, Heorhiy Tykhy, called this an \"unacceptable error\" and stated that Ukraine expected \"a public apology\" from FIFA. He criticized FIFA for acting \"against international law\" and supporting \"Russian propaganda, war crimes, and the crime of aggression against Ukraine.\"\n",
            "\n",
            "The Ukrainian Football Association also sent a formal letter of complaint to FIFA and UEFA officials expressing their \"deep concern\" about the cartographic representation. FIFA acknowledged they were \"aware of an issue\" and subsequently removed the image.\n",
            "Time taken: 0.62 seconds\n",
            "\n",
            "Query 3: What happened when Apple's AI feature generated a false BBC headline about a murder case in New York?\n",
            "Response: According to the context, Apple Intelligence (an AI feature that summarizes notifications) generated a false headline that made it appear as if BBC News had published an article claiming Luigi Mangione, who was arrested for the murder of healthcare insurance CEO Brian Thompson in New York, had shot himself. This was completely false - Mangione had not shot himself.\n",
            "\n",
            "The BBC complained to Apple about this misrepresentation, with a BBC spokesperson stating they had \"contacted Apple to raise this concern and fix the problem.\" The BBC emphasized that as \"the most trusted news media in the world,\" it's essential that audiences can trust information published in their name, including notifications.\n",
            "\n",
            "This wasn't an isolated incident - the context mentions that Apple's AI feature also misrepresented a New York Times article, incorrectly summarizing it as \"Netanyahu arrested\" when the actual article was about the International Criminal Court issuing an arrest warrant for the Israeli prime minister.\n",
            "Time taken: 0.51 seconds\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    queries = [\n",
        "        \"What happened when Apple's AI feature generated a false BBC headline about a murder case in New York?\",\n",
        "        \"What happened with the map shown during the 2026 FIFA World Cup draw regarding Ukraine and Crimea? What was the controversy?\", # Repeated query\n",
        "        \"What happened when Apple's AI feature generated a false BBC headline about a murder case in New York?\", # Repeated query\n",
        "    ]\n",
        "\n",
        "    for i, query in enumerate(queries, 1):\n",
        "        print(f\"\\nQuery {i}: {query}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        response = rag_chain.invoke(query)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"Response: {response.content}\")\n",
        "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
        "except AuthenticationError as e:\n",
        "    print(f\"Authentication error: {str(e)}\")\n",
        "except InternalServerFailureException as e:\n",
        "    if \"query request rejected\" in str(e):\n",
        "        print(\"Error: Search request was rejected due to rate limiting. Please try again later.\")\n",
        "    else:\n",
        "        print(f\"Internal server error occurred: {str(e)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Unexpected error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJQ5P8E29go1"
      },
      "source": [
        "## Conclusion\n",
        "By following these steps, you’ll have a fully functional semantic search engine that leverages the strengths of Couchbase and Claude(by Anthropic). This guide is designed not just to show you how to build the system, but also to explain why each step is necessary, giving you a deeper understanding of the principles behind semantic search and how to implement it effectively. Whether you’re a newcomer to software development or an experienced developer looking to expand your skills, this guide will provide you with the knowledge and tools you need to create a powerful, AI-driven search engine."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
