{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "In this guide, we will walk you through building a powerful semantic search engine using Couchbase as the backend database and Claude as the AI-powered embedding and language model provider. Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval. This tutorial is designed to be beginner-friendly, with clear, step-by-step instructions that will equip you with the knowledge to create a fully functional semantic search system from scratch."
      ],
      "metadata": {
        "id": "kNdImxzypDlm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting the Stage: Installing Necessary Libraries\n",
        "To build our semantic search engine, we need a robust set of tools. The libraries we install handle everything from connecting to databases to performing complex machine learning tasks. Each library has a specific role: Couchbase libraries manage database operations, LangChain handles AI model integrations, and Claude provides advanced AI models for generating embeddings and understanding natural language. By setting up these libraries, we ensure our environment is equipped to handle the data-intensive and computationally complex tasks required for semantic search."
      ],
      "metadata": {
        "id": "NH2o6pqa69oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install couchbase datasets langchain tqdm python-dotenv langchain-couchbase langchain-community langchain-anthropic langchain-openai openai"
      ],
      "metadata": {
        "id": "DYhPj0Ta8l_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "565c53df-b3ea-46ea-9dbe-16298429d818"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: couchbase in /usr/local/lib/python3.10/dist-packages (4.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.14)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: langchain-couchbase in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
            "Requirement already satisfied: langchain-anthropic in /usr/local/lib/python3.10/dist-packages (0.1.23)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.1.22)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.42.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.32 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.34)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.101)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: anthropic<1,>=0.30.0 in /usr/local/lib/python3.10/dist-packages (from langchain-anthropic) (0.34.1)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from langchain-anthropic) (0.7.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic<1,>=0.30.0->langchain-anthropic) (0.19.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.32->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Necessary Libraries\n",
        "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading. These libraries provide essential functions for working with data, managing database connections, and processing machine learning models."
      ],
      "metadata": {
        "id": "1pp7GtNg8mB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "from datetime import timedelta\n",
        "from uuid import uuid4\n",
        "\n",
        "import numpy as np\n",
        "from couchbase.auth import PasswordAuthenticator\n",
        "from couchbase.cluster import Cluster\n",
        "from couchbase.exceptions import CollectionNotFoundException, CouchbaseException, InternalServerFailureException, QueryIndexAlreadyExistsException\n",
        "from couchbase.management.search import SearchIndex\n",
        "from couchbase.options import ClusterOptions\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.globals import set_llm_cache\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_couchbase.cache import CouchbaseCache\n",
        "from langchain_couchbase.vectorstores import CouchbaseVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "8GzS6tfL8mFP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Logging\n",
        "Logging is configured to track the progress of the script and capture any errors or warnings. This is crucial for debugging and understanding the flow of execution. The logging output includes timestamps, log levels (e.g., INFO, ERROR), and messages that describe what is happening in the script.\n"
      ],
      "metadata": {
        "id": "pBnMp5vb8mIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ],
      "metadata": {
        "id": "Yv8kWcuf8mLx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuring the Environment\n",
        "Additionally, we'll load environment variables, which contains sensitive information like API keys and database credentials. Using environment variables instead of hardcoding these values into our script is a best practice that enhances security and flexibility, making it easier to manage and deploy our code in different environments."
      ],
      "metadata": {
        "id": "K9G5a0en8mPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
        "CB_HOST = os.getenv('CB_HOST', 'couchbase://localhost')\n",
        "CB_USERNAME = os.getenv('CB_USERNAME', 'Administrator')\n",
        "CB_PASSWORD = os.getenv('CB_PASSWORD', 'password')\n",
        "CB_BUCKET_NAME = os.getenv('CB_BUCKET_NAME', 'vector-search-testing')\n",
        "INDEX_NAME = os.getenv('INDEX_NAME', 'vector_search_claude')\n",
        "\n",
        "SCOPE_NAME = os.getenv('SCOPE_NAME', 'shared')\n",
        "COLLECTION_NAME = os.getenv('COLLECTION_NAME', 'claude')\n",
        "CACHE_COLLECTION = os.getenv('CACHE_COLLECTION', 'cache')"
      ],
      "metadata": {
        "id": "PFGyHll18mSe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connecting to the Couchbase Cluster\n",
        "Connecting to a Couchbase cluster is the foundation of our project. Couchbase will serve as our primary data store, handling all the storage and retrieval operations required for our semantic search engine. By establishing this connection, we enable our application to interact with the database, allowing us to perform operations such as storing embeddings, querying data, and managing collections. This connection is the gateway through which all data will flow, so ensuring it's set up correctly is paramount.\n",
        "\n"
      ],
      "metadata": {
        "id": "qtGrYzUY8mV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
        "    options = ClusterOptions(auth)\n",
        "    cluster = Cluster(CB_HOST, options)\n",
        "    cluster.wait_until_ready(timedelta(seconds=5))\n",
        "    logging.info(\"Successfully connected to Couchbase\")\n",
        "except Exception as e:\n",
        "    raise ConnectionError(f\"Failed to connect to Couchbase: {str(e)}\")"
      ],
      "metadata": {
        "id": "Zb3kK-7W8mZK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up Collections in Couchbase\n",
        "In Couchbase, data is organized in buckets, which can be further divided into scopes and collections. Think of a collection as a table in a traditional SQL database. Before we can store any data, we need to ensure that our collections exist. If they don't, we must create them. This step is important because it prepares the database to handle the specific types of data our application will process. By setting up collections, we define the structure of our data storage, which is essential for efficient data retrieval and management.\n",
        "\n",
        "Moreover, setting up collections allows us to isolate different types of data within the same bucket, providing a more organized and scalable data structure. This is particularly useful when dealing with large datasets, as it ensures that related data is stored together, making it easier to manage and query."
      ],
      "metadata": {
        "id": "C_Gpy32N8mcZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    bucket = cluster.bucket(CB_BUCKET_NAME)\n",
        "    bucket_manager = bucket.collections()\n",
        "\n",
        "    # Check if collection exists, create if it doesn't\n",
        "    collections = bucket_manager.get_all_scopes()\n",
        "    collection_exists = any(\n",
        "        scope.name == SCOPE_NAME and COLLECTION_NAME in [col.name for col in scope.collections]\n",
        "        for scope in collections\n",
        "    )\n",
        "\n",
        "    if not collection_exists:\n",
        "        logging.info(f\"Collection '{COLLECTION_NAME}' does not exist. Creating it...\")\n",
        "        bucket_manager.create_collection(SCOPE_NAME, COLLECTION_NAME)\n",
        "        logging.info(f\"Collection '{COLLECTION_NAME}' created successfully.\")\n",
        "    else:\n",
        "        logging.info(f\"Collection '{COLLECTION_NAME}' already exists.\")\n",
        "\n",
        "    # Wait for the collection to be available\n",
        "    max_retries = 3\n",
        "    retry_delay = 1\n",
        "    for _ in range(max_retries):\n",
        "        try:\n",
        "            collection = bucket.scope(SCOPE_NAME).collection(COLLECTION_NAME)\n",
        "            break\n",
        "        except CollectionNotFoundException:\n",
        "            time.sleep(retry_delay)\n",
        "    else:\n",
        "        raise RuntimeError(f\"Collection '{COLLECTION_NAME}' not available after {max_retries} retries\")\n",
        "\n",
        "    # Ensure primary index exists\n",
        "    try:\n",
        "        cluster.query(f\"CREATE PRIMARY INDEX ON `{CB_BUCKET_NAME}`.`{SCOPE_NAME}`.`{COLLECTION_NAME}`\").execute()\n",
        "        logging.info(f\"Primary index created on collection '{COLLECTION_NAME}'\")\n",
        "    except QueryIndexAlreadyExistsException:\n",
        "        logging.info(f\"Primary index already exists on collection '{COLLECTION_NAME}'\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error creating primary index: {str(e)}\")\n",
        "\n",
        "    # Clear all documents in the collection\n",
        "    try:\n",
        "        query = f\"DELETE FROM `{CB_BUCKET_NAME}`.`{SCOPE_NAME}`.`{COLLECTION_NAME}`\"\n",
        "        cluster.query(query).execute()\n",
        "        logging.info(\"All documents cleared from the collection.\")\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Error while clearing documents: {str(e)}. The collection might be empty.\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Error setting up collection: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "ACZcwUnG8mf2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Index Definition\n",
        "Semantic search requires an efficient way to retrieve relevant documents based on a user’s query. This is where the Couchbase Full-Text Search (FTS) index comes in. An FTS index is designed to enable full-text search capabilities, such as searching for words or phrases within documents stored in Couchbase. In this step, we load the FTS index definition from a JSON file, which specifies how the index should be structured. This includes the fields to be indexed and other parameters that determine how the search engine processes queries. By defining an index, we ensure that our search engine can quickly and accurately retrieve the most relevant documents, which is crucial for delivering fast and relevant search results to users.\n"
      ],
      "metadata": {
        "id": "NMJ7RRYp8mjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# index_definition_path = '/path_to_your_index_file/claude_index.json'\n",
        "\n",
        "# Prompt user to upload to google drive\n",
        "from google.colab import files\n",
        "print(\"Upload your index definition file\")\n",
        "uploaded = files.upload()\n",
        "index_definition_path = list(uploaded.keys())[0]\n",
        "\n",
        "try:\n",
        "    with open(index_definition_path, 'r') as file:\n",
        "        index_definition = json.load(file)\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error loading index definition from {index_definition_path}: {str(e)}\")"
      ],
      "metadata": {
        "id": "y7xiCrOc8mmj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "57e9e963-384c-4515-da8f-7510ce7d9b34"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your index definition file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-edac0743-b248-4d60-a8db-2e525a363e7a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-edac0743-b248-4d60-a8db-2e525a363e7a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving claude_index.json to claude_index (1).json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating or Updating Search Indexes\n",
        "With the index definition loaded, the next step is to create or update the FTS index in Couchbase. This step is fundamental because it optimizes our database for text search operations, allowing us to perform searches based on the content of documents rather than just their metadata. By creating or updating an FTS index, we make it possible for our search engine to handle complex queries that involve finding specific text within documents, which is essential for a robust semantic search engine.\n"
      ],
      "metadata": {
        "id": "v_ddPQ_Y8mpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    scope_index_manager = cluster.bucket(CB_BUCKET_NAME).scope(SCOPE_NAME).search_indexes()\n",
        "\n",
        "    # Check if index already exists\n",
        "    existing_indexes = scope_index_manager.get_all_indexes()\n",
        "    index_name = index_definition[\"name\"]\n",
        "\n",
        "    if index_name in [index.name for index in existing_indexes]:\n",
        "        logging.info(f\"Index '{index_name}' already exists. Updating...\")\n",
        "    else:\n",
        "        logging.info(f\"Creating new index '{index_name}'...\")\n",
        "\n",
        "    # Create SearchIndex object\n",
        "    search_index = SearchIndex(\n",
        "        name=index_definition[\"name\"],\n",
        "        source_type=index_definition.get(\"sourceType\", \"couchbase\"),\n",
        "        idx_type=index_definition[\"type\"],\n",
        "        source_name=index_definition[\"sourceName\"],\n",
        "        params=index_definition[\"params\"],\n",
        "        source_params=index_definition.get(\"sourceParams\", {}),\n",
        "        plan_params=index_definition.get(\"planParams\", {})\n",
        "    )\n",
        "\n",
        "    # Upsert the index (create if not exists, update if exists)\n",
        "    scope_index_manager.upsert_index(search_index)\n",
        "    logging.info(f\"Index '{index_name}' successfully created/updated.\")\n",
        "except QueryIndexAlreadyExistsException:\n",
        "    logging.info(f\"Index '{index_name}' already exists. Skipping creation/update.\")\n",
        "except InternalServerFailureException as e:\n",
        "    logging.error(f\"InternalServerFailureException raised: {str(e)}\")\n",
        "    raise RuntimeError(f\"Internal server error while creating/updating search index: {str(e)}\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Unexpected error creating/updating search index: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "bHEpUu1l8msx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the TREC Dataset\n",
        "To build a search engine, we need data to search through. We use the TREC dataset, a well-known benchmark in the field of information retrieval. This dataset contains a wide variety of text data that we'll use to train our search engine. Loading the dataset is a crucial step because it provides the raw material that our search engine will work with. The quality and diversity of the data in the TREC dataset make it an excellent choice for testing and refining our search engine, ensuring that it can handle a wide range of queries effectively.\n",
        "\n",
        "The TREC dataset's rich content allows us to simulate real-world scenarios where users ask complex questions, enabling us to fine-tune our search engine's ability to understand and respond to various types of queries."
      ],
      "metadata": {
        "id": "QRV4k06L8mwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    trec = load_dataset('trec', split='train[:1000]')\n",
        "    logging.info(f\"Successfully loaded TREC dataset with {len(trec)} samples\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error loading TREC dataset: {str(e)}\")"
      ],
      "metadata": {
        "id": "TRfRslF_8mzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66ef990a-a467-44f5-85e5-e6b54a587daa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Claude Embeddings\n",
        "Embeddings are at the heart of semantic search. They are numerical representations of text that capture the semantic meaning of the words and phrases. Unlike traditional keyword-based search, which looks for exact matches, embeddings allow our search engine to understand the context and nuances of language, enabling it to retrieve documents that are semantically similar to the query, even if they don't contain the exact keywords. By creating embeddings using Claude, we equip our search engine with the ability to understand and process natural language in a way that's much closer to how humans understand language. This step transforms our raw text data into a format that the search engine can use to find and rank relevant documents.\n",
        "\n"
      ],
      "metadata": {
        "id": "7FvxRsg38m3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model='text-embedding-ada-002')\n",
        "    logging.info(\"Successfully created OpenAIEmbeddings\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error creating OpenAIEmbeddings: {str(e)}\")"
      ],
      "metadata": {
        "id": "_75ZyCRh8m6m"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Setting Up the Couchbase Vector Store\n",
        "A vector store is where we'll keep our embeddings. Unlike the FTS index, which is used for text-based search, the vector store is specifically designed to handle embeddings and perform similarity searches. When a user inputs a query, the search engine converts the query into an embedding and compares it against the embeddings stored in the vector store. This allows the engine to find documents that are semantically similar to the query, even if they don't contain the exact same words. By setting up the vector store in Couchbase, we create a powerful tool that enables our search engine to understand and retrieve information based on the meaning and context of the query, rather than just the specific words used."
      ],
      "metadata": {
        "id": "8IwZMUnF8m-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    vector_store = CouchbaseVectorStore(\n",
        "        cluster=cluster,\n",
        "        bucket_name=CB_BUCKET_NAME,\n",
        "        scope_name=SCOPE_NAME,\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        embedding=embeddings,\n",
        "        index_name=INDEX_NAME,\n",
        "    )\n",
        "    logging.info(\"Successfully created vector store\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Failed to create vector store: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "DwIJQjYT9RV_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Data to the Vector Store\n",
        "With the vector store set up, the next step is to populate it with data. We save the TREC dataset to the vector store in batches. This method is efficient and ensures that our search engine can handle large datasets without running into performance issues. By saving the data in this way, we prepare our search engine to quickly and accurately respond to user queries. This step is essential for making the dataset searchable, transforming raw data into a format that can be easily queried by our search engine.\n",
        "\n",
        "Batch processing is particularly important when dealing with large datasets, as it prevents memory overload and ensures that the data is stored in a structured and retrievable manner. This approach not only optimizes performance but also ensures the scalability of our system."
      ],
      "metadata": {
        "id": "C6DJVz7A9RZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 50\n",
        "try:\n",
        "    for i in tqdm(range(0, len(trec['text']), batch_size), desc=\"Processing Batches\"):\n",
        "        batch = trec['text'][i:i + batch_size]\n",
        "        documents = [Document(page_content=text) for text in batch]\n",
        "        uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "        vector_store.add_documents(documents=documents, ids=uuids)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to save documents to vector store: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "_6opqqvx9Rb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74c7314-97ed-47bf-e4df-917b6215537d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batches: 100%|██████████| 20/20 [00:14<00:00,  1.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up a Couchbase Cache\n",
        "To further optimize our system, we set up a Couchbase-based cache. A cache is a temporary storage layer that holds data that is frequently accessed, speeding up operations by reducing the need to repeatedly retrieve the same information from the database. In our setup, the cache will help us accelerate repetitive tasks, such as looking up similar documents. By implementing a cache, we enhance the overall performance of our search engine, ensuring that it can handle high query volumes and deliver results quickly.\n",
        "\n",
        "Caching is particularly valuable in scenarios where users may submit similar queries multiple times or where certain pieces of information are frequently requested. By storing these in a cache, we can significantly reduce the time it takes to respond to these queries, improving the user experience.\n"
      ],
      "metadata": {
        "id": "8Pn8-dQw9RfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    cache = CouchbaseCache(\n",
        "        cluster=cluster,\n",
        "        bucket_name=CB_BUCKET_NAME,\n",
        "        scope_name=SCOPE_NAME,\n",
        "        collection_name=CACHE_COLLECTION,\n",
        "    )\n",
        "    logging.info(\"Successfully created cache\")\n",
        "    set_llm_cache(cache)\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Failed to create cache: {str(e)}\")"
      ],
      "metadata": {
        "id": "V2y7dyjf9Rid"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the OpenAi Language Model (LLM)\n",
        "Language models are AI systems that are trained to understand and generate human language. We'll be using OpenAI's language model to process user queries and generate meaningful responses. This model is a key component of our semantic search engine, allowing it to go beyond simple keyword matching and truly understand the intent behind a query. By creating this language model, we equip our search engine with the ability to interpret complex queries, understand the nuances of language, and provide more accurate and contextually relevant responses.\n",
        "\n",
        "The language model's ability to understand context and generate coherent responses is what makes our search engine truly intelligent. It can not only find the right information but also present it in a way that is useful and understandable to the user.\n",
        "\n"
      ],
      "metadata": {
        "id": "uehAx36o9Rlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    llm = ChatAnthropic(temperature=0, anthropic_api_key=ANTHROPIC_API_KEY, model_name='claude-3-5-sonnet-20240620')\n",
        "    logging.info(\"Successfully created ChatAnthropic\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error creating ChatAnthropic: {str(e)}. Please check your API key and network connection.\")\n",
        "    raise"
      ],
      "metadata": {
        "id": "yRAfBRLH9RpO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Retrieval-Augmented Generation (RAG) Chain\n",
        "A Retrieval-Augmented Generation (RAG) chain combines the power of retrieval (searching through data) with generation (producing a coherent response). This step integrates the vector store with the language model, enabling our search engine to retrieve relevant documents and generate well-formed answers to user queries. The RAG chain is what makes our search engine more intelligent, allowing it to not only find relevant information but also present it in a user-friendly manner. By building this chain, we create a system that can handle complex queries and provide users with meaningful, contextually appropriate responses.\n",
        "\n",
        "The RAG chain bridges the gap between information retrieval and natural language understanding, creating a seamless experience where users can ask questions in plain language and receive detailed, relevant answers. This integration of retrieval and generation is what sets modern search engines apart, allowing them to provide much more than just a list of links or documents."
      ],
      "metadata": {
        "id": "uAbLD2NA9gby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_template = \"You are a helpful assistant that answers questions based on the provided context.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "human_template = \"Context: {context}\\n\\nQuestion: {question}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    system_message_prompt,\n",
        "    human_message_prompt\n",
        "])\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "chain = (\n",
        "    {\"context\": lambda x: format_docs(vector_store.similarity_search(x)), \"question\": RunnablePassthrough()}\n",
        "    | chat_prompt\n",
        "    | llm\n",
        ")\n",
        "logging.info(\"Successfully created RAG chain\")"
      ],
      "metadata": {
        "id": "ZGUXQQmv9ge4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Executing a Sample Query and Performing Semantic Search\n",
        "Finally, we bring everything together by executing a sample query. This step demonstrates the full power of the system we've built. The search engine processes the query, retrieves the most relevant documents from the vector store, and uses the language model to generate a coherent response. This step is where all the components of our system—Couchbase, OpenAi embeddings, the vector store, the cache, and the language model—work together to deliver a result that is not only accurate but also contextually appropriate.\n",
        "\n",
        "This final step is a testament to the effectiveness of semantic search. It shows how our search engine can understand and respond to complex queries, providing users with the information they need in a format that's easy to understand. By combining advanced AI models with efficient database management, we've created a search engine that goes beyond traditional keyword-based search, offering a richer and more intuitive search experience."
      ],
      "metadata": {
        "id": "hXu4Jcad9giD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What caused the 1929 Great Depression?\"\n",
        "\n",
        "# Get responses\n",
        "start_time = time.time()\n",
        "rag_response = chain.invoke(query)\n",
        "rag_elapsed_time = time.time() - start_time\n",
        "logging.info(f\"RAG response generated in {rag_elapsed_time:.2f} seconds\")\n",
        "\n",
        "print(f\"RAG Response: {rag_response.content}\")\n",
        "\n",
        "# Perform semantic search\n",
        "try:\n",
        "    search_results = vector_store.similarity_search_with_score(query, k=10)\n",
        "    results = [{'id': doc.metadata.get('id', 'N/A'), 'text': doc.page_content, 'distance': score}\n",
        "               for doc, score in search_results]\n",
        "    elapsed_time = time.time() - start_time\n",
        "    logging.info(f\"Semantic search completed in {elapsed_time:.2f} seconds\")\n",
        "except CouchbaseException as e:\n",
        "    raise RuntimeError(f\"Error performing semantic search: {str(e)}\")\n",
        "\n",
        "print(f\"\\nSemantic Search Results (completed in {elapsed_time:.2f} seconds):\")\n",
        "for result in results:\n",
        "    print(f\"Distance: {result['distance']:.4f}, Text: {result['text']}\")\n"
      ],
      "metadata": {
        "id": "aIdayPzw9glT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f21385-216b-4186-c780-b545c6dcd368"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Response: Based on the context provided, the world entered a global depression in 1929. This event is commonly known as \"the Great Depression.\" While the context doesn't provide specific causes for the Great Depression, it's generally understood that it was triggered by a combination of factors, including:\n",
            "\n",
            "1. The stock market crash of 1929\n",
            "2. Bank failures\n",
            "3. A decline in consumer spending and investment\n",
            "4. International economic instabilities\n",
            "\n",
            "It's important to note that the exact causes of the Great Depression are complex and have been debated by historians and economists. The context doesn't provide detailed information about these specific causes, but it does confirm that 1929 was the year when the global depression began.\n",
            "\n",
            "Semantic Search Results (completed in 4.74 seconds):\n",
            "Distance: 0.9177, Text: Why did the world enter a global depression in 1929 ?\n",
            "Distance: 0.8714, Text: When was `` the Great Depression '' ?\n",
            "Distance: 0.8116, Text: What crop failure caused the Irish Famine ?\n",
            "Distance: 0.7984, Text: What historical event happened in Dogtown in 1899 ?\n",
            "Distance: 0.7918, Text: What caused the Lynmouth floods ?\n",
            "Distance: 0.7916, Text: When was the first Wall Street Journal published ?\n",
            "Distance: 0.7912, Text: When did the Dow first reach ?\n",
            "Distance: 0.7885, Text: What were popular songs and types of songs in the 1920s ?\n",
            "Distance: 0.7858, Text: When did World War I start ?\n",
            "Distance: 0.7842, Text: What caused Harry Houdini 's death ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By following these steps, you’ll have a fully functional semantic search engine that leverages the strengths of Couchbase and Claude. This guide is designed not just to show you how to build the system, but also to explain why each step is necessary, giving you a deeper understanding of the principles behind semantic search and how to implement it effectively. Whether you’re a newcomer to software development or an experienced developer looking to expand your skills, this guide will provide you with the knowledge and tools you need to create a powerful, AI-driven search engine."
      ],
      "metadata": {
        "id": "yJQ5P8E29go1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M5m2lMOb9gr-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aYGDAbGY9gvV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t8uZvGFO9gzA"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}