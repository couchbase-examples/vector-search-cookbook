{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "251b8fa3",
   "metadata": {},
   "source": [
    "# Semantic Search with Couchbase GSI Vector Search and Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c7d51d",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c2adca",
   "metadata": {},
   "source": [
    "In this guide, we will walk you through building a powerful semantic search engine using Couchbase as the backend database and [Hugging Face](https://huggingface.co/) as the AI-powered embedding model provider. Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval.\n",
    "\n",
    "This tutorial demonstrates how to leverage Couchbase's **Global Secondary Index (GSI) vector search capabilities** with Hugging Face embeddings to create a high-performance semantic search system. GSI vector search in Couchbase offers significant advantages over traditional FTS (Full-Text Search) approaches, particularly for vector-first workloads and scenarios requiring complex filtering with high query-per-second (QPS) performance.\n",
    "\n",
    "This guide is designed to be comprehensive yet accessible, with clear step-by-step instructions that will equip you with the knowledge to create a fully functional semantic search system. Whether you're building a recommendation engine, content discovery platform, or any application requiring intelligent document retrieval, this tutorial provides the foundation you need.\n",
    "\n",
    "**Note**: If you want to perform semantic search using the FTS (Full-Text Search) index instead, please take a look at [this alternative approach](https://developer.couchbase.com//tutorial-huggingface-couchbase-vector-search-with-fts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebcc4ec",
   "metadata": {},
   "source": [
    "## How to Run This Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55ceff",
   "metadata": {},
   "source": [
    "This tutorial is available as a Jupyter Notebook (`.ipynb` file) that you can run interactively. You can access the original notebook [here](https://github.com/couchbase-examples/vector-search-cookbook/blob/main/huggingface/gsi/hugging_face.ipynb).\n",
    "\n",
    "You can either download the notebook file and run it on [Google Colab](https://colab.research.google.com/) or run it on your system by setting up the Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b3538",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6acf675",
   "metadata": {},
   "source": [
    "### Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64560670",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet langchain-couchbase==0.5.0 transformers==4.56.1 sentence_transformers==5.1.0 langchain_huggingface==0.3.1 python-dotenv==1.1.1 ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b877640",
   "metadata": {},
   "source": [
    "### Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b06f138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer\n",
    "from langchain_huggingface.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.options import ClusterOptions\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_couchbase.cache import CouchbaseCache\n",
    "from langchain_couchbase.vectorstores import CouchbaseQueryVectorStore\n",
    "from langchain_couchbase.vectorstores import DistanceStrategy\n",
    "from langchain_couchbase.vectorstores import IndexType\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ad3a4",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc1bf0a",
   "metadata": {},
   "source": [
    "To run this tutorial successfully, you will need the following requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5a3444",
   "metadata": {},
   "source": [
    "#### Couchbase Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84613bb",
   "metadata": {},
   "source": [
    "**Version Requirements:**\n",
    "- **Couchbase Server 8.0+** or **Couchbase Capella** with Query Service enabled\n",
    "- Note: GSI vector search is a newer feature that requires Couchbase Server 8.0 or above, unlike FTS-based vector search which works with 7.6+\n",
    "\n",
    "**Access Requirements:**\n",
    "- A configured Bucket, Scope, and Collection\n",
    "- User credentials with **Read and Write** access to your target collection\n",
    "- Network connectivity to your Couchbase cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e1c84",
   "metadata": {},
   "source": [
    "#### Create and Deploy Your Free Tier Operational Cluster on Capella"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217bfdd2",
   "metadata": {},
   "source": [
    "To get started with Couchbase Capella, create an account and use it to deploy a forever free tier operational cluster. This account provides you with an environment where you can explore and learn about Capella with no time constraint.\n",
    "\n",
    "To learn more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dae722",
   "metadata": {},
   "source": [
    "#### Couchbase Capella Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360e9b0",
   "metadata": {},
   "source": [
    "When running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met:\n",
    "\n",
    "* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the required bucket (Read and Write) used in the application.\n",
    "* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827906f6",
   "metadata": {},
   "source": [
    "#### Python Environment Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0450be34",
   "metadata": {},
   "source": [
    "- **Python 3.8+** \n",
    "- Required Python packages (installed via pip in the next section):\n",
    "  - `langchain-couchbase==0.5.0rc1`\n",
    "  - `transformers==4.56.1` \n",
    "  - `sentence_transformers==5.1.0`\n",
    "  - `langchain_huggingface==0.3.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "464ff771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(\"./.env\")\n",
    "\n",
    "# Configuration\n",
    "couchbase_cluster_url = os.getenv('CB_CLUSTER_URL') or input(\"Couchbase Cluster URL:\")\n",
    "couchbase_username = os.getenv('CB_USERNAME') or input(\"Couchbase Username:\")\n",
    "couchbase_password = os.getenv('CB_PASSWORD') or getpass.getpass(\"Couchbase password:\")\n",
    "couchbase_bucket = os.getenv('CB_BUCKET') or input(\"Couchbase Bucket:\")\n",
    "couchbase_scope = os.getenv('CB_SCOPE') or input(\"Couchbase Scope:\")\n",
    "couchbase_collection = os.getenv('CB_COLLECTION') or input(\"Couchbase Collection:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72cf81",
   "metadata": {},
   "source": [
    "## Couchbase Connection Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5f9a2",
   "metadata": {},
   "source": [
    "### Create Authentication Object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a0637d",
   "metadata": {},
   "source": [
    "In this section, we first need to create a `PasswordAuthenticator` object that would hold our Couchbase credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b131ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = PasswordAuthenticator(\n",
    "    couchbase_username,\n",
    "    couchbase_password\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1266276",
   "metadata": {},
   "source": [
    "### Connect to Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9669e4b",
   "metadata": {},
   "source": [
    "Then, we use this object to connect to Couchbase Cluster and select specified above bucket, scope and collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61bd6302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to cluster at URL: couchbase://localhost\n",
      "Connected to the cluster\n"
     ]
    }
   ],
   "source": [
    "print(\"Connecting to cluster at URL: \" + couchbase_cluster_url)\n",
    "cluster = Cluster(couchbase_cluster_url, ClusterOptions(auth))\n",
    "cluster.wait_until_ready(timedelta(seconds=5))\n",
    "\n",
    "bucket = cluster.bucket(couchbase_bucket)\n",
    "scope = bucket.scope(couchbase_scope)\n",
    "collection = scope.collection(couchbase_collection)\n",
    "print(\"Connected to the cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c546353",
   "metadata": {},
   "source": [
    "## Understanding GSI Vector Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154912ee",
   "metadata": {},
   "source": [
    "### Optimizing Vector Search with Global Secondary Index (GSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de6df3",
   "metadata": {},
   "source": [
    "With Couchbase 8.0+, you can leverage the power of GSI-based vector search, which offers significant performance improvements over traditional Full-Text Search (FTS) approaches for vector-first workloads. GSI vector search provides high-performance vector similarity search with advanced filtering capabilities and is designed to scale to billions of vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e22ebc",
   "metadata": {},
   "source": [
    "#### GSI vs FTS: Choosing the Right Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7259e62",
   "metadata": {},
   "source": [
    "| Feature               | GSI Vector Search                                               | FTS Vector Search                         |\n",
    "| --------------------- | --------------------------------------------------------------- | ----------------------------------------- |\n",
    "| **Best For**          | Vector-first workloads, complex filtering, high QPS performance| Hybrid search and high recall rates      |\n",
    "| **Couchbase Version** | 8.0.0+                                                         | 7.6+                                      |\n",
    "| **Filtering**         | Pre-filtering with `WHERE` clauses (Composite) or post-filtering (BHIVE) | Pre-filtering with flexible ordering |\n",
    "| **Scalability**       | Up to billions of vectors (BHIVE)                              | Up to 10 million vectors                  |\n",
    "| **Performance**       | Optimized for concurrent operations with low memory footprint  | Good for mixed text and vector queries   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ed398",
   "metadata": {},
   "source": [
    "#### GSI Vector Index Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855bf6c8",
   "metadata": {},
   "source": [
    "Couchbase offers two distinct GSI vector index types, each optimized for different use cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6789c3",
   "metadata": {},
   "source": [
    "##### Hyperscale Vector Indexes (BHIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3181c3",
   "metadata": {},
   "source": [
    "- **Best for**: Pure vector searches like content discovery, recommendations, and semantic search\n",
    "- **Use when**: You primarily perform vector-only queries without complex scalar filtering\n",
    "- **Features**: \n",
    "  - High performance with low memory footprint\n",
    "  - Optimized for concurrent operations\n",
    "  - Designed to scale to billions of vectors\n",
    "  - Supports post-scan filtering for basic metadata filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a534a2",
   "metadata": {},
   "source": [
    "##### Composite Vector Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a142bb",
   "metadata": {},
   "source": [
    "- **Best for**: Filtered vector searches that combine vector similarity with scalar value filtering\n",
    "- **Use when**: Your queries combine vector similarity with scalar filters that eliminate large portions of data\n",
    "- **Features**: \n",
    "  - Efficient pre-filtering where scalar attributes reduce the vector comparison scope\n",
    "  - Best for well-defined workloads requiring complex filtering using GSI features\n",
    "  - Supports range lookups combined with vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995acfa6",
   "metadata": {},
   "source": [
    "#### Index Type Selection for This Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac316b5",
   "metadata": {},
   "source": [
    "In this tutorial, we'll demonstrate creating a **BHIVE index** and running vector similarity queries using GSI. BHIVE is ideal for semantic search scenarios where you want:\n",
    "\n",
    "1. **High-performance vector search** across large datasets\n",
    "2. **Low latency** for real-time applications\n",
    "3. **Scalability** to handle growing vector collections\n",
    "4. **Concurrent operations** for multi-user environments\n",
    "\n",
    "The BHIVE index will provide optimal performance for our Hugging Face embedding-based semantic search implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da42658",
   "metadata": {},
   "source": [
    "#### Alternative: Composite Vector Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d8501c",
   "metadata": {},
   "source": [
    "If your use case requires complex filtering with scalar attributes, you may want to consider using a **Composite Vector Index** instead:\n",
    "\n",
    "```python\n",
    "# Alternative: Create a Composite index for filtered searches\n",
    "vector_store.create_index(\n",
    "    index_type=IndexType.COMPOSITE,\n",
    "    index_description=\"IVF,SQ8\",\n",
    "    distance_metric=DistanceStrategy.COSINE,\n",
    "    index_name=\"huggingface_composite_index\",\n",
    ")\n",
    "```\n",
    "\n",
    "**Use Composite indexes when:**\n",
    "- You need to filter by document metadata or attributes before vector similarity\n",
    "- Your queries combine vector search with WHERE clauses\n",
    "- You have well-defined filtering requirements that can reduce the search space\n",
    "\n",
    "**Note**: Composite indexes enable pre-filtering with scalar attributes, making them ideal for applications where you need to search within specific categories, date ranges, or user-specific data segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9588f3",
   "metadata": {},
   "source": [
    "#### Understanding GSI Index Configuration (Couchbase 8.0 Feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528616ee",
   "metadata": {},
   "source": [
    "Before creating our BHIVE index, it's important to understand the configuration parameters that optimize vector storage and search performance. The `index_description` parameter controls how Couchbase optimizes vector storage through centroids and quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889eeaf0",
   "metadata": {},
   "source": [
    "##### Index Description Format: `'IVF[<centroids>],{PQ|SQ}<settings>'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aee0fd",
   "metadata": {},
   "source": [
    "###### Centroids (IVF - Inverted File)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96944800",
   "metadata": {},
   "source": [
    "- Controls how the dataset is subdivided for faster searches\n",
    "- **More centroids** = faster search, slower training time\n",
    "- **Fewer centroids** = slower search, faster training time\n",
    "- If omitted (like `IVF,SQ8`), Couchbase auto-selects based on dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c6a79",
   "metadata": {},
   "source": [
    "###### Quantization Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8595fcb",
   "metadata": {},
   "source": [
    "**Scalar Quantization (SQ):**\n",
    "- `SQ4`, `SQ6`, `SQ8` (4, 6, or 8 bits per dimension)\n",
    "- Lower memory usage, faster search, slightly reduced accuracy\n",
    "\n",
    "**Product Quantization (PQ):**\n",
    "- Format: `PQ<subquantizers>x<bits>` (e.g., `PQ32x8`)\n",
    "- Better compression for very large datasets\n",
    "- More complex but can maintain accuracy with smaller index size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df99f84",
   "metadata": {},
   "source": [
    "###### Common Configuration Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2823bc5f",
   "metadata": {},
   "source": [
    "- **`IVF,SQ8`** - Auto centroids, 8-bit scalar quantization (good default)\n",
    "- **`IVF1000,SQ6`** - 1000 centroids, 6-bit scalar quantization\n",
    "- **`IVF,PQ32x8`** - Auto centroids, 32 subquantizers with 8 bits\n",
    "\n",
    "For detailed configuration options, see the [Quantization & Centroid Settings](https://docs.couchbase.com/cloud/vector-index/hyperscale-vector-index.html#algo_settings).\n",
    "\n",
    "For more information on GSI vector indexes, see [Couchbase GSI Vector Documentation](https://docs.couchbase.com/cloud/vector-index/use-vector-indexes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5761c4eb",
   "metadata": {},
   "source": [
    "##### Our Configuration Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa5296",
   "metadata": {},
   "source": [
    "In this tutorial, we use `IVF,SQ8` which provides:\n",
    "- **Auto-selected centroids** optimized for our dataset size\n",
    "- **8-bit scalar quantization** for good balance of speed, memory usage, and accuracy\n",
    "- **COSINE distance metric** ideal for semantic similarity search\n",
    "- **Optimal performance** for most semantic search use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e83fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BHIVE GSI vector index (good default: IVF,SQ8)\n",
    "vector_store = CouchbaseQueryVectorStore(\n",
    "    cluster=cluster,\n",
    "    bucket_name=couchbase_bucket,\n",
    "    scope_name=couchbase_scope,\n",
    "    collection_name=couchbase_collection,\n",
    "    embedding=HuggingFaceEmbeddings(), # Hugging Face Initialization\n",
    "    distance_metric=DistanceStrategy.COSINE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3d0cfc",
   "metadata": {},
   "source": [
    "## Document Processing and Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad369d8",
   "metadata": {},
   "source": [
    "### Embedding Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba8456",
   "metadata": {},
   "source": [
    "Now that we have set up our vector store with Hugging Face embeddings, we can add documents to our collection. The `CouchbaseQueryVectorStore` automatically handles the embedding generation process using the Hugging Face transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4244077b",
   "metadata": {},
   "source": [
    "#### Understanding the Embedding Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cdd922",
   "metadata": {},
   "source": [
    "When we add text documents to our vector store, several important processes happen automatically:\n",
    "\n",
    "1. **Text Preprocessing**: The input text is preprocessed and tokenized according to the Hugging Face model's requirements\n",
    "2. **Vector Generation**: Each document is converted into a high-dimensional vector (embedding) that captures its semantic meaning\n",
    "3. **Storage**: The embeddings are stored in Couchbase along with the original text and any metadata\n",
    "4. **Indexing**: The vectors are indexed using our BHIVE GSI index for efficient similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c487bb8",
   "metadata": {},
   "source": [
    "#### Adding Sample Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45318804",
   "metadata": {},
   "source": [
    "In this example, we're adding sample documents that demonstrate Couchbase's capabilities. The system will:\n",
    "- Generate embeddings for each text document using the Hugging Face model\n",
    "- Store them in our Couchbase collection\n",
    "- Make them immediately available for semantic search once the GSI index is ready\n",
    "\n",
    "**Note**: The `batch_size` parameter controls how many documents are processed together, which can help optimize performance for large document sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed198f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7c601881e4bf4c53b5b4c2a25628d904',\n",
       " '0442f351aec2415481138315d492ee80',\n",
       " 'e20a8dcd8b464e8e819b87c9a0ff05c3']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON’s versatility, with a foundation that is extremely fast and scalable.\",\n",
    "    \"It’s used across industries for things like user profiles, dynamic product catalogs, GenAI apps, vector search, high-speed caching, and much more.\",\n",
    "    input(\"Enter custom embedding text:\")\n",
    "]\n",
    "vector_store.add_texts(texts=texts, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a2497f",
   "metadata": {},
   "source": [
    "## Vector Search Performance Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb6515",
   "metadata": {},
   "source": [
    "Now let's demonstrate the performance benefits of different optimization approaches available in Couchbase. We'll compare three optimization levels to show how each contributes to building a production-ready semantic search system:\n",
    "\n",
    "1. **Baseline (Raw Search)**: Basic vector similarity search without GSI optimization\n",
    "2. **GSI-Optimized Search**: High-performance search using BHIVE GSI index\n",
    "3. **Cache Benefits**: Show how caching can be applied on top of any search approach\n",
    "\n",
    "**Important**: Caching is orthogonal to index types - you can apply caching benefits to both raw searches and GSI-optimized searches to improve repeated query performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32138349",
   "metadata": {},
   "source": [
    "### Understanding Vector Search Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29ba255",
   "metadata": {},
   "source": [
    "Before we start our RAG comparisons, let's understand what the search results mean:\n",
    "\n",
    "When you perform a search query with vector search:\n",
    "\n",
    "1. **Query Embedding**: Your search text is converted into a vector embedding using the Hugging Face model\n",
    "2. **Vector Similarity Calculation**: The system compares your query vector against all stored document vectors\n",
    "3. **Distance Computation**: Using the COSINE distance metric, the system calculates similarity distances\n",
    "4. **Result Ranking**: Documents are ranked by their distance values (lower = more similar)\n",
    "5. **Post-processing**: Results include both the document content and metadata\n",
    "\n",
    "**Note**: The returned value represents the vector distance between query and document embeddings. Lower distance values indicate higher similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066ba9d",
   "metadata": {},
   "source": [
    "### RAG Search Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6703df0",
   "metadata": {},
   "source": [
    "Let's create a comprehensive search function for our RAG performance comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fb710ff",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def search_with_performance_metrics(query_text, stage_name, k=3):\n",
    "    \"\"\"Perform optimized semantic search with detailed performance metrics\"\"\"\n",
    "    print(f\"\\n=== {stage_name.upper()} ===\")\n",
    "    print(f\"Query: \\\"{query_text}\\\"\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = vector_store.similarity_search_with_score(query_text, k=k)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    search_time = end_time - start_time\n",
    "    print(f\"Search Time: {search_time:.4f} seconds\")\n",
    "    print(f\"Results Found: {len(results)} documents\")\n",
    "    \n",
    "    for i, (doc, distance) in enumerate(results, 1):\n",
    "        print(f\"\\n[Result {i}]\")\n",
    "        print(f\"Vector Distance: {distance:.6f} (lower = more similar)\")\n",
    "        # Use the document content directly from search results (no additional KV call needed)\n",
    "        print(f\"Document Content: {doc.page_content}\")\n",
    "        if hasattr(doc, 'metadata') and doc.metadata:\n",
    "            print(f\"Metadata: {doc.metadata}\")\n",
    "    \n",
    "    return search_time, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8093ac9a",
   "metadata": {},
   "source": [
    "### Phase 1: Baseline Performance (Raw Vector Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07579883",
   "metadata": {},
   "source": [
    "First, let's establish baseline performance with raw vector search - no GSI optimization yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd30f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing baseline performance without GSI optimization...\n",
      "\n",
      "=== PHASE 1: BASELINE VECTOR SEARCH ===\n",
      "Query: \"What are the key features of a scalable NoSQL database?\"\n",
      "Search Time: 0.1484 seconds\n",
      "Results Found: 3 documents\n",
      "\n",
      "[Result 1]\n",
      "Vector Distance: 0.586197 (lower = more similar)\n",
      "Document Content: Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON’s versatility, with a foundation that is extremely fast and scalable.\n",
      "\n",
      "[Result 2]\n",
      "Vector Distance: 0.645435 (lower = more similar)\n",
      "Document Content: It’s used across industries for things like user profiles, dynamic product catalogs, GenAI apps, vector search, high-speed caching, and much more.\n",
      "\n",
      "[Result 3]\n",
      "Vector Distance: 0.976888 (lower = more similar)\n",
      "Document Content: this is a sample text with the data \"hello\"\n"
     ]
    }
   ],
   "source": [
    "test_query = \"What are the key features of a scalable NoSQL database?\"\n",
    "print(\"Testing baseline performance without GSI optimization...\")\n",
    "baseline_time, baseline_results = search_with_performance_metrics(\n",
    "    test_query, \"Phase 1: Baseline Vector Search\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0486ab77",
   "metadata": {},
   "source": [
    "### Phase 2: Create BHIVE GSI Index and Test Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5758586f",
   "metadata": {},
   "source": [
    "Now let's create the BHIVE GSI index and measure the performance improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e2fa28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BHIVE GSI vector index...\n",
      "✓ BHIVE GSI vector index created successfully!\n",
      "Waiting for index to become available...\n",
      "\n",
      "Testing performance with BHIVE GSI optimization...\n",
      "\n",
      "=== PHASE 2: GSI-OPTIMIZED SEARCH ===\n",
      "Query: \"What are the key features of a scalable NoSQL database?\"\n",
      "Search Time: 0.0848 seconds\n",
      "Results Found: 3 documents\n",
      "\n",
      "[Result 1]\n",
      "Vector Distance: 0.586197 (lower = more similar)\n",
      "Document Content: Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON’s versatility, with a foundation that is extremely fast and scalable.\n",
      "\n",
      "[Result 2]\n",
      "Vector Distance: 0.645435 (lower = more similar)\n",
      "Document Content: It’s used across industries for things like user profiles, dynamic product catalogs, GenAI apps, vector search, high-speed caching, and much more.\n",
      "\n",
      "[Result 3]\n",
      "Vector Distance: 0.976888 (lower = more similar)\n",
      "Document Content: this is a sample text with the data \"hello\"\n"
     ]
    }
   ],
   "source": [
    "# Create BHIVE index for optimized vector search\n",
    "print(\"Creating BHIVE GSI vector index...\")\n",
    "try:\n",
    "    vector_store.create_index(\n",
    "        index_type=IndexType.BHIVE,\n",
    "        index_description=\"IVF,SQ8\",\n",
    "        distance_metric=DistanceStrategy.COSINE,\n",
    "        index_name=\"huggingface_bhive_index\",\n",
    "    )\n",
    "    print(\"✓ BHIVE GSI vector index created successfully!\")\n",
    "    \n",
    "    # Wait for index to become available\n",
    "    print(\"Waiting for index to become available...\")\n",
    "    time.sleep(3)\n",
    "    \n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(\"✓ BHIVE GSI vector index already exists, proceeding...\")\n",
    "    else:\n",
    "        print(f\"Error creating GSI index: {str(e)}\")\n",
    "\n",
    "# Test the same query with GSI optimization\n",
    "print(\"\\nTesting performance with BHIVE GSI optimization...\")\n",
    "gsi_time, gsi_results = search_with_performance_metrics(\n",
    "    test_query, \"Phase 2: GSI-Optimized Search\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18d80ec",
   "metadata": {},
   "source": [
    "### Phase 3: Demonstrate Cache Benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d312ab",
   "metadata": {},
   "source": [
    "Now let's show how caching can improve performance for repeated queries. **Note**: Caching benefits apply to both raw searches and GSI-optimized searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d52edb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Couchbase cache for improved performance on repeated queries...\n",
      "✓ Couchbase cache enabled!\n",
      "\n",
      "Testing cache benefits with a different query...\n",
      "First execution (cache miss):\n",
      "\n",
      "=== PHASE 3A: FIRST QUERY (CACHE MISS) ===\n",
      "Query: \"How does a distributed database handle high-speed operations?\"\n",
      "Search Time: 0.1024 seconds\n",
      "Results Found: 2 documents\n",
      "\n",
      "[Result 1]\n",
      "Vector Distance: 0.632770 (lower = more similar)\n",
      "Document Content: Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON’s versatility, with a foundation that is extremely fast and scalable.\n",
      "\n",
      "[Result 2]\n",
      "Vector Distance: 0.677951 (lower = more similar)\n",
      "Document Content: It’s used across industries for things like user profiles, dynamic product catalogs, GenAI apps, vector search, high-speed caching, and much more.\n",
      "\n",
      "Second execution (cache hit):\n",
      "\n",
      "=== PHASE 3B: REPEATED QUERY (CACHE HIT) ===\n",
      "Query: \"How does a distributed database handle high-speed operations?\"\n",
      "Search Time: 0.0289 seconds\n",
      "Results Found: 2 documents\n",
      "\n",
      "[Result 1]\n",
      "Vector Distance: 0.632770 (lower = more similar)\n",
      "Document Content: Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON’s versatility, with a foundation that is extremely fast and scalable.\n",
      "\n",
      "[Result 2]\n",
      "Vector Distance: 0.677951 (lower = more similar)\n",
      "Document Content: It’s used across industries for things like user profiles, dynamic product catalogs, GenAI apps, vector search, high-speed caching, and much more.\n"
     ]
    }
   ],
   "source": [
    "# Set up Couchbase cache (can be applied to any search approach)\n",
    "print(\"Setting up Couchbase cache for improved performance on repeated queries...\")\n",
    "cache = CouchbaseCache(\n",
    "    cluster=cluster,\n",
    "    bucket_name=couchbase_bucket,\n",
    "    scope_name=couchbase_scope,\n",
    "    collection_name=couchbase_collection,\n",
    ")\n",
    "set_llm_cache(cache)\n",
    "print(\"✓ Couchbase cache enabled!\")\n",
    "\n",
    "# Test cache benefits with the same query (should show improvement on second run)\n",
    "cache_query = \"How does a distributed database handle high-speed operations?\"\n",
    "\n",
    "print(\"\\nTesting cache benefits with a different query...\")\n",
    "print(\"First execution (cache miss):\")\n",
    "cache_time_1, _ = search_with_performance_metrics(\n",
    "    cache_query, \"Phase 3a: First Query (Cache Miss)\", k=2\n",
    ")\n",
    "\n",
    "print(\"\\nSecond execution (cache hit):\")\n",
    "cache_time_2, _ = search_with_performance_metrics(\n",
    "    cache_query, \"Phase 3b: Repeated Query (Cache Hit)\", k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09890e4",
   "metadata": {},
   "source": [
    "### Complete Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decacd85",
   "metadata": {},
   "source": [
    "Let's analyze the complete performance improvements across all optimization levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "640a1bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VECTOR SEARCH PERFORMANCE OPTIMIZATION SUMMARY\n",
      "================================================================================\n",
      "Phase 1 - Baseline (Raw Search):     0.1484 seconds\n",
      "Phase 2 - GSI-Optimized Search:      0.0848 seconds\n",
      "Phase 3 - Cache Benefits:\n",
      "  First execution (cache miss):      0.1024 seconds\n",
      "  Second execution (cache hit):      0.0289 seconds\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "OPTIMIZATION IMPACT ANALYSIS:\n",
      "--------------------------------------------------------------------------------\n",
      "GSI Index Benefit:      1.75x faster (42.8% improvement)\n",
      "Cache Benefit:          3.55x faster (71.8% improvement)\n",
      "\n",
      "Key Insights:\n",
      "• GSI optimization provides consistent performance benefits, especially with larger datasets\n",
      "• Caching benefits apply to both raw and GSI-optimized searches\n",
      "• Combined GSI + Cache provides the best performance for production applications\n",
      "• BHIVE indexes scale to billions of vectors with optimized concurrent operations\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VECTOR SEARCH PERFORMANCE OPTIMIZATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Phase 1 - Baseline (Raw Search):     {baseline_time:.4f} seconds\")\n",
    "print(f\"Phase 2 - GSI-Optimized Search:      {gsi_time:.4f} seconds\")\n",
    "print(f\"Phase 3 - Cache Benefits:\")\n",
    "print(f\"  First execution (cache miss):      {cache_time_1:.4f} seconds\")\n",
    "print(f\"  Second execution (cache hit):      {cache_time_2:.4f} seconds\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"OPTIMIZATION IMPACT ANALYSIS:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# GSI improvement analysis\n",
    "if gsi_time and baseline_time and gsi_time < baseline_time:\n",
    "    gsi_speedup = baseline_time / gsi_time\n",
    "    gsi_improvement = ((baseline_time - gsi_time) / baseline_time) * 100\n",
    "    print(f\"GSI Index Benefit:      {gsi_speedup:.2f}x faster ({gsi_improvement:.1f}% improvement)\")\n",
    "else:\n",
    "    print(f\"GSI Index Benefit:      Performance similar to baseline (may vary with dataset size)\")\n",
    "\n",
    "# Cache improvement analysis\n",
    "if cache_time_2 and cache_time_1 and cache_time_2 < cache_time_1:\n",
    "    cache_speedup = cache_time_1 / cache_time_2\n",
    "    cache_improvement = ((cache_time_1 - cache_time_2) / cache_time_1) * 100\n",
    "    print(f\"Cache Benefit:          {cache_speedup:.2f}x faster ({cache_improvement:.1f}% improvement)\")\n",
    "else:\n",
    "    print(f\"Cache Benefit:          No significant improvement (results may be cached already)\")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"• GSI optimization provides consistent performance benefits, especially with larger datasets\")\n",
    "print(f\"• Caching benefits apply to both raw and GSI-optimized searches\")\n",
    "print(f\"• Combined GSI + Cache provides the best performance for production applications\")\n",
    "print(f\"• BHIVE indexes scale to billions of vectors with optimized concurrent operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458a2df",
   "metadata": {},
   "source": [
    "### Interactive Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390746a",
   "metadata": {},
   "source": [
    "Try your own queries with the optimized search system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26b9d9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== INTERACTIVE GSI-OPTIMIZED SEARCH ===\n",
      "Query: \"What is the sample data?\"\n",
      "Search Time: 0.0812 seconds\n",
      "Results Found: 3 documents\n",
      "\n",
      "[Result 1]\n",
      "Vector Distance: 0.623644 (lower = more similar)\n",
      "Document Content: this is a sample text with the data \"hello\"\n",
      "\n",
      "[Result 2]\n",
      "Vector Distance: 0.860599 (lower = more similar)\n",
      "Document Content: It’s used across industries for things like user profiles, dynamic product catalogs, GenAI apps, vector search, high-speed caching, and much more.\n",
      "\n",
      "[Result 3]\n",
      "Vector Distance: 0.909207 (lower = more similar)\n",
      "Document Content: Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON’s versatility, with a foundation that is extremely fast and scalable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.08118820190429688,\n",
       " [(Document(id='e20a8dcd8b464e8e819b87c9a0ff05c3', metadata={}, page_content='this is a sample text with the data \"hello\"'),\n",
       "   0.6236441411684932),\n",
       "  (Document(id='0442f351aec2415481138315d492ee80', metadata={}, page_content='It’s used across industries for things like user profiles, dynamic product catalogs, GenAI apps, vector search, high-speed caching, and much more.'),\n",
       "   0.8605992009935179),\n",
       "  (Document(id='7c601881e4bf4c53b5b4c2a25628d904', metadata={}, page_content='Couchbase Server is a multipurpose, distributed database that fuses the strengths of relational databases such as SQL and ACID transactions with JSON’s versatility, with a foundation that is extremely fast and scalable.'),\n",
       "   0.9092065785676496)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_query = input(\"Enter your search query: \")\n",
    "search_with_performance_metrics(custom_query, \"Interactive GSI-Optimized Search\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c61cd3c",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118302ed",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "source": [
    "You have successfully built a powerful semantic search engine using Couchbase's GSI vector search capabilities and Hugging Face embeddings. This guide has walked you through the complete process of creating a high-performance vector search system that can scale to handle billions of documents."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
