{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrewAI with Couchbase Short-Term Memory\n",
    "\n",
    "This notebook demonstrates how to implement a custom storage backend for CrewAI's memory system using Couchbase and vector search. Here's a breakdown of each section:\n",
    "\n",
    "How to run this tutorial\n",
    "----------------------\n",
    "This tutorial is available as a Jupyter Notebook (.ipynb file) that you can run \n",
    "interactively. You can access the original notebook here.\n",
    "\n",
    "You can either:\n",
    "- Download the notebook file and run it on [Google Colab](https://colab.research.google.com)\n",
    "- Run it on your system by setting up the Python environment\n",
    "\n",
    "Before you start\n",
    "---------------\n",
    "\n",
    "1. Create and Deploy Your Free Tier Operational cluster on [Capella](https://cloud.couchbase.com/sign-up)\n",
    "   - To get started with [Couchbase Capella](https://cloud.couchbase.com), create an account and use it to deploy \n",
    "     a forever free tier operational cluster\n",
    "   - This account provides you with an environment where you can explore and learn \n",
    "     about Capella with no time constraint\n",
    "   - To know more, please follow the [Getting Started Guide](https://docs.couchbase.com/cloud/get-started/create-account.html)\n",
    "\n",
    "2. Couchbase Capella Configuration\n",
    "   When running Couchbase using Capella, the following prerequisites need to be met:\n",
    "   - Create the database credentials to access the required bucket (Read and Write) \n",
    "     used in the application\n",
    "   - Allow access to the Cluster from the IP on which the application is running by following the [Network Security documentation](https://docs.couchbase.com/cloud/security/security.html#public-access)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Libraries\n",
    "This section installs the necessary Python packages:\n",
    "- `crewai`: The main CrewAI framework\n",
    "- `langchain-couchbase`: LangChain integration for Couchbase\n",
    "- `langchain-openai`: LangChain integration for OpenAI\n",
    "- `python-dotenv`: For loading environment variables\n",
    "- `couchbase`: The Couchbase Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet crewai langchain-couchbase langchain-openai python-dotenv couchbase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\n",
    "The script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from typing import Any, Dict, List, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# CrewAI imports\n",
    "from crewai.memory.storage.rag_storage import RAGStorage\n",
    "from crewai.memory.short_term.short_term_memory import ShortTermMemory\n",
    "from crewai import Agent, Crew, Task, Process\n",
    "\n",
    "# Couchbase imports\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.options import ClusterOptions\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.diagnostics import PingState, ServiceType\n",
    "from langchain_couchbase.vectorstores import CouchbaseVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Sensitive Informnation\n",
    "In this section, we prompt the user to input essential configuration settings needed. These settings include sensitive information like database credentials, and specific configuration names. Instead of hardcoding these details into the script, we request the user to provide them at runtime, ensuring flexibility and security.\n",
    "\n",
    "The script uses environment variables to store sensitive information, enhancing the overall security and maintainability of your code by avoiding hardcoded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Verify environment variables\n",
    "required_vars = ['OPENAI_API_KEY', 'CB_HOST', 'CB_USERNAME', 'CB_PASSWORD']\n",
    "for var in required_vars:\n",
    "    if not os.getenv(var):\n",
    "        raise ValueError(f\"{var} environment variable is required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement CouchbaseStorage\n",
    "\n",
    "This section demonstrates the implementation of a custom vector storage solution using Couchbase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouchbaseStorage(RAGStorage):\n",
    "    def __init__(self, type: str, allow_reset: bool = True, embedder_config: Optional[Dict[str, Any]] = None, crew: Optional[Any] = None):\n",
    "        super().__init__(type, allow_reset, embedder_config, crew)\n",
    "        self._initialize_app()\n",
    "\n",
    "    def search(self, query: str, limit: int = 3, filter: Optional[dict] = None, score_threshold: float = 0) -> List[Dict[str, Any]]:\n",
    "        try:\n",
    "            # Add type filter\n",
    "            search_filter = {\"memory_type\": self.type}\n",
    "            if filter:\n",
    "                search_filter.update(filter)\n",
    "\n",
    "            # Execute search\n",
    "            results = self.vector_store.similarity_search_with_score(\n",
    "                query,\n",
    "                k=limit,\n",
    "                filter=search_filter\n",
    "            )\n",
    "            \n",
    "            # Format results and deduplicate by content\n",
    "            seen_contents = set()\n",
    "            formatted_results = []\n",
    "            \n",
    "            for i, (doc, score) in enumerate(results):\n",
    "                if score >= score_threshold:\n",
    "                    content = doc.page_content\n",
    "                    if content not in seen_contents:\n",
    "                        seen_contents.add(content)\n",
    "                        formatted_results.append({\n",
    "                            \"id\": doc.metadata.get(\"memory_id\", str(i)),\n",
    "                            \"metadata\": doc.metadata,\n",
    "                            \"context\": content,\n",
    "                            \"score\": float(score)\n",
    "                        })\n",
    "            return formatted_results\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Search failed: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def save(self, value: Any, metadata: Dict[str, Any]) -> None:\n",
    "        try:\n",
    "            # Generate unique ID\n",
    "            memory_id = str(uuid.uuid4())\n",
    "            timestamp = int(time.time() * 1000)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            if not metadata:\n",
    "                metadata = {}\n",
    "            metadata.update({\n",
    "                \"memory_id\": memory_id,\n",
    "                \"memory_type\": self.type,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"source\": \"crewai\"\n",
    "            })\n",
    "\n",
    "            # Convert value to string if needed\n",
    "            if isinstance(value, (dict, list)):\n",
    "                value = json.dumps(value)\n",
    "            elif not isinstance(value, str):\n",
    "                value = str(value)\n",
    "\n",
    "            # Save to vector store\n",
    "            self.vector_store.add_texts(\n",
    "                texts=[value],\n",
    "                metadatas=[metadata],\n",
    "                ids=[memory_id]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Save failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        if not self.allow_reset:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.cluster.query(\n",
    "                f\"DELETE FROM `{self.bucket_name}`.`{self.scope_name}`.`{self.collection_name}` WHERE memory_type = $type\",\n",
    "                type=self.type\n",
    "            ).execute()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Reset failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _initialize_app(self):\n",
    "        try:\n",
    "            # Initialize embeddings\n",
    "            if self.embedder_config and self.embedder_config.get(\"provider\") == \"openai\":\n",
    "                self.embeddings = OpenAIEmbeddings(\n",
    "                    openai_api_key=os.getenv('OPENAI_API_KEY'),\n",
    "                    model=self.embedder_config.get(\"config\", {}).get(\"model\", \"text-embedding-3-small\")\n",
    "                )\n",
    "            else:\n",
    "                self.embeddings = OpenAIEmbeddings(\n",
    "                    openai_api_key=os.getenv('OPENAI_API_KEY'),\n",
    "                    model=\"text-embedding-3-small\"\n",
    "                )\n",
    "\n",
    "            # Connect to Couchbase\n",
    "            auth = PasswordAuthenticator(\n",
    "                os.getenv('CB_USERNAME', ''),\n",
    "                os.getenv('CB_PASSWORD', '')\n",
    "            )\n",
    "            options = ClusterOptions(auth)\n",
    "            \n",
    "            # Initialize cluster connection\n",
    "            self.cluster = Cluster(os.getenv('CB_HOST', ''), options)\n",
    "            self.cluster.wait_until_ready(timedelta(seconds=5))\n",
    "\n",
    "            # Check search service\n",
    "            ping_result = self.cluster.ping()\n",
    "            search_available = False\n",
    "            for service_type, endpoints in ping_result.endpoints.items():\n",
    "                if service_type == ServiceType.Search:\n",
    "                    for endpoint in endpoints:\n",
    "                        if endpoint.state == PingState.OK:\n",
    "                            search_available = True\n",
    "                            break\n",
    "                    break\n",
    "            if not search_available:\n",
    "                raise RuntimeError(\"Search/FTS service not found or not responding\")\n",
    "            \n",
    "            # Set up storage configuration\n",
    "            self.bucket_name = os.getenv('CB_BUCKET_NAME', 'vector-search-testing')\n",
    "            self.scope_name = os.getenv('SCOPE_NAME', 'shared')\n",
    "            self.collection_name = os.getenv('COLLECTION_NAME', 'crew')\n",
    "            self.index_name = os.getenv('INDEX_NAME', 'vector_search_crew')\n",
    "\n",
    "            # Initialize vector store\n",
    "            self.vector_store = CouchbaseVectorStore(\n",
    "                cluster=self.cluster,\n",
    "                bucket_name=self.bucket_name,\n",
    "                scope_name=self.scope_name,\n",
    "                collection_name=self.collection_name,\n",
    "                embedding=self.embeddings,\n",
    "                index_name=self.index_name\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Initialization failed: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Basic Storage\n",
    "\n",
    "Test storing and retrieving a simple memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: Vector search enables semantic similarity matching by converting data into high-dimensional vectors\n",
      "Score: 0.6105956435203552\n",
      "Metadata: {}\n"
     ]
    }
   ],
   "source": [
    "# Initialize storage\n",
    "storage = CouchbaseStorage(\n",
    "    type=\"short_term\",\n",
    "    embedder_config={\n",
    "        \"provider\": \"openai\",\n",
    "        \"config\": {\"model\": \"text-embedding-3-small\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "# Reset storage\n",
    "storage.reset()\n",
    "\n",
    "# Test storage\n",
    "test_memory = \"Vector search enables semantic similarity matching by converting data into high-dimensional vectors\"\n",
    "test_metadata = {\"category\": \"technology\"}\n",
    "storage.save(test_memory, test_metadata)\n",
    "\n",
    "# Test search\n",
    "results = storage.search(\"What is vector search?\", limit=1)\n",
    "for result in results:\n",
    "    print(f\"Found: {result['context']}\\nScore: {result['score']}\\nMetadata: {result['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CrewAI Integration\n",
    "\n",
    "Create agents and tasks to test memory retention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m03:26:07 - LiteLLM:INFO\u001b[0m: utils.py:2896 - \n",
      "LiteLLM completion() model= gpt-4; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearch Expert\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mResearch vector search capabilities in modern databases. Focus on Couchbase vector search features.\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m03:26:21 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearch Expert\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "Vector search, also referred to as semantic search, is an innovative search capability that employs machine learning algorithms to not just match the keywords but understand the contextual meaning of the query. This helps in providing more accurate and relevant results that are in line with the user's intent.\n",
      "\n",
      "Vector search operates by converting the text into multi-dimensional vectors that encapsulate the semantic meaning of the text. When a search query is input, it is also converted into a vector. The database then finds and returns the vectors that are closest to the search query vector. This 'closeness' is calculated by measuring the cosine distance between vectors. This method allows the vector search to provide contextually relevant results, even if they don't contain the exact search keywords.\n",
      "\n",
      "Couchbase, a NoSQL database, has incorporated this advanced search capability in its Full Text Search (FTS) feature. Couchbase FTS allows for searching documents that contain certain words or phrases. The power of Couchbase FTS comes from the Bleve library, which enables full-text search and employs various techniques such as term stemming, phrase matching, and relevance scoring, among others.\n",
      "\n",
      "Couchbase FTS supports different kinds of queries, including match queries, phrase queries, and prefix queries. It also accommodates more complex queries like conjunction, disjunction, and boolean field queries. \n",
      "\n",
      "However, it is important to note that as of its current version, Couchbase does not natively support vector search. For enabling vector search capabilities in Couchbase, integration with other services that offer vector search, such as Elasticsearch, is required.\n",
      "\n",
      "There are several benefits of vector search, including more accurate search results, better handling of synonyms and related terms, and the ability to perform semantic search. However, the major drawback is the significant computational resources required to convert text into vectors and perform cosine distance calculations. \n",
      "\n",
      "In conclusion, while vector search capabilities provide several advantages in understanding the context and intent behind search queries, these capabilities are not natively supported in Couchbase. Integration with other services that offer these capabilities, such as Elasticsearch, is required to enable these features. Despite the computational requirements being high, the enhanced search accuracy and relevancy make it a valuable addition to your application.\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m03:26:22 - LiteLLM:INFO\u001b[0m: utils.py:2896 - \n",
      "LiteLLM completion() model= gpt-4; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4; provider = openai\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m03:26:33 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m03:26:38 - LiteLLM:INFO\u001b[0m: utils.py:2896 - \n",
      "LiteLLM completion() model= gpt-4; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTechnical Writer\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mCreate documentation about vector search findings, focusing on practical implementation details.\u001b[00m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m03:26:51 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTechnical Writer\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "# Vector Search Findings: Practical Implementation Details\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "Vector search, also known as semantic search, is a leading-edge search capability that employs machine learning algorithms to understand the contextual meaning of a query. This technology enables the provision of more accurate and relevant results, aligning with users' intent.\n",
      "\n",
      "**Conceptual Understanding of Vector Search**\n",
      "\n",
      "Vector search turns text into multi-dimensional vectors that capture its semantic meaning. When a search query is entered, it's also converted into a vector. The database then identifies and returns the vectors that are nearest to the search query vector. This 'nearness' is calculated by measuring the cosine distance between vectors, which allows vector search to provide contextually relevant results, even if they don't contain the exact search keywords.\n",
      "\n",
      "**Couchbase and Full Text Search (FTS)**\n",
      "\n",
      "Couchbase, a NoSQL database, has embedded this advanced search capability into its Full Text Search (FTS) feature. Couchbase FTS allows for searching documents that contain specific words or phrases. The strength of Couchbase FTS originates from the Bleve library, which enables full-text search and employs various techniques such as term stemming, phrase matching, and relevance scoring, among others.\n",
      "\n",
      "Couchbase FTS supports various kinds of queries, including match queries, phrase queries, and prefix queries. It also supports complex queries like conjunction, disjunction, and boolean field queries.\n",
      "\n",
      "**Integration with Elasticsearch**\n",
      "\n",
      "However, Couchbase does not natively support vector search as of its current version. To enable vector search capabilities in Couchbase, integration with other services that offer vector search, such as Elasticsearch, is required. Elasticsearch is a search engine that can be integrated with Couchbase to enable vector search capabilities.\n",
      "\n",
      "**Benefits and Drawbacks**\n",
      "\n",
      "The benefits of vector search include more accurate search results, better handling of synonyms and related terms, and the ability to perform semantic search. The major drawback is the significant computational resources required to convert text into vectors and perform cosine distance calculations.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "In conclusion, while vector search capabilities provide several advantages in understanding the context and intent behind search queries, these capabilities are not natively supported in Couchbase. Integration with other services that offer these capabilities, such as Elasticsearch, is required to enable these features. Despite the computational requirements being high, the enhanced search accuracy and relevancy make it a valuable addition to your application.\u001b[00m\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m03:26:52 - LiteLLM:INFO\u001b[0m: utils.py:2896 - \n",
      "LiteLLM completion() model= gpt-4; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4; provider = openai\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m03:27:03 - LiteLLM:INFO\u001b[0m: utils.py:1084 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crew Result:\n",
      " --------------------------------------------------------------------------------\n",
      "# Vector Search Findings: Practical Implementation Details\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "Vector search, also known as semantic search, is a leading-edge search capability that employs machine learning algorithms to understand the contextual meaning of a query. This technology enables the provision of more accurate and relevant results, aligning with users' intent.\n",
      "\n",
      "**Conceptual Understanding of Vector Search**\n",
      "\n",
      "Vector search turns text into multi-dimensional vectors that capture its semantic meaning. When a search query is entered, it's also converted into a vector. The database then identifies and returns the vectors that are nearest to the search query vector. This 'nearness' is calculated by measuring the cosine distance between vectors, which allows vector search to provide contextually relevant results, even if they don't contain the exact search keywords.\n",
      "\n",
      "**Couchbase and Full Text Search (FTS)**\n",
      "\n",
      "Couchbase, a NoSQL database, has embedded this advanced search capability into its Full Text Search (FTS) feature. Couchbase FTS allows for searching documents that contain specific words or phrases. The strength of Couchbase FTS originates from the Bleve library, which enables full-text search and employs various techniques such as term stemming, phrase matching, and relevance scoring, among others.\n",
      "\n",
      "Couchbase FTS supports various kinds of queries, including match queries, phrase queries, and prefix queries. It also supports complex queries like conjunction, disjunction, and boolean field queries.\n",
      "\n",
      "**Integration with Elasticsearch**\n",
      "\n",
      "However, Couchbase does not natively support vector search as of its current version. To enable vector search capabilities in Couchbase, integration with other services that offer vector search, such as Elasticsearch, is required. Elasticsearch is a search engine that can be integrated with Couchbase to enable vector search capabilities.\n",
      "\n",
      "**Benefits and Drawbacks**\n",
      "\n",
      "The benefits of vector search include more accurate search results, better handling of synonyms and related terms, and the ability to perform semantic search. The major drawback is the significant computational resources required to convert text into vectors and perform cosine distance calculations.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "In conclusion, while vector search capabilities provide several advantages in understanding the context and intent behind search queries, these capabilities are not natively supported in Couchbase. Integration with other services that offer these capabilities, such as Elasticsearch, is required to enable these features. Despite the computational requirements being high, the enhanced search accuracy and relevancy make it a valuable addition to your application.\n"
     ]
    }
   ],
   "source": [
    "# Initialize language model\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "\n",
    "# Create agents\n",
    "researcher = Agent(\n",
    "    role='Research Expert',\n",
    "    goal='Research vector search capabilities',\n",
    "    backstory='Expert at finding and analyzing information about vector search technology',\n",
    "    llm=llm,\n",
    "    memory=True,\n",
    "    memory_storage=ShortTermMemory(storage=storage)\n",
    ")\n",
    "\n",
    "writer = Agent(\n",
    "    role='Technical Writer',\n",
    "    goal='Create clear documentation',\n",
    "    backstory='Expert at technical documentation',\n",
    "    llm=llm,\n",
    "    memory=True,\n",
    "    memory_storage=ShortTermMemory(storage=storage)\n",
    ")\n",
    "\n",
    "# Create tasks\n",
    "research_task = Task(\n",
    "    description='Research vector search capabilities in modern databases. Focus on Couchbase vector search features.',\n",
    "    agent=researcher,\n",
    "    expected_output=\"A comprehensive analysis of vector search capabilities in modern databases.\"\n",
    ")\n",
    "\n",
    "writing_task = Task(\n",
    "    description='Create documentation about vector search findings, focusing on practical implementation details.',\n",
    "    agent=writer,\n",
    "    context=[research_task],\n",
    "    expected_output=\"A well-structured technical document explaining vector search implementation.\"\n",
    ")\n",
    "\n",
    "# Create crew\n",
    "crew = Crew(\n",
    "    agents=[researcher, writer],\n",
    "    tasks=[research_task, writing_task],\n",
    "    process=Process.sequential,\n",
    "    memory=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run crew\n",
    "result = crew.kickoff()\n",
    "print(\"\\nCrew Result:\\n\", \"-\"*80)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Memory Retention\n",
    "\n",
    "Query the stored memories to verify retention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory Search Results:\n",
      " --------------------------------------------------------------------------------\n",
      "Context: Vector search enables semantic similarity matching by converting data into high-dimensional vectors\n",
      "Score: 0.5251279473304749\n",
      "Metadata: {}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Wait for memories to be stored\n",
    "time.sleep(2)\n",
    "\n",
    "# Query memories\n",
    "memory_results = storage.search(\n",
    "    query=\"What are the key features of vector search in Couchbase?\",\n",
    "    limit=2,\n",
    "    score_threshold=0.0\n",
    ")\n",
    "\n",
    "print(\"\\nMemory Search Results:\\n\", \"-\"*80)\n",
    "for result in memory_results:\n",
    "    print(f\"Context: {result['context']}\")\n",
    "    print(f\"Score: {result['score']}\")\n",
    "    print(f\"Metadata: {result['metadata']}\")\n",
    "    print(\"-\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
