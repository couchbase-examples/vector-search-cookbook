{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this guide, we will walk you through building a Retrieval Augmented Generation (RAG) application using Couchbase Capella as the database, [gpt-4o](https://platform.openai.com/docs/models/gpt-4o) model as the large language model provided by OpenAI. We will use the [text-embedding-3-large](https://platform.openai.com/docs/guides/embeddings/embedding-models) model for generating embeddings.\n",
    "\n",
    "This notebook demonstrates how to build a RAG system using:\n",
    "- The [BBC News dataset](https://huggingface.co/datasets/RealTimeData/bbc_news_alltime) containing news articles\n",
    "- Couchbase Capella as the vector store with query-based vector indexing for vector search\n",
    "- LlamaIndex framework for the RAG pipeline\n",
    "- OpenAI for embeddings and text generation\n",
    "\n",
    "We leverage Couchbase's query-based vector search capabilities to create and manage vector indexes, enabling efficient semantic search capabilities. The Query Service provides high-performance vector search with support for both Hyperscale Vector Indexes and Composite Vector Indexes, designed to scale to billions of vectors with low memory footprint and optimized concurrent operations.\n",
    "\n",
    "Semantic search goes beyond simple keyword matching by understanding the context and meaning behind the words in a query, making it an essential tool for applications that require intelligent information retrieval. This tutorial will equip you with the knowledge to create a fully functional RAG system using OpenAI Services and LlamaIndex with Couchbase's advanced Hyperscale and Composite Vector Index search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Before you start\n\n### Create and Deploy Your Operational cluster on Capella\n\nTo get started with Couchbase Capella, create an account and use it to deploy an operational cluster.\n\nTo know more, please follow the [instructions](https://docs.couchbase.com/cloud/get-started/create-account.html). "
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Couchbase Capella Configuration\n\nWhen running Couchbase using [Capella](https://cloud.couchbase.com/sign-in), the following prerequisites need to be met:\n\n* Have a multi-node Capella cluster running the Data, Query, Index, and Search services.\n* Create the [database credentials](https://docs.couchbase.com/cloud/clusters/manage-database-users.html) to access the bucket (Read and Write) used in the application.\n* [Allow access](https://docs.couchbase.com/cloud/clusters/allow-ip-address.html) to the Cluster from the IP on which the application is running.\n\n### OpenAI Models Setup\n\nIn order to create the RAG application, we need an embedding model to ingest the documents for Vector Search and a large language model (LLM) for generating the responses based on the context. \n\nFor this implementation, we'll use OpenAI's models which provide state-of-the-art performance for both embeddings and text generation:\n\n**Embedding Model**: We'll use OpenAI's `text-embedding-3-large` model, which provides high-quality embeddings with 3,072 dimensions for semantic search capabilities.\n\n**Large Language Model**: We'll use OpenAI's `gpt-4o` model for generating responses based on the retrieved context. This model offers excellent reasoning capabilities and can handle complex queries effectively.\n\n**Prerequisites for OpenAI Integration**:\n* Create an OpenAI account at [platform.openai.com](https://platform.openai.com)\n* Generate an API key from your OpenAI dashboard\n* Ensure you have sufficient credits or a valid payment method set up\n* Set up your API key as an environment variable or input it securely in the notebook\n\nFor more details about OpenAI's models and pricing, please refer to the [OpenAI documentation](https://platform.openai.com/docs/models)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Necessary Libraries\nTo build our RAG system, we need a set of libraries. The libraries we install handle everything from connecting to databases to performing AI tasks. Each library has a specific role: Couchbase libraries manage database operations, LlamaIndex handles AI model integrations, and we will use the OpenAI SDK for generating embeddings and calling OpenAI's language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install datasets llama-index-vector-stores-couchbase==0.6.0 llama-index-embeddings-openai==0.5.1 llama-index-llms-openai==0.5.6 llama-index==0.14.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries\nThe script starts by importing a series of libraries required for various tasks, including handling JSON, logging, time tracking, Couchbase connections, embedding generation, and dataset loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import base64\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from couchbase.auth import PasswordAuthenticator\n",
    "from couchbase.cluster import Cluster\n",
    "from couchbase.exceptions import CouchbaseException\n",
    "from couchbase.options import ClusterOptions, KnownConfigProfiles, QueryOptions\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from llama_index.core import Settings, Document\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.couchbase import CouchbaseQueryVectorStore\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.couchbase import CouchbaseQueryVectorStore, QueryVectorSearchSimilarity, QueryVectorSearchType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Sensitive Information\n",
    "In this section, we prompt the user to input essential configuration settings needed. These settings include sensitive information like database credentials, collection names, and API keys. Instead of hardcoding these details into the script, we request the user to provide them at runtime, ensuring flexibility and security.\n",
    "\n",
    "The script also validates that all required inputs are provided, raising an error if any crucial information is missing. This approach ensures that your integration is both secure and correctly configured without hardcoding sensitive information, enhancing the overall security and maintainability of your code.\n",
    "\n",
    "**OPENAI_API_KEY** is your OpenAI API key which can be obtained from your OpenAI dashboard at [platform.openai.com](https://platform.openai.com/api-keys).\n",
    "\n",
    "**INDEX_NAME** is the name of the vector index we will create for vector search operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB_CONNECTION_STRING = input(\"Couchbase Cluster URL (default: localhost): \") or \"localhost\"\n",
    "CB_USERNAME = input(\"Couchbase Username (default: admin): \") or \"admin\"\n",
    "CB_PASSWORD = input(\"Couchbase password (default: Password@12345): \") or \"Password@12345\"\n",
    "CB_BUCKET_NAME = input(\"Couchbase Bucket: \")\n",
    "SCOPE_NAME = input(\"Couchbase Scope: \")\n",
    "COLLECTION_NAME = input(\"Couchbase Collection: \")\n",
    "INDEX_NAME = input(\"Vector Search Index: \")\n",
    "OPENAI_API_KEY = input(\"OpenAI API Key: \")\n",
    "\n",
    "# Check if the variables are correctly loaded\n",
    "if not all([CB_CONNECTION_STRING, CB_USERNAME, CB_PASSWORD, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME, INDEX_NAME, OPENAI_API_KEY]):\n",
    "    raise ValueError(\"All configuration variables must be provided.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Logging\nLogging is essential for tracking the execution of our script and debugging any issues that may arise. We set up a logger that will display information about the script's progress, including timestamps and log levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Couchbase Capella\nThe next step is to establish a connection to our Couchbase Capella cluster. This connection will allow us to interact with the database, store and retrieve documents, and perform vector searches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize the Couchbase Cluster\n",
    "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
    "    options = ClusterOptions(auth)\n",
    "    options.apply_profile(KnownConfigProfiles.WanDevelopment)\n",
    "    # Connect to the cluster\n",
    "    cluster = Cluster(CB_CONNECTION_STRING, options)\n",
    "    \n",
    "    # Wait for the cluster to be ready\n",
    "    cluster.wait_until_ready(timedelta(seconds=5))\n",
    "\n",
    "    logging.info(\"Successfully connected to the Couchbase cluster\")\n",
    "except CouchbaseException as e:\n",
    "    raise RuntimeError(f\"Failed to connect to Couchbase: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Bucket, Scope, and Collection\nBefore we can store our data, we need to ensure that the appropriate bucket, scope, and collection exist in our Couchbase cluster. The code below checks if these components exist and creates them if they don't, providing a foundation for storing our vector embeddings and documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from couchbase.management.buckets import CreateBucketSettings\n",
    "import json\n",
    "\n",
    "# Create bucket if it does not exist\n",
    "bucket_manager = cluster.buckets()\n",
    "try:\n",
    "    bucket_manager.get_bucket(CB_BUCKET_NAME)\n",
    "    print(f\"Bucket '{CB_BUCKET_NAME}' already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Bucket '{CB_BUCKET_NAME}' does not exist. Creating bucket...\")\n",
    "    bucket_settings = CreateBucketSettings(name=CB_BUCKET_NAME, ram_quota_mb=500)\n",
    "    bucket_manager.create_bucket(bucket_settings)\n",
    "    print(f\"Bucket '{CB_BUCKET_NAME}' created successfully.\")\n",
    "\n",
    "# Create scope and collection if they do not exist\n",
    "collection_manager = cluster.bucket(CB_BUCKET_NAME).collections()\n",
    "scopes = collection_manager.get_all_scopes()\n",
    "scope_exists = any(scope.name == SCOPE_NAME for scope in scopes)\n",
    "\n",
    "if scope_exists:\n",
    "    print(f\"Scope '{SCOPE_NAME}' already exists.\")\n",
    "else:\n",
    "    print(f\"Scope '{SCOPE_NAME}' does not exist. Creating scope...\")\n",
    "    collection_manager.create_scope(SCOPE_NAME)\n",
    "    print(f\"Scope '{SCOPE_NAME}' created successfully.\")\n",
    "\n",
    "collections = [collection.name for scope in scopes if scope.name == SCOPE_NAME for collection in scope.collections]\n",
    "collection_exists = COLLECTION_NAME in collections\n",
    "\n",
    "if collection_exists:\n",
    "    print(f\"Collection '{COLLECTION_NAME}' already exists in scope '{SCOPE_NAME}'.\")\n",
    "else:\n",
    "    print(f\"Collection '{COLLECTION_NAME}' does not exist in scope '{SCOPE_NAME}'. Creating collection...\")\n",
    "    collection_manager.create_collection(collection_name=COLLECTION_NAME, scope_name=SCOPE_NAME)\n",
    "    print(f\"Collection '{COLLECTION_NAME}' created successfully.\")\n",
    "\n",
    "scope = cluster.bucket(CB_BUCKET_NAME).scope(SCOPE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Query-Based Vector Search\n",
    "In this section, we'll set up the Couchbase vector store using Couchbase Hyperscale and Composite Vector Index for high-performance vector search. Unlike FTS-based vector search, Hyperscale and Composite Vector Index search provides optimized performance for pure vector similarity operations and can scale to billions of vectors with low memory footprint.\n",
    "\n",
    "Hyperscale and Composite Vector Index search supports two main index types:\n",
    "- **Hyperscale Vector Indexes**: Best for pure vector searches with high performance and concurrent operations\n",
    "- **Composite Vector Indexes**: Best for filtered vector searches combining vector similarity with scalar filtering\n",
    "\n",
    "For this tutorial, we'll use the Query vector store.\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the BBC News Dataset\nTo build a RAG engine, we need data to search through. We use the [BBC Realtime News dataset](https://huggingface.co/datasets/RealTimeData/bbc_news_alltime), a dataset with up-to-date BBC news articles grouped by month. This dataset contains articles that were created after the LLM was trained. It will showcase the use of RAG to augment the LLM. \n\nThe BBC News dataset's varied content allows us to simulate real-world scenarios where users ask complex questions, enabling us to fine-tune our RAG's ability to understand and respond to various types of queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    news_dataset = load_dataset('RealTimeData/bbc_news_alltime', '2024-12', split=\"train\")\n",
    "    print(f\"Loaded the BBC News dataset with {len(news_dataset)} rows\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error loading BBC News dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first two examples from the dataset\n",
    "print(\"Dataset columns:\", news_dataset.column_names)\n",
    "print(\"\\nFirst two examples:\")\n",
    "print(news_dataset[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for RAG\n\nWe need to extract the context passages from the dataset to use as our knowledge base for the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "news_articles = news_dataset\n",
    "unique_articles = {}\n",
    "\n",
    "for article in news_articles:\n",
    "    content = article.get(\"content\")\n",
    "    if content:\n",
    "        content_hash = hashlib.md5(content.encode()).hexdigest()  # Generate hash of content\n",
    "        if content_hash not in unique_articles:\n",
    "            unique_articles[content_hash] = article  # Store full article\n",
    "\n",
    "unique_news_articles = list(unique_articles.values())  # Convert back to list\n",
    "\n",
    "print(f\"We have {len(unique_news_articles)} unique articles in our database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings using OpenAI\nEmbeddings are numerical representations of text that capture semantic meaning. Unlike keyword-based search, embeddings enable semantic search to understand context and retrieve documents that are conceptually similar even without exact keyword matches. We'll use OpenAI's `text-embedding-3-large` model to create high-quality embeddings with 3,072 dimensions. This model transforms our text data into vector representations that can be efficiently searched, with a batch size of 30 for optimal processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Set up the embedding model\n",
    "    embed_model = OpenAIEmbedding(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        embed_batch_size=30,\n",
    "        model=\"text-embedding-3-large\"\n",
    "    )\n",
    "    \n",
    "    # Configure LlamaIndex to use this embedding model\n",
    "    Settings.embed_model = embed_model\n",
    "    print(\"Successfully created embedding model\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating embedding model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Embeddings Model\nWe can test the embeddings model by generating an embedding for a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding = embed_model.get_text_embedding(\"this is a test sentence\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Understanding Hyperscale and Composite Vector Search\n\n### Optimizing Vector Search with Hyperscale and Composite Vector Index\n\nWith Couchbase 8.0+, you can leverage the power of query-based vector search, which offers significant performance improvements over traditional Full-Text Search (FTS) approaches for vector-first workloads. Hyperscale and Composite Vector Index search provides high-performance vector similarity search with advanced filtering capabilities and is designed to scale to billions of vectors.\n\n#### Hyperscale/Composite vs Search Vector Index: Choosing the Right Approach\n\n| Feature               | Hyperscale/Composite Vector Index                                               | Search Vector Index                         |\n| --------------------- | --------------------------------------------------------------- | ----------------------------------------- |\n| **Best For**          | Vector-first workloads, complex filtering, high QPS performance| Hybrid search and high recall rates      |\n| **Couchbase Version** | 8.0.0+                                                         | 7.6+                                      |\n| **Filtering**         | Pre-filtering with `WHERE` clauses (Composite) or post-filtering (Hyperscale) | Pre-filtering with flexible ordering |\n| **Scalability**       | Up to billions of vectors (Hyperscale)                              | Up to 10 million vectors                  |\n| **Performance**       | Optimized for concurrent operations with low memory footprint  | Good for mixed text and vector queries   |\n\n#### Query-Based Vector Index Types\n\nCouchbase offers two distinct query-based vector index types, each optimized for different use cases:\n\n##### Hyperscale Vector Indexes\n\n- **Best for**: Pure vector searches like content discovery, recommendations, and semantic search\n- **Use when**: You primarily perform vector-only queries without complex scalar filtering\n- **Features**: \n  - High performance with low memory footprint\n  - Optimized for concurrent operations\n  - Designed to scale to billions of vectors\n  - Supports post-scan filtering for basic metadata filtering\n\n##### Composite Vector Indexes\n\n- **Best for**: Filtered vector searches that combine vector similarity with scalar value filtering\n- **Use when**: Your queries combine vector similarity with scalar filters that eliminate large portions of data\n- **Features**: \n  - Efficient pre-filtering where scalar attributes reduce the vector comparison scope\n  - Best for well-defined workloads requiring complex filtering\n  - Supports range lookups combined with vector search\n\n#### Understanding Index Configuration\n\nThe `index_description` parameter controls how Couchbase optimizes vector storage through centroids and quantization.\n\n##### Index Description Format: `'IVF[<centroids>],{PQ|SQ}<settings>'`\n\n**Centroids (IVF - Inverted File)**:\n- Controls how the dataset is subdivided for faster searches\n- More centroids = faster search, slower training\n- If omitted (like `IVF,SQ8`), Couchbase auto-selects based on dataset size\n\n**Quantization Options**:\n- **SQ (Scalar Quantization)**: SQ4, SQ6, SQ8 (4, 6, or 8 bits per dimension)\n- **PQ (Product Quantization)**: PQ<subquantizers>x<bits> (e.g., PQ32x8)\n\nFor detailed configuration options, see the [Quantization & Centroid Settings](https://docs.couchbase.com/cloud/vector-index/hyperscale-vector-index.html#algo_settings).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Couchbase Query Vector Store\n",
    "The query vector store is set up to store the documents from the dataset using Couchbase's query-based vector search capabilities. This vector store is optimized for high-performance vector similarity search operations and can scale to billions of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create the Couchbase query vector store\n",
    "    vector_store = CouchbaseQueryVectorStore(\n",
    "        cluster=cluster,\n",
    "        bucket_name=CB_BUCKET_NAME,\n",
    "        scope_name=SCOPE_NAME,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        search_type=QueryVectorSearchType.ANN,\n",
    "        similarity=QueryVectorSearchSimilarity.DOT,\n",
    "        nprobes=10\n",
    "    )\n",
    "    print(\"Successfully created query vector store\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Failed to create query vector store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating LlamaIndex Documents\nIn this section, we'll process our news articles and create LlamaIndex Document objects.\nEach Document is created with specific metadata and formatting templates to control what the LLM and embedding model see.\nWe'll observe examples of the formatted content to understand how the documents are structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "llama_documents = []\n",
    "# Process and store documents\n",
    "for article in unique_news_articles:  # Limit to first 100 for demo\n",
    "    try:\n",
    "        document = Document(\n",
    "            text=article[\"content\"],\n",
    "            metadata={\n",
    "                \"title\": article[\"title\"],\n",
    "                \"description\": article[\"description\"],\n",
    "                \"published_date\": article[\"published_date\"],\n",
    "                \"link\": article[\"link\"],\n",
    "            },\n",
    "            excluded_llm_metadata_keys=[\"description\"],\n",
    "            excluded_embed_metadata_keys=[\"description\", \"published_date\", \"link\"],\n",
    "            metadata_template=\"{key}=>{value}\",\n",
    "            text_template=\"Metadata: \\n{metadata_str}\\n-----\\nContent: {content}\",\n",
    "        )\n",
    "        llama_documents.append(document)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save document to vector store: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Observing an example of what the LLM and Embedding model receive as input\n",
    "print(\"The LLM sees this:\")\n",
    "print(llama_documents[0].get_content(metadata_mode=MetadataMode.LLM))\n",
    "print(\"The Embedding model sees this:\")\n",
    "print(llama_documents[0].get_content(metadata_mode=MetadataMode.EMBED))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Running the Ingestion Pipeline\n\nIn this section, we'll create an ingestion pipeline to process our documents. The pipeline will:\n\n1. Split the documents into smaller chunks (nodes) using the SentenceSplitter\n2. Generate embeddings for each node using our embedding model\n3. Store these nodes with their embeddings in our Couchbase vector store\n\nThis process transforms our raw documents into a searchable knowledge base that can be queried semantically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Process documents: split into nodes, generate embeddings, and store in vector database\n",
    "# Step 3: Create and Run IndexPipeline\n",
    "index_pipeline = IngestionPipeline(\n",
    "    transformations=[SentenceSplitter(),embed_model], \n",
    "    vector_store=vector_store,\n",
    ")\n",
    "\n",
    "index_pipeline.run(documents=llama_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenAI's Large Language Model (LLM)\nLarge language models are AI systems that are trained to understand and generate human language. We'll be using OpenAI's `gpt-4o` model to process user queries and generate meaningful responses based on the retrieved context from our Couchbase vector store. This model is a key component of our RAG system, allowing it to go beyond simple keyword matching and truly understand the intent behind a query. By integrating OpenAI's LLM, we equip our RAG system with the ability to interpret complex queries, understand the nuances of language, and provide more accurate and contextually relevant responses.\n\nThe language model's ability to understand context and generate coherent responses is what makes our RAG system truly intelligent. It can not only find the right information but also present it in a way that is useful and understandable to the user.\n\nThe LLM is configured using LlamaIndex's OpenAI-like provider with OpenAI's API endpoint and your OpenAI API key for seamless integration with their services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Set up the LLM\n",
    "    llm = OpenAI(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        model=\"gpt-4o\",\n",
    "        \n",
    "    )\n",
    "    # Configure LlamaIndex to use this LLM\n",
    "    Settings.llm = llm\n",
    "    logging.info(\"Successfully created the OpenAI LLM\")\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error creating OpenAI LLM: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Vector Store Index\n\nIn this section, we'll create a VectorStoreIndex from our Couchbase vector store. This index serves as the foundation for our RAG system, enabling semantic search capabilities and efficient retrieval of relevant information.\n\nThe VectorStoreIndex provides a high-level interface to interact with our vector store, allowing us to:\n1. Perform semantic searches based on user queries\n2. Retrieve the most relevant documents or chunks\n3. Generate contextually appropriate responses using our LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "rag = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Understanding Semantic Search in Couchbase\n\nSemantic search goes beyond traditional keyword matching by understanding the meaning and context behind queries. Here's how it works in Couchbase:\n\n### How Semantic Search Works\n\n1. **Vector Embeddings**: Documents and queries are converted into high-dimensional vectors using an embeddings model (in our case, OpenAI's text-embedding-3-large model)\n\n2. **Similarity Calculation**: When a query is made, Couchbase compares the query vector against stored document vectors using the DOT product similarity metric\n\n3. **Result Ranking**: Documents are ranked by their vector similarity scores\n\n4. **Flexible Configuration**: Different similarity metrics (dot product, cosine, euclidean) and embedding models can be used based on your needs\n\nNow let's see semantic search in action and measure its performance with different optimization strategies.\n\n## Vector Search Performance Testing\n\n### Phase 1: Baseline Performance (No Hyperscale Index)\n\nFirst, we'll run a RAG query without using a Hyperscale index to establish our baseline performance. This search uses linear brute force which compares the query vector against every document in the collection. This works for small datasets but can become slow as the dataset grows."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample query from the dataset\n",
    "\n",
    "query = \"Who will Daniel Dubois fight in Saudi Arabia on 22 February?\"\n",
    "\n",
    "try:\n",
    "    # Perform the semantic search\n",
    "    start_time = time.time()\n",
    "    response = rag.query(query)\n",
    "    search_elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Display search results\n",
    "    print(f\"\\nSemantic Search Results (completed in {search_elapsed_time:.2f} seconds):\")\n",
    "    print(response)\n",
    "\n",
    "except RecursionError as e:\n",
    "    raise RuntimeError(f\"Error performing semantic search: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Creating the Hyperscale Index\n\nNow we'll create a Hyperscale index to significantly improve query performance. The index uses IVF (Inverted File) with PQ (Product Quantization) for optimal balance between speed and accuracy.\n\nThe index configuration uses:\n- **IVF1024**: 1024 centroids for fast search\n- **PQ32x8**: Product quantization with 32 subquantizers and 8 bits each\n- **DOT similarity**: Optimized for dot product similarity matching"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Hyperscale Vector Index for optimized vector search\ntry:\n    hyperscale_index_name = f\"{INDEX_NAME}_hyperscale\"\n\n    options = {\n        \"dimension\": 3072,\n        \"description\": \"IVF1024,PQ32x8\",\n        \"similarity\": \"DOT\",\n    }\n    scope.query(\n        f\"\"\"\n        CREATE INDEX {hyperscale_index_name}\n        ON {COLLECTION_NAME} (embedding VECTOR)\n        USING GSI WITH {json.dumps(options)}\n        \"\"\",\n    QueryOptions(\n        timeout=timedelta(seconds=300)\n    )).execute()\n    print(f\"Successfully created Hyperscale index: {hyperscale_index_name}\")\nexcept Exception as e:\n    print(f\"Hyperscale index may already exist or error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Phase 2: Hyperscale-Optimized Performance\n\nNow let's run the same RAG query using the Hyperscale index we just created. You'll notice improved performance as the index efficiently retrieves data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the optimized vector search with Hyperscale index\n",
    "query = \"Who will Daniel Dubois fight in Saudi Arabia on 22 February?\"\n",
    "try:\n",
    "    # Create a new query engine using the optimized vector store\n",
    "    optimized_rag = index.as_query_engine()\n",
    "    \n",
    "    # Perform the semantic search with Hyperscale optimization\n",
    "    start_time = time.time()\n",
    "    response = optimized_rag.query(query)\n",
    "    search_elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Display search results\n",
    "    print(f\"\\nOptimized Hyperscale Vector Search Results (completed in {search_elapsed_time:.2f} seconds):\")\n",
    "    print(response)\n",
    "\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error performing optimized semantic search: {e}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Performance Analysis Summary\n\nThe performance comparison between Phase 1 (baseline) and Phase 2 (Hyperscale-optimized) demonstrates the significant benefits of using Hyperscale indexes for vector search:\n\n| Metric | Phase 1 (Baseline) | Phase 2 (Hyperscale) | Improvement |\n|--------|-------------------|---------------------|-------------|\n| Search Method | Brute force | IVF + PQ indexed | Optimized |\n| Index Type | None | Hyperscale (IVF1024,PQ32x8) | Optimized |\n| Scalability | Limited | Billions of vectors | High |\n\n**Key Takeaways:**\n- **Hyperscale indexes** provide substantial performance improvements for semantic search\n- The **IVF + PQ configuration** offers an excellent balance of speed and accuracy\n- For production workloads, always create appropriate indexes based on your query patterns\n- Consider **Composite indexes** when you need to combine vector search with scalar filtering",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n\nYou've built a RAG system using Couchbase Hyperscale/Composite indexes with OpenAI and LlamaIndex. For the Search Vector Index alternative, see the [search_based tutorial](https://developer.couchbase.com/tutorial-openai-llamaindex-couchbase-rag-with-search-vector-index)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}