{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYUkZqeoEykk",
        "outputId": "d2237190-e36b-4b5d-bf73-511bea52525b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: couchbase in /usr/local/lib/python3.10/dist-packages (4.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.10/dist-packages (0.2.34)\n",
            "Requirement already satisfied: langchain_couchbase in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: langchain_voyageai in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.1.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (0.1.104)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (4.12.2)\n",
            "Requirement already satisfied: voyageai<1,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from langchain_voyageai) (0.2.3)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.42.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (0.27.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (3.10.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.7.24)\n",
            "Requirement already satisfied: aiolimiter<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from voyageai<1,>=0.2.1->langchain_voyageai) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain_core) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain_core) (0.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install python-dotenv\n",
        "!pip install python-dotenv couchbase datasets langchain_core langchain_couchbase langchain_voyageai langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Necessary Libraries\n",
        "This block imports all the required libraries and modules used in the notebook. These include libraries for environment management, data handling, natural language processing, interaction with Couchbase, and embeddings generation. Each library serves a specific function, such as managing environment variables, handling datasets, or interacting with the Couchbase database."
      ],
      "metadata": {
        "id": "Dw3IL3GEJSj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import getpass\n",
        "from datetime import timedelta\n",
        "from uuid import uuid4\n",
        "\n",
        "import numpy as np\n",
        "from couchbase.auth import PasswordAuthenticator\n",
        "from couchbase.cluster import Cluster\n",
        "from couchbase.exceptions import (CouchbaseException,\n",
        "                                  InternalServerFailureException,\n",
        "                                  QueryIndexAlreadyExistsException)\n",
        "from couchbase.management.search import SearchIndex\n",
        "from couchbase.options import ClusterOptions\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.globals import set_llm_cache\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_couchbase.cache import CouchbaseCache\n",
        "from langchain_couchbase.vectorstores import CouchbaseVectorStore\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_voyageai import VoyageAIEmbeddings\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "oziN03NZJLQw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Logging\n",
        "Logging is configured to track the progress of the script and capture any errors or warnings. This is crucial for debugging and understanding the flow of execution. The logging output includes timestamps, log levels (e.g., INFO, ERROR), and messages that describe what is happening in the script.\n"
      ],
      "metadata": {
        "id": "DbfOiG-8-QF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s',force=True)"
      ],
      "metadata": {
        "id": "gKcgEHlU-QLv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Environment Variables\n",
        " Environment variables, such as API keys and database credentials, are loaded using dotenv to securely manage sensitive information. We prompt the user for these values if they are not set.\n",
        "\n"
      ],
      "metadata": {
        "id": "zOwSwRoHJLXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOYAGE_API_KEY = getpass.getpass('Enter your VoyageAI API key: ')\n",
        "OPENAI_API_KEY = getpass.getpass('Enter your OpenAI API key: ')\n",
        "CB_HOST = input('Enter your Couchbase host (default: couchbase://localhost): ') or 'couchbase://localhost'\n",
        "CB_USERNAME = input('Enter your Couchbase username (default: Administrator): ') or 'Administrator'\n",
        "CB_PASSWORD = getpass.getpass('Enter your Couchbase password (default: password): ') or 'password'\n",
        "CB_BUCKET_NAME = input('Enter your Couchbase bucket name (default: vector-search-testing): ') or 'vector-search-testing'\n",
        "INDEX_NAME = input('Enter your index name (default: vector_search_voyage): ') or 'vector_search_voyage'\n",
        "SCOPE_NAME = input('Enter your scope name (default: shared): ') or 'shared'\n",
        "COLLECTION_NAME = input('Enter your collection name (default: voyage): ') or 'voyage'\n",
        "CACHE_COLLECTION = input('Enter your cache collection name (default: cache): ') or 'cache'\n",
        "\n",
        "# Verifying that essential environment variables are set\n",
        "if not VOYAGE_API_KEY:\n",
        "    raise ValueError(\"VOYAGE_API_KEY is required.\")\n",
        "if not OPENAI_API_KEY:\n",
        "    raise ValueError(\"OPENAI_API_KEY is required.\")"
      ],
      "metadata": {
        "id": "y2H9xphrJLbP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717a2731-8c04-44a4-a67c-f5cd4dcf492c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your VoyageAI API key: ··········\n",
            "Enter your OpenAI API key: ··········\n",
            "Enter your Couchbase host (default: couchbase://localhost): couchbases://cb.hlcup4o4jmjr55yf.cloud.couchbase.com\n",
            "Enter your Couchbase username (default: Administrator): vector-search-rag-demos\n",
            "Enter your Couchbase password (default: password): ··········\n",
            "Enter your Couchbase bucket name (default: vector-search-testing): \n",
            "Enter your index name (default: vector_search_voyage): \n",
            "Enter your scope name (default: shared): \n",
            "Enter your collection name (default: voyage): \n",
            "Enter your cache collection name (default: cache): \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to Couchbase\n",
        "The script attempts to establish a connection to the Couchbase database using the credentials retrieved from the environment variables. Couchbase is a NoSQL database known for its flexibility, scalability, and support for various data models, including document-based storage. The connection is authenticated using a username and password, and the script waits until the connection is fully established before proceeding.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sdKdLg9pJLl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    auth = PasswordAuthenticator(CB_USERNAME, CB_PASSWORD)\n",
        "    options = ClusterOptions(auth)\n",
        "    cluster = Cluster(CB_HOST, options)\n",
        "    cluster.wait_until_ready(timedelta(seconds=5))\n",
        "    logging.info(\"Successfully connected to Couchbase\")\n",
        "except Exception as e:\n",
        "    raise ConnectionError(f\"Failed to connect to Couchbase: {str(e)}\")"
      ],
      "metadata": {
        "id": "HubiGMCSJLqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8761de03-95af-4dbd-a85b-0ad1943d12f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-25 21:03:25,867 - INFO - Successfully connected to Couchbase\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up Collections in Couchbase\n",
        "In Couchbase, data is organized in buckets, which can be further divided into scopes and collections. Think of a collection as a table in a traditional SQL database. Before we can store any data, we need to ensure that our collections exist. If they don't, we must create them. This step is important because it prepares the database to handle the specific types of data our application will process. By setting up collections, we define the structure of our data storage, which is essential for efficient data retrieval and management.\n",
        "\n",
        "Moreover, setting up collections allows us to isolate different types of data within the same bucket, providing a more organized and scalable data structure. This is particularly useful when dealing with large datasets, as it ensures that related data is stored together, making it easier to manage and query.\n"
      ],
      "metadata": {
        "id": "Bft3THhTRctT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_collection(cluster, bucket_name, scope_name, collection_name):\n",
        "    try:\n",
        "        bucket = cluster.bucket(bucket_name)\n",
        "        bucket_manager = bucket.collections()\n",
        "\n",
        "        # Check if collection exists, create if it doesn't\n",
        "        collections = bucket_manager.get_all_scopes()\n",
        "        collection_exists = any(\n",
        "            scope.name == scope_name and collection_name in [col.name for col in scope.collections]\n",
        "            for scope in collections\n",
        "        )\n",
        "\n",
        "        if not collection_exists:\n",
        "            logging.info(f\"Collection '{collection_name}' does not exist. Creating it...\")\n",
        "            bucket_manager.create_collection(scope_name, collection_name)\n",
        "            logging.info(f\"Collection '{collection_name}' created successfully.\")\n",
        "        else:\n",
        "            logging.info(f\"Collection '{collection_name}' already exists.Skipping creation.\")\n",
        "\n",
        "        collection = bucket.scope(scope_name).collection(collection_name)\n",
        "\n",
        "        # Ensure primary index exists\n",
        "        try:\n",
        "            cluster.query(f\"CREATE PRIMARY INDEX IF NOT EXISTS ON `{bucket_name}`.`{scope_name}`.`{collection_name}`\").execute()\n",
        "            logging.info(\"Primary index present or created successfully.\")\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error creating primary index: {str(e)}\")\n",
        "\n",
        "        # Clear all documents in the collection\n",
        "        try:\n",
        "            query = f\"DELETE FROM `{bucket_name}`.`{scope_name}`.`{collection_name}`\"\n",
        "            cluster.query(query).execute()\n",
        "            logging.info(\"All documents cleared from the collection.\")\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error while clearing documents: {str(e)}. The collection might be empty.\")\n",
        "\n",
        "        return collection\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error setting up collection: {str(e)}\")\n",
        "\n",
        "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, COLLECTION_NAME)\n",
        "setup_collection(cluster, CB_BUCKET_NAME, SCOPE_NAME, CACHE_COLLECTION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXPrLjBGRc14",
        "outputId": "62dd6fef-c342-4c9d-d847-7db7beb2546a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-25 21:03:26,061 - INFO - Collection 'voyage' already exists.Skipping creation.\n",
            "2024-08-25 21:03:26,091 - INFO - Primary index present or created successfully.\n",
            "2024-08-25 21:03:27,334 - INFO - All documents cleared from the collection.\n",
            "2024-08-25 21:03:27,504 - INFO - Collection 'cache' already exists.Skipping creation.\n",
            "2024-08-25 21:03:27,536 - INFO - Primary index present or created successfully.\n",
            "2024-08-25 21:03:27,571 - INFO - All documents cleared from the collection.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<couchbase.collection.Collection at 0x7a2d0def1360>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Index Definition\n",
        "The search index definition is loaded from a JSON file. This index defines how the data in Couchbase should be indexed for fast search and retrieval. Indexing is critical for optimizing search queries, especially when dealing with large datasets. The JSON file contains details about the index, such as its name, source type, and parameters."
      ],
      "metadata": {
        "id": "TXGj5YokJLuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# index_definition_path = '/path_to_your_index_file/voyage_index.json'\n",
        "\n",
        "# Prompt user to upload to google drive\n",
        "from google.colab import files\n",
        "print(\"Upload your index definition file\")\n",
        "uploaded = files.upload()\n",
        "index_definition_path = list(uploaded.keys())[0]\n",
        "\n",
        "try:\n",
        "    with open(index_definition_path, 'r') as file:\n",
        "        index_definition = json.load(file)\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error loading index definition from {index_definition_path}: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "OkCMCb0-Rnbn",
        "outputId": "b7077367-8f37-4430-e329-75b526a9a226"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your index definition file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-045e8309-f8e8-484c-8a50-eefd5391f3e8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-045e8309-f8e8-484c-8a50-eefd5391f3e8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving voyage_index.json to voyage_index (1).json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create or Update Search Index\n",
        "The script checks if the search index already exists in Couchbase. If it exists, the index is updated; if not, a new index is created. This step ensures that the data is properly indexed, allowing for efficient search operations later in the script. The index is associated with a specific bucket, scope, and collection in Couchbase, which organizes the data.\n"
      ],
      "metadata": {
        "id": "7vM2unopRnfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    scope_index_manager = cluster.bucket(CB_BUCKET_NAME).scope(SCOPE_NAME).search_indexes()\n",
        "\n",
        "    # Check if index already exists\n",
        "    existing_indexes = scope_index_manager.get_all_indexes()\n",
        "    index_name = index_definition[\"name\"]\n",
        "\n",
        "    if index_name in [index.name for index in existing_indexes]:\n",
        "        logging.info(f\"Index '{index_name}' found\")\n",
        "    else:\n",
        "        logging.info(f\"Creating new index '{index_name}'...\")\n",
        "\n",
        "    # Create SearchIndex object from JSON definition\n",
        "    search_index = SearchIndex.from_json(index_definition)\n",
        "\n",
        "    # Upsert the index (create if not exists, update if exists)\n",
        "    scope_index_manager.upsert_index(search_index)\n",
        "    logging.info(f\"Index '{index_name}' successfully created/updated.\")\n",
        "\n",
        "except QueryIndexAlreadyExistsException:\n",
        "    logging.info(f\"Index '{index_name}' already exists. Skipping creation/update.\")\n",
        "\n",
        "except InternalServerFailureException as e:\n",
        "    error_message = str(e)\n",
        "    logging.error(f\"InternalServerFailureException raised: {error_message}\")\n",
        "\n",
        "    try:\n",
        "        # Accessing the response_body attribute from the context\n",
        "        error_context = e.context\n",
        "        response_body = error_context.response_body\n",
        "        if response_body:\n",
        "            error_details = json.loads(response_body)\n",
        "            error_message = error_details.get('error', '')\n",
        "\n",
        "            if \"collection: 'voyage' doesn't belong to scope: 'shared'\" in error_message:\n",
        "                raise ValueError(\"Collection 'voyage' does not belong to scope 'shared'. Please check the collection and scope names.\")\n",
        "\n",
        "    except ValueError as ve:\n",
        "        logging.error(str(ve))\n",
        "        raise\n",
        "\n",
        "    except Exception as json_error:\n",
        "        logging.error(f\"Failed to parse the error message: {json_error}\")\n",
        "        raise RuntimeError(f\"Internal server error while creating/updating search index: {error_message}\")"
      ],
      "metadata": {
        "id": "VHeB_AVmLJlx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e37544-3d69-4ca6-b4f6-87e300673138"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-25 21:03:43,964 - INFO - Index 'vector_search_voyage' found\n",
            "2024-08-25 21:03:44,102 - INFO - Index 'vector_search_voyage' already exists. Skipping creation/update.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load TREC Dataset\n",
        "The TREC dataset is loaded using the datasets library. TREC is a well-known dataset used in information retrieval and natural language processing (NLP) tasks. In this script, the dataset will be used to generate embeddings, which are numerical representations of text that capture its meaning in a form suitable for machine learning models.\n"
      ],
      "metadata": {
        "id": "iar2fABrLJjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    trec = load_dataset('trec', split='train[:2000]')\n",
        "    logging.info(f\"Successfully loaded TREC dataset with {len(trec)} samples\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error loading TREC dataset: {str(e)}\")"
      ],
      "metadata": {
        "id": "cjASXR3dLJgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1ffca58-7204-417d-e0a4-f92102a55821"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "2024-08-25 21:03:47,292 - INFO - Successfully loaded TREC dataset with 2000 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Embeddings\n",
        "Embeddings are created using the Voyage API. Embeddings are vectors (arrays of numbers) that represent the meaning of text in a high-dimensional space. These embeddings are crucial for tasks like semantic search, where the goal is to find text that is semantically similar to a query. The script uses a pre-trained model provided by Voyage to generate embeddings for the text in the TREC dataset."
      ],
      "metadata": {
        "id": "LdmbxLdCLJdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    embeddings = VoyageAIEmbeddings(voyage_api_key=VOYAGE_API_KEY,model=\"voyage-large-2\")\n",
        "    logging.info(\"Successfully created VoyageAIEmbeddings\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error creating VoyageAIEmbeddings: {str(e)}\")"
      ],
      "metadata": {
        "id": "HrJbgaKRLJbS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c73dd38-f7c9-4d71-f29d-77368c53c77f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-25 21:03:47,303 - INFO - Successfully created VoyageAIEmbeddings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch size None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up Vector Store\n",
        "The vector store is set up to manage the embeddings created in the previous step. The vector store is essentially a database optimized for storing and retrieving high-dimensional vectors. In this case, the vector store is built on top of Couchbase, allowing the script to store the embeddings in a way that can be efficiently searched.\n"
      ],
      "metadata": {
        "id": "ToQ2acrSLJY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    vector_store = CouchbaseVectorStore(\n",
        "        cluster=cluster,\n",
        "        bucket_name=CB_BUCKET_NAME,\n",
        "        scope_name=SCOPE_NAME,\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        embedding=embeddings,\n",
        "        index_name=INDEX_NAME,\n",
        "    )\n",
        "    logging.info(\"Successfully created vector store\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Failed to create vector store: {str(e)}\")"
      ],
      "metadata": {
        "id": "qZDXvq88LJWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8765553-9554-4770-e8e1-c00e839a6104"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-25 21:03:47,848 - INFO - Successfully created vector store\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Data to Vector Store in Batches\n",
        "To avoid overloading memory, the TREC dataset's text fields are saved to the vector store in batches. This step is important for handling large datasets, as it breaks down the data into manageable chunks that can be processed sequentially. Each piece of text is converted into a document, assigned a unique identifier, and then stored in the vector store.\n"
      ],
      "metadata": {
        "id": "GQpib0zKLJTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    batch_size = 50\n",
        "    for i in tqdm(range(0, len(trec['text']), batch_size), desc=\"Processing Batches\"):\n",
        "        batch = trec['text'][i:i + batch_size]\n",
        "        documents = [Document(page_content=text) for text in batch]\n",
        "        uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "        vector_store.add_documents(documents=documents, ids=uuids)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to save documents to vector store: {str(e)}\")"
      ],
      "metadata": {
        "id": "7eV1X5xILJRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a67239ee-93bb-4d8d-c1aa-d13aaf23105c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Batches: 100%|██████████| 40/40 [01:28<00:00,  2.21s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up Cache\n",
        " A cache is set up using Couchbase to store intermediate results and frequently accessed data. Caching is important for improving performance, as it reduces the need to repeatedly calculate or retrieve the same data. The cache is linked to a specific collection in Couchbase, and it is used later in the script to store the results of language model queries.\n"
      ],
      "metadata": {
        "id": "Bt44X6-bLJOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    cache = CouchbaseCache(\n",
        "        cluster=cluster,\n",
        "        bucket_name=CB_BUCKET_NAME,\n",
        "        scope_name=SCOPE_NAME,\n",
        "        collection_name=CACHE_COLLECTION,\n",
        "    )\n",
        "    logging.info(\"Successfully created cache\")\n",
        "    set_llm_cache(cache)\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Failed to create cache: {str(e)}\")"
      ],
      "metadata": {
        "id": "6cGJfwS2LI_O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da02e5b-ef12-410f-e334-0f6a1b523eaa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-25 21:05:16,632 - INFO - Successfully created cache\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Language Model (LLM)\n",
        "The script initializes a Cohere language model (LLM) that will be used for generating responses to queries. LLMs are powerful tools for natural language understanding and generation, capable of producing human-like text based on input prompts. The model is configured with specific parameters, such as the temperature, which controls the randomness of its outputs.\n"
      ],
      "metadata": {
        "id": "wQ0fNbphbWpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    llm = ChatOpenAI(\n",
        "        openai_api_key=OPENAI_API_KEY,\n",
        "        model=\"gpt-4o-2024-08-06\",\n",
        "        temperature=0\n",
        "    )\n",
        "    logging.info(f\"Successfully created OpenAI LLM with model gpt-4o-2024-08-06\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error creating OpenAI LLM: {str(e)}\")"
      ],
      "metadata": {
        "id": "q7b1qLZF_gmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6650a3-08d1-42d0-9b66-a42f9ff243d4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-25 21:05:16,742 - INFO - Successfully created OpenAI LLM with model gpt-4o-2024-08-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform Semantic Search\n",
        "Semantic search in Couchbase involves converting a query and documents into vector representations using a model like OpenAI's embeddings. These vectors capture the semantic meaning of the text and are stored in Couchbase's vector store. When a query is made, Couchbase performs a similarity search by comparing the query vector against the stored document vectors, often using cosine similarity to measure how close they are in meaning. The system retrieves the top k most relevant documents based on these similarity scores.\n",
        "\n",
        "In the provided code, the search process begins by recording the start time, then executing the similarity_search_with_score function, which searches Couchbase for the most relevant documents. The search results include the document content and a similarity score, indicating how close each document is to the query in semantic space. The time taken to perform this search is then calculated and logged, and the results are displayed, showing the most relevant documents along with their similarity scores. In this process, Couchbase acts as both the storage and retrieval engine, enabling efficient and scalable semantic searches based on vector embeddings."
      ],
      "metadata": {
        "id": "nkAS-YQ3S-S9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What caused the 1929 Great Depression?\"\n",
        "\n",
        "try:\n",
        "    # Perform semantic search\n",
        "    start_time = time.time()\n",
        "    search_results = vector_store.similarity_search_with_score(query, k=10)\n",
        "    results = [{'id': doc.metadata.get('id', 'N/A'), 'text': doc.page_content, 'distance': score}\n",
        "               for doc, score in search_results]\n",
        "    search_elapsed_time = time.time() - start_time\n",
        "    logging.info(f\"Semantic search completed in {search_elapsed_time:.2f} seconds\")\n",
        "\n",
        "    print(f\"\\nSemantic Search Results (completed in {search_elapsed_time:.2f} seconds):\")\n",
        "    for result in results:\n",
        "        print(f\"Distance: {result['distance']:.4f}, Text: {result['text']}\")\n",
        "except CouchbaseException as e:\n",
        "    raise RuntimeError(f\"Error performing semantic search: {str(e)}\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Unexpected error: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI_TrKsWS-4Y",
        "outputId": "5cab8af3-ed14-444d-a7f9-4afbd78263b0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-25 21:05:17,117 - INFO - Semantic search completed in 0.37 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Semantic Search Results (completed in 0.37 seconds):\n",
            "Distance: 0.8595, Text: Why did the world enter a global depression in 1929 ?\n",
            "Distance: 0.8011, Text: When was `` the Great Depression '' ?\n",
            "Distance: 0.7282, Text: What were popular songs and types of songs in the 1920s ?\n",
            "Distance: 0.7188, Text: What tragedy befell the city of Dogtown in 1899 ?\n",
            "Distance: 0.7153, Text: What happened in dogtown in 1899 to make that year remembered ?\n",
            "Distance: 0.7145, Text: What are some of the significant historical events of the 1990s ?\n",
            "Distance: 0.7136, Text: What crop failure caused the Irish Famine ?\n",
            "Distance: 0.7135, Text: What are the signs of a country going into a recession ?\n",
            "Distance: 0.7078, Text: What were the causes of the Civil War ?\n",
            "Distance: 0.7057, Text: What Mexican leader was shot dead in 1923 ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval-Augmented Generation (RAG) with Couchbase and Langchain\n",
        "Couchbase and LangChain can be seamlessly integrated to create RAG (Retrieval-Augmented Generation) chains, enhancing the process of generating contextually relevant responses. In this setup, Couchbase serves as the vector store, where embeddings of documents are stored. When a query is made, LangChain retrieves the most relevant documents from Couchbase by comparing the query’s embedding with the stored document embeddings. These documents, which provide contextual information, are then passed to a generative language model within LangChain.\n",
        "\n",
        "The language model, equipped with the context from the retrieved documents, generates a response that is both informed and contextually accurate. This integration allows the RAG chain to leverage Couchbase’s efficient storage and retrieval capabilities, while LangChain handles the generation of responses based on the context provided by the retrieved documents. Together, they create a powerful system that can deliver highly relevant and accurate answers by combining the strengths of both retrieval and generation."
      ],
      "metadata": {
        "id": "4mYXK4BySemo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    template = \"\"\"You are a helpful bot. If you cannot answer based on the context provided, respond with a generic answer. Answer the question as truthfully as possible using the context below:\n",
        "    {context}\n",
        "    Question: {question}\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "    rag_chain = (\n",
        "        {\"context\": vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    logging.info(\"Successfully created RAG chain\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error creating LLM chains: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ft4gpN0Se80",
        "outputId": "a0ab28b3-7c0f-49c1-f240-00809fe3f07c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-25 21:05:17,128 - INFO - Successfully created RAG chain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Get RAG response\n",
        "    start_time = time.time()\n",
        "    rag_response = rag_chain.invoke(query)\n",
        "    rag_elapsed_time = time.time() - start_time\n",
        "    logging.info(f\"RAG response generated in {rag_elapsed_time:.2f} seconds\")\n",
        "\n",
        "    print(f\"RAG Response: {rag_response}\")\n",
        "\n",
        "except CouchbaseException as e:\n",
        "    raise RuntimeError(f\"Error performing semantic search: {str(e)}\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Unexpected error: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWObk3wu_g0b",
        "outputId": "489492b8-ee91-4324-fa6e-5e5cabbe3f8d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-25 21:05:18,699 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2024-08-25 21:05:18,748 - INFO - RAG response generated in 1.61 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Response: The 1929 Great Depression was caused by a combination of factors, including the stock market crash of October 1929, bank failures, reduction in consumer spending and investment, and flawed economic policies. These factors led to a severe worldwide economic downturn.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Couchbase as a caching mechanism\n",
        "Couchbase can be effectively used as a caching mechanism for RAG (Retrieval-Augmented Generation) responses by storing and retrieving precomputed results for specific queries. This approach enhances the system's efficiency and speed, particularly when dealing with repeated or similar queries. When a query is first processed, the RAG chain retrieves relevant documents, generates a response using the language model, and then stores this response in Couchbase, with the query serving as the key.\n",
        "\n",
        "For subsequent requests with the same query, the system checks Couchbase first. If a cached response is found, it is retrieved directly from Couchbase, bypassing the need to re-run the entire RAG process. This significantly reduces response time because the computationally expensive steps of document retrieval and response generation are skipped. Couchbase's role in this setup is to provide a fast and scalable storage solution for caching these responses, ensuring that frequently asked queries can be answered more quickly and efficiently."
      ],
      "metadata": {
        "id": "TXYWRaUoBx1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    queries = [\n",
        "        \"How does photosynthesis work?\",\n",
        "        \"What is the capital of France?\",\n",
        "        \"What caused the 1929 Great Depression?\",  # Repeated query\n",
        "        \"How does photosynthesis work?\",  # Repeated query\n",
        "    ]\n",
        "\n",
        "    for i, query in enumerate(queries, 1):\n",
        "        print(f\"\\nQuery {i}: {query}\")\n",
        "        start_time = time.time()\n",
        "        response = rag_chain.invoke(query)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"Response: {response}\")\n",
        "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error generating RAG response: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkmVbXmYTljt",
        "outputId": "0f0ca46b-85c2-4f05-a0de-9e38998c2805"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query 1: How does photosynthesis work?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-25 21:05:21,024 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy, usually from the sun, into chemical energy in the form of glucose. This process takes place primarily in the chloroplasts of plant cells. During photosynthesis, plants take in carbon dioxide from the air and water from the soil. Using the energy from sunlight, these substances are transformed into glucose and oxygen. The overall chemical equation for photosynthesis is:\n",
            "\n",
            "6CO2 + 6H2O + light energy → C6H12O6 + 6O2\n",
            "\n",
            "This process is crucial for life on Earth as it provides the oxygen we breathe and forms the base of the food chain.\n",
            "Time taken: 2.30 seconds\n",
            "\n",
            "Query 2: What is the capital of France?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-25 21:05:22,074 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: The capital of France is Paris.\n",
            "Time taken: 1.05 seconds\n",
            "\n",
            "Query 3: What caused the 1929 Great Depression?\n",
            "Response: The 1929 Great Depression was caused by a combination of factors, including the stock market crash of October 1929, bank failures, reduction in consumer spending and investment, and flawed economic policies. These factors led to a severe worldwide economic downturn.\n",
            "Time taken: 0.53 seconds\n",
            "\n",
            "Query 4: How does photosynthesis work?\n",
            "Response: Photosynthesis is the process by which green plants, algae, and some bacteria convert light energy, usually from the sun, into chemical energy in the form of glucose. This process takes place primarily in the chloroplasts of plant cells. During photosynthesis, plants take in carbon dioxide from the air and water from the soil. Using the energy from sunlight, these substances are transformed into glucose and oxygen. The overall chemical equation for photosynthesis is:\n",
            "\n",
            "6CO2 + 6H2O + light energy → C6H12O6 + 6O2\n",
            "\n",
            "This process is crucial for life on Earth as it provides the oxygen we breathe and forms the base of the food chain.\n",
            "Time taken: 0.42 seconds\n"
          ]
        }
      ]
    }
  ]
}